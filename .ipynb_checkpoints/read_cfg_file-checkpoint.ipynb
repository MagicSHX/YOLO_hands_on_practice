{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from __future__ import division\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import glob\n",
    "import glob\n",
    "from PIL import ImageTk, Image\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "from numpy import savetxt\n",
    "import gc\n",
    "\n",
    "from Model.function_bank import *\n",
    "from Model.Module_layers import *\n",
    "from Model.Module_layers_backup import *\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "cfgfile = \"cfg/yolov4.cfg\"\n",
    "model_file_path = \"Model/model.pt\"\n",
    "TL_model_file_path = \"Model/TL_model.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "anchors = [12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401]\n",
    "mask = [0, 1, 2]\n",
    "classes = 80\n",
    "input_image_size = 608"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nYolo_input = np.array([1.0 for i in range(255 * 76 * 76 * 2)]).reshape(2, 255, 76, 76)\\nYolo_input = torch.from_numpy(Yolo_input)\\n#Yolo_run_layer = Yolo(anchors, mask, classes, input_image_size)\\n\\n#b_x, b_y, b_w, b_h, objective_p, class_p = Yolo_run_layer(Yolo_input)\\n#combined_yolo_output = torch.cat((b_x, b_y, b_w, b_h, objective_p, class_p), 2)\\n\\nYolo_input = np.array([1.0 for i in range(255 * 76 * 76 * 2)]).reshape(2, 255, 76, 76)\\nYolo_input = torch.from_numpy(Yolo_input)\\ntarget = np.array([0 for i in range(3 * 76 * 76 * 85)])\\ninput_tensor = Yolo_input\\noutput_tensor = torch.Tensor(target)\\n\\nlearning_rate = 0.08\\nepoch_size = 5\\nsteps_for_printing_out_loss = 1\\n\\nYOLO_Module_WIP = Yolo(anchors, mask, classes, input_image_size)\\nYOLO_Module_WIP.cuda()\\n#Model_WIP.to(device)\\nloss_functioin = nn.MSELoss()\\noptimizer = optim.SGD(YOLO_Module_WIP.parameters(), lr = learning_rate)\\n\\ninput = input_tensor.cuda()\\ntarget = output_tensor.cuda()\\n\\ndef training_model():\\n    for i in range(1, epoch_size + 1):\\n        optimizer.zero_grad()\\n        output = YOLO_Module_WIP(input.cuda())\\n        print(output.size())\\n        #b_x, b_y, b_w, b_h, objective_p, class_p = YOLO_v4_Module_WIP(input.cuda())\\n        #output = b_x\\n        loss = loss_functioin(output, target.reshape(output.size(0), output.size(1), output.size(2), output.size(3), output.size(4)))\\n        loss.backward()\\n        optimizer.step()\\n        if i % (steps_for_printing_out_loss) == 0:\\n            print('Loss (epoch: ' + str(i) + '): ' + str(loss.cpu().detach().numpy()))\\n    torch.save({'state_dict': YOLO_v4_Module_WIP.state_dict(),'optimizer': optimizer.state_dict()}, model_file_path)\\n\\ntraining_model()\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train only on YOLO layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Yolo_input = np.array([1.0 for i in range(255 * 76 * 76 * 2)]).reshape(2, 255, 76, 76)\n",
    "Yolo_input = torch.from_numpy(Yolo_input)\n",
    "#Yolo_run_layer = Yolo(anchors, mask, classes, input_image_size)\n",
    "\n",
    "#b_x, b_y, b_w, b_h, objective_p, class_p = Yolo_run_layer(Yolo_input)\n",
    "#combined_yolo_output = torch.cat((b_x, b_y, b_w, b_h, objective_p, class_p), 2)\n",
    "\n",
    "Yolo_input = np.array([1.0 for i in range(255 * 76 * 76 * 2)]).reshape(2, 255, 76, 76)\n",
    "Yolo_input = torch.from_numpy(Yolo_input)\n",
    "target = np.array([0 for i in range(3 * 76 * 76 * 85)])\n",
    "input_tensor = Yolo_input\n",
    "output_tensor = torch.Tensor(target)\n",
    "\n",
    "learning_rate = 0.08\n",
    "epoch_size = 5\n",
    "steps_for_printing_out_loss = 1\n",
    "\n",
    "YOLO_Module_WIP = Yolo(anchors, mask, classes, input_image_size)\n",
    "YOLO_Module_WIP.cuda()\n",
    "#Model_WIP.to(device)\n",
    "loss_functioin = nn.MSELoss()\n",
    "optimizer = optim.SGD(YOLO_Module_WIP.parameters(), lr = learning_rate)\n",
    "\n",
    "input = input_tensor.cuda()\n",
    "target = output_tensor.cuda()\n",
    "\n",
    "def training_model():\n",
    "    for i in range(1, epoch_size + 1):\n",
    "        optimizer.zero_grad()\n",
    "        output = YOLO_Module_WIP(input.cuda())\n",
    "        print(output.size())\n",
    "        #b_x, b_y, b_w, b_h, objective_p, class_p = YOLO_v4_Module_WIP(input.cuda())\n",
    "        #output = b_x\n",
    "        loss = loss_functioin(output, target.reshape(output.size(0), output.size(1), output.size(2), output.size(3), output.size(4)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % (steps_for_printing_out_loss) == 0:\n",
    "            print('Loss (epoch: ' + str(i) + '): ' + str(loss.cpu().detach().numpy()))\n",
    "    torch.save({'state_dict': YOLO_v4_Module_WIP.state_dict(),'optimizer': optimizer.state_dict()}, model_file_path)\n",
    "\n",
    "training_model()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n",
      "162\n",
      "{'batch_normalize': '1', 'filters': '64', 'size': '3', 'stride': '2', 'pad': '1', 'activation': 'mish'}\n"
     ]
    }
   ],
   "source": [
    "#read TOLOv4 model architecture from cfg file\n",
    "layer_type, layer_details = read_cfg_file(cfgfile)\n",
    "net_layer = layer_details[0]\n",
    "layer_type = layer_type[1:]\n",
    "layer_details = layer_details[1:]\n",
    "\n",
    "print(len(layer_type))\n",
    "print(len(layer_details))\n",
    "print(layer_details[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is used to test end to end structure of YOLOv4\n",
    "#need to adjust target size according to YOLOv4 output size before testin\n",
    "\n",
    "input = np.array([1 for i in range(608 * 608 * 3 * 1)]).reshape(1, 3, 608, 608)\n",
    "#target = np.array([0 for i in range(7 * 7 * 30)])\n",
    "\n",
    "target = np.array([0 for i in range(3 * 19 * 19 * 85)])\n",
    "\n",
    "input_tensor = torch.Tensor(input)\n",
    "output_tensor = torch.Tensor(target)\n",
    "input = input_tensor.cuda()\n",
    "target = output_tensor.cuda()\n",
    "\n",
    "#x = np.array([1 for i in range(608 * 608 * 3)]).reshape(1, 3, 608, 608)\n",
    "#x = torch.tensor(x)\n",
    "\n",
    "learning_rate = 0.08\n",
    "epoch_size = 2\n",
    "steps_for_printing_out_loss = 1\n",
    "\n",
    "YOLO_v4_Module_WIP = YOLO_v4_model(layer_details, layer_type)\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.SGD(YOLO_v4_Module_WIP.parameters(), lr = learning_rate)\n",
    "YOLO_v4_Module_WIP.load_state_dict(torch.load(\"C:/Users/HX/Desktop/model_YOLOv4.pt\")['state_dict'])\n",
    "YOLO_v4_Module_WIP.eval()\n",
    "YOLO_v4_Module_WIP.cuda()\n",
    "\"\"\"\n",
    "for name, param in YOLO_v4_Module_WIP.named_parameters():\n",
    "    print('name: ', name)\n",
    "    print(type(param))\n",
    "    print('param.shape: ', param.shape)\n",
    "    print('param.requires_grad: ', param.requires_grad)\n",
    "    print('=====')\n",
    "#transfer learning:\n",
    "\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if name in ['fc.weight', 'fc.bias']:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\"\"\"\n",
    "\n",
    "def training_model():\n",
    "    for i in range(1, epoch_size + 1):\n",
    "        optimizer.zero_grad()\n",
    "        output, b, c = YOLO_v4_Module_WIP(input.cuda())\n",
    "        #print(output.size())\n",
    "        #b_x, b_y, b_w, b_h, objective_p, class_p = YOLO_v4_Module_WIP(input.cuda())\n",
    "        #output = b_x\n",
    "        #loss = loss_function(output, target.reshape(output.size(0), output.size(1), output.size(2), output.size(3)))\n",
    "        global output_tensor\n",
    "        output_tensor = output\n",
    "        loss = loss_function(output, target.reshape(output.size(0), output.size(1), output.size(2), output.size(3)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % (steps_for_printing_out_loss) == 0:\n",
    "            print('Loss (epoch: ' + str(i) + '): ' + str(loss.cpu().detach().numpy()))\n",
    "    #torch.save({'state_dict': YOLO_v4_Module_WIP.state_dict(),'optimizer': optimizer.state_dict()}, model_file_path)\n",
    "\n",
    "\n",
    "#training_model()\n",
    "#torch.save({'output': output_tensor}, 'Model/output.pt')\n",
    "#YOLO_v4_Module_WIP.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nYOLO_v4_Module_WIP = YOLO_v4_model(layer_details, layer_type)\\nYOLO_v4_Module_WIP.cuda()\\nYOLO_v4_Module_WIP.load_state_dict(torch.load(\"C:/Users/HX/Desktop/model_YOLOv4.pt\")[\\'state_dict\\'])\\nYOLO_v4_Module_WIP.eval()\\n\\n#for current_batch_no in range(len(image_path_list) // batch_size):\\nfor current_batch_no in range(0, 1200):\\n    input = image_reader(image_path_list[batch_size * current_batch_no: batch_size * (current_batch_no + 1)])\\n    input_tensor = torch.Tensor(input).cuda()\\n    #print(input.shape)\\n    output_137, output_148, output_159 = YOLO_v4_Module_WIP(input_tensor)\\n    print(output_137.shape)\\n    file_name = \\'F:/FlyAI/UnderwaterDetection_roundB/TL_input_data/\\' + str(current_batch_no) + \\'.pt\\'\\n    torch.save({\\'output_137\\': output_137, \\'output_148\\': output_148, \\'output_159\\': output_159}, file_name)\\n    \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to use YOLOv4 layer to convert image into layer output from 137, 148, 159\n",
    "\n",
    "#training_data_image_folder = \"F:/FlyAI/UnderwaterDetection_roundA/train-A/image/\"\n",
    "training_data_image_folder = \"F:/FlyAI/UnderwaterDetection_roundB/train-B/\"\n",
    "batch_size = 1\n",
    "\n",
    "image_path_list = glob.glob(training_data_image_folder + \"*.jpg\")\n",
    "\n",
    "batch_no = math.ceil(len(image_path_list) / batch_size)\n",
    "print(batch_no)\n",
    "\n",
    "\n",
    "\n",
    "#########convert original image into output from YOLOv4\n",
    "\"\"\"\n",
    "YOLO_v4_Module_WIP = YOLO_v4_model(layer_details, layer_type)\n",
    "YOLO_v4_Module_WIP.cuda()\n",
    "YOLO_v4_Module_WIP.load_state_dict(torch.load(\"C:/Users/HX/Desktop/model_YOLOv4.pt\")['state_dict'])\n",
    "YOLO_v4_Module_WIP.eval()\n",
    "\n",
    "#for current_batch_no in range(len(image_path_list) // batch_size):\n",
    "for current_batch_no in range(0, 1200):\n",
    "    input = image_reader(image_path_list[batch_size * current_batch_no: batch_size * (current_batch_no + 1)])\n",
    "    input_tensor = torch.Tensor(input).cuda()\n",
    "    #print(input.shape)\n",
    "    output_137, output_148, output_159 = YOLO_v4_Module_WIP(input_tensor)\n",
    "    print(output_137.shape)\n",
    "    file_name = 'F:/FlyAI/UnderwaterDetection_roundB/TL_input_data/' + str(current_batch_no) + '.pt'\n",
    "    torch.save({'output_137': output_137, 'output_148': output_148, 'output_159': output_159}, file_name)\n",
    "    \n",
    "\"\"\"\n",
    "#torch.save({'output_137': output_137}, 'Model/output.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_layerrr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['holothurian',\n",
       " 0,\n",
       " 225.46666666666667,\n",
       " 233.62962962962962,\n",
       " 265.3666666666667,\n",
       " 322.5777777777778,\n",
       " '000001',\n",
       " 1920,\n",
       " 1080,\n",
       " 39.900000000000006,\n",
       " 88.94814814814816,\n",
       " 3,\n",
       " 15,\n",
       " 17,\n",
       " 0.33854166666666785,\n",
       " 0.38148148148147953,\n",
       " 0.08692810457516341,\n",
       " 0.22181583079338693]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import training data\n",
    "#convert image into label data (for training purpose)\n",
    "\n",
    "round_A_data_folder = \"F:/FlyAI/UnderwaterDetection_roundA/\"\n",
    "round_A_image_folder = round_A_data_folder + \"train-A/image/\"\n",
    "round_A_box_folder = round_A_data_folder + \"train-A/box/\"\n",
    "\n",
    "box_file_path_list = glob.glob(round_A_box_folder + \"*.xml\")\n",
    "box_file_name_list = [x.split('\\\\')[1] for x in box_file_path_list]\n",
    "\n",
    "image_file_name_list = [(x.split('.')[0] + '.jpg') for x in box_file_name_list]\n",
    "image_file_path_list = [(round_A_image_folder + x) for x in image_file_name_list]\n",
    "\n",
    "\n",
    "anchors = [12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401]\n",
    "anchor_shape_1 = int(len(anchors) / 2)\n",
    "anchors = np.array(anchors).reshape(anchor_shape_1, 2)\n",
    "def best_anchor(image_info):\n",
    "    max_IoU = 0\n",
    "    for m in range(9):\n",
    "        x_GT = 0\n",
    "        y_GT = 0\n",
    "        w_GT = image_info[9]\n",
    "        h_GT = image_info[10]\n",
    "        x_PD = image_info[9] / 2 - anchors[m][0] / 2\n",
    "        y_PD = image_info[10] / 2 - anchors[m][1] / 2\n",
    "        w_PD = anchors[m][0]\n",
    "        h_PD = anchors[m][1]\n",
    "        \n",
    "        current_IOU = IoU(x_GT, y_GT, w_GT, h_GT, x_PD, y_PD, w_PD, h_PD)\n",
    "        #print(current_IOU)\n",
    "        if current_IOU >= max_IoU:\n",
    "            max_IoU = current_IOU\n",
    "            selected_anchors = m\n",
    "    \n",
    "    if selected_anchors < 3:\n",
    "        grid_size = 608 / 76\n",
    "    elif selected_anchors < 6:\n",
    "        grid_size = 608 / 38\n",
    "    elif selected_anchors < 9:\n",
    "        grid_size = 608 / 19\n",
    "    \n",
    "    grid_no_x = int((image_info[2] + image_info[4]) / 2 / grid_size)\n",
    "    grid_no_y = int((image_info[3] + image_info[5]) / 2 / grid_size)\n",
    "    \n",
    "    w_expanded_time = image_info[9] / w_PD\n",
    "    h_expanded_time = image_info[10] / h_PD\n",
    "    \n",
    "    position_x = (image_info[2] + image_info[4]) / 2 / grid_size - grid_no_x\n",
    "    position_y = (image_info[3] + image_info[5]) / 2 / grid_size - grid_no_y\n",
    "    \n",
    "    #print(image_info)\n",
    "    #print(grid_no_x)\n",
    "    #print(grid_no_y)\n",
    "    return selected_anchors, grid_no_x, grid_no_y, position_x, position_y, w_expanded_time, h_expanded_time\n",
    "\n",
    "def read_xml_into_training_data(box_file_path_list):\n",
    "    All_image = []\n",
    "    for box_file_path in box_file_path_list:\n",
    "        dict_object = {\"holothurian\": 0, \"echinus\": 1, \"scallop\": 2, \"starfish\": 3, 'waterweeds': 0}\n",
    "        tree = ET.parse(box_file_path)\n",
    "        root = tree.getroot()\n",
    "        \"\"\"\n",
    "        root.tag\n",
    "        root.attrib\n",
    "        \n",
    "        for child in root:\n",
    "            for sub_child in child:\n",
    "                print(sub_child.tag)\n",
    "        \"\"\"\n",
    "        \n",
    "        object = [None for i in range(6)]\n",
    "        All_object = []\n",
    "        All_image_size = []\n",
    "        image_size = [None, None]\n",
    "        frame_name = []\n",
    "        image_size_width = []\n",
    "        image_size_height = []\n",
    "        object_name = []\n",
    "        object_type = []\n",
    "        object_xmin = []\n",
    "        object_ymin = []\n",
    "        object_xmax = []\n",
    "        object_ymax = []\n",
    "\n",
    "        for name in root.findall(\"./frame\"):\n",
    "            frame_name.append(name.text)\n",
    "        for name in root.findall(\"./size/width\"):\n",
    "            image_size_width.append(int(name.text))\n",
    "        for name in root.findall(\"./size/height\"):\n",
    "            image_size_height.append(int(name.text))\n",
    "\n",
    "        for name in root.findall(\"./object/name\"):\n",
    "            object_name.append(name.text)\n",
    "\n",
    "        for name in root.findall(\"./object/bndbox/xmin\"):\n",
    "            object_xmin.append(int(name.text))\n",
    "        for name in root.findall(\"./object/bndbox/ymin\"):\n",
    "            object_ymin.append(int(name.text))\n",
    "        for name in root.findall(\"./object/bndbox/xmax\"):\n",
    "            object_xmax.append(int(name.text))\n",
    "        for name in root.findall(\"./object/bndbox/ymax\"):\n",
    "            object_ymax.append(int(name.text))\n",
    "\n",
    "        for i in range(len(object_name)):\n",
    "            current_object = []\n",
    "            current_object.append(object_name[i])\n",
    "            current_object.append(dict_object[object_name[i]])\n",
    "            current_object.append(object_xmin[i])\n",
    "            current_object.append(object_ymin[i])\n",
    "            current_object.append(object_xmax[i])\n",
    "            current_object.append(object_ymax[i])\n",
    "            current_object.append(frame_name[0])\n",
    "            current_object.append(image_size_width[0])\n",
    "            current_object.append(image_size_height[0])\n",
    "            All_object.append(current_object)\n",
    "        All_image.append(All_object)\n",
    "    return All_image\n",
    "\n",
    "\n",
    "training_input_data = read_image(image_file_path_list[0: 2])\n",
    "\n",
    "All_image = read_xml_into_training_data(box_file_path_list[0:4000])\n",
    "\n",
    "yolo_size = 608\n",
    "\n",
    "for i in range(len(All_image)):\n",
    "    for j in range(len(All_image[i])):\n",
    "        All_image[i][j][2] = All_image[i][j][2] / All_image[i][j][7] * yolo_size\n",
    "        All_image[i][j][4] = All_image[i][j][4] / All_image[i][j][7] * yolo_size\n",
    "        All_image[i][j][3] = All_image[i][j][3] / All_image[i][j][8] * yolo_size\n",
    "        All_image[i][j][5] = All_image[i][j][5] / All_image[i][j][8] * yolo_size\n",
    "        All_image[i][j].append(All_image[i][j][4] - All_image[i][j][2])\n",
    "        All_image[i][j].append(All_image[i][j][5] - All_image[i][j][3])\n",
    "        selected_anchors, grid_no_x, grid_no_y, position_x, position_y, w_expanded_time, h_expanded_time = best_anchor(All_image[i][j])\n",
    "        All_image[i][j].append(selected_anchors)\n",
    "        All_image[i][j].append(grid_no_x)\n",
    "        All_image[i][j].append(grid_no_y)\n",
    "        All_image[i][j].append(position_x)\n",
    "        All_image[i][j].append(position_y)\n",
    "        All_image[i][j].append(w_expanded_time)\n",
    "        All_image[i][j].append(h_expanded_time)\n",
    "        \n",
    "All_image[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Max memory allocated: 284.12 MB\n",
      "Max memory cached: 291.50 MB\n",
      "Max memory allocated: 308.58 MB\n",
      "Max memory cached: 316.67 MB\n",
      "Max memory allocated: 329.55 MB\n",
      "Max memory cached: 339.74 MB\n",
      "Max memory allocated: 350.52 MB\n",
      "Max memory cached: 360.71 MB\n",
      "Max memory allocated: 371.49 MB\n",
      "Max memory cached: 381.68 MB\n",
      "Max memory allocated: 392.46 MB\n",
      "Max memory cached: 402.65 MB\n",
      "Max memory allocated: 413.43 MB\n",
      "Max memory cached: 423.62 MB\n",
      "Max memory allocated: 434.41 MB\n",
      "Max memory cached: 444.60 MB\n",
      "Max memory allocated: 455.38 MB\n",
      "Max memory cached: 465.57 MB\n",
      "Max memory allocated: 476.35 MB\n",
      "Max memory cached: 486.54 MB\n",
      "Max memory allocated: 497.32 MB\n",
      "Max memory cached: 507.51 MB\n",
      "Max memory allocated: 518.29 MB\n",
      "Max memory cached: 528.48 MB\n",
      "Max memory allocated: 539.26 MB\n",
      "Max memory cached: 549.45 MB\n",
      "Max memory allocated: 560.24 MB\n",
      "Max memory cached: 570.43 MB\n",
      "Max memory allocated: 581.21 MB\n",
      "Max memory cached: 591.40 MB\n",
      "Max memory allocated: 602.18 MB\n",
      "Max memory cached: 612.37 MB\n",
      "Max memory allocated: 623.15 MB\n",
      "Max memory cached: 631.24 MB\n",
      "Max memory allocated: 644.12 MB\n",
      "Max memory cached: 654.31 MB\n",
      "Max memory allocated: 665.09 MB\n",
      "Max memory cached: 675.28 MB\n",
      "Max memory allocated: 686.06 MB\n",
      "Max memory cached: 696.25 MB\n",
      "Max memory allocated: 707.04 MB\n",
      "Max memory cached: 717.23 MB\n",
      "Max memory allocated: 728.01 MB\n",
      "Max memory cached: 738.20 MB\n",
      "Max memory allocated: 748.98 MB\n",
      "Max memory cached: 759.17 MB\n",
      "Max memory allocated: 769.95 MB\n",
      "Max memory cached: 780.14 MB\n",
      "Max memory allocated: 790.92 MB\n",
      "Max memory cached: 801.11 MB\n",
      "Max memory allocated: 811.89 MB\n",
      "Max memory cached: 822.08 MB\n",
      "Max memory allocated: 832.87 MB\n",
      "Max memory cached: 843.06 MB\n",
      "Max memory allocated: 853.84 MB\n",
      "Max memory cached: 864.03 MB\n",
      "Max memory allocated: 874.81 MB\n",
      "Max memory cached: 885.00 MB\n",
      "Max memory allocated: 895.78 MB\n",
      "Max memory cached: 905.97 MB\n",
      "Max memory allocated: 916.75 MB\n",
      "Max memory cached: 926.94 MB\n",
      "Max memory allocated: 937.72 MB\n",
      "Max memory cached: 947.91 MB\n",
      "Max memory allocated: 958.69 MB\n",
      "Max memory cached: 968.88 MB\n",
      "Max memory allocated: 979.67 MB\n",
      "Max memory cached: 989.86 MB\n",
      "Max memory allocated: 1000.64 MB\n",
      "Max memory cached: 1010.83 MB\n",
      "Max memory allocated: 1021.61 MB\n",
      "Max memory cached: 1031.80 MB\n",
      "Max memory allocated: 1042.58 MB\n",
      "Max memory cached: 1052.77 MB\n",
      "Max memory allocated: 1063.55 MB\n",
      "Max memory cached: 1073.74 MB\n",
      "Max memory allocated: 1084.52 MB\n",
      "Max memory cached: 1094.71 MB\n",
      "Max memory allocated: 1105.50 MB\n",
      "Max memory cached: 1115.68 MB\n",
      "Max memory allocated: 1126.47 MB\n",
      "Max memory cached: 1136.66 MB\n",
      "Max memory allocated: 1147.44 MB\n",
      "Max memory cached: 1157.63 MB\n",
      "Max memory allocated: 1168.41 MB\n",
      "Max memory cached: 1178.60 MB\n",
      "Max memory allocated: 1189.38 MB\n",
      "Max memory cached: 1199.57 MB\n",
      "Max memory allocated: 1210.35 MB\n",
      "Max memory cached: 1220.54 MB\n",
      "Max memory allocated: 1231.32 MB\n",
      "Max memory cached: 1241.51 MB\n",
      "Max memory allocated: 1252.30 MB\n",
      "Max memory cached: 1262.49 MB\n",
      "Max memory allocated: 1273.27 MB\n",
      "Max memory cached: 1283.46 MB\n",
      "Max memory allocated: 1294.24 MB\n",
      "Max memory cached: 1304.43 MB\n",
      "Max memory allocated: 1315.21 MB\n",
      "Max memory cached: 1325.40 MB\n",
      "Loss (epoch: 1): 4660215.0\n",
      "tensor(2139.0496, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2598702.2500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(909214.1875, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(1150161.1250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Max memory allocated: 1330.70 MB\n",
      "Max memory cached: 1346.37 MB\n",
      "Loss (epoch: 1): 2056489.0\n",
      "tensor(10.7563, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(1361728.3750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(37600., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(657149.5000, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#torch.autograd.set_detect_anomaly(True)\n",
    "#training loop for transfer layer\n",
    "\n",
    "\n",
    "\n",
    "#@profile\n",
    "def TL_model_training():\n",
    "    learning_rate = 0.001\n",
    "    epoch_size = 1\n",
    "    batch_size = 50\n",
    "    steps_for_printing_out_loss = 1\n",
    "    \n",
    "    loss_function_MSE = nn.MSELoss(size_average=False)\n",
    "    loss_function_BCE = nn.BCELoss(size_average=False)\n",
    "    \n",
    "    #loss_function_MSE = nn.MSELoss()\n",
    "    #loss_function_BCE = nn.BCELoss()\n",
    "    \n",
    "    TL_model = transfer_learning_model().cuda()\n",
    "    optimizer = optim.SGD(TL_model.parameters(), lr = learning_rate)\n",
    "    #TL_model.load_state_dict(torch.load('Model/TL_model_starting_point.pt')['state_dict'])\n",
    "    #TL_model.load_state_dict(torch.load('Model/TL_model1.pt')['state_dict'])\n",
    "    #TL_model.eval()\n",
    "    \n",
    "    for i in range(1, epoch_size + 1):\n",
    "        #loss = 0\n",
    "        for current_batch_no in range(int(100/ batch_size)):\n",
    "            optimizer.zero_grad()\n",
    "            total_loss = 0\n",
    "            total_loss_w_h = 0\n",
    "            total_loss_class = 0\n",
    "            total_loss_obj_p = 0\n",
    "            total_loss_x_y = 0\n",
    "            for image_pt_name in range(current_batch_no * batch_size, (current_batch_no + 1) * batch_size):\n",
    "                torch.cuda.empty_cache()\n",
    "                if image_pt_name % 200 == 0:\n",
    "                    print(image_pt_name)\n",
    "\n",
    "                file_name = 'F:/FlyAI/TL_input_data/' + str(image_pt_name) + '.pt'\n",
    "                input_data = torch.load(file_name)\n",
    "                layer_137_out = input_data['output_137'].cuda()\n",
    "                layer_148_out = input_data['output_148'].cuda()\n",
    "                layer_159_out = input_data['output_159'].cuda()\n",
    "                #print(layer_137_out.shape)\n",
    "                \n",
    "                output_76, output_38, output_19 = TL_model(layer_137_out, layer_148_out, layer_159_out)\n",
    "                output_file_name = 'F:/FlyAI/TL_output_data/' + str(image_pt_name) + '.pt'\n",
    "                torch.save({'output_76': output_76, 'output_38': output_38, 'output_19': output_19}, output_file_name)\n",
    "                \n",
    "                target_76 = output_76.clone()\n",
    "                target_38 = output_38.clone()\n",
    "                target_19 = output_19.clone()\n",
    "                #there is a possibility of more than one GT are mapped into same grid of a anchor, may need to check from training data?\n",
    "                image_set = range(0, 1)\n",
    "                target_76[:,:,4,:,:] = 0\n",
    "                target_38[:,:,4,:,:] = 0\n",
    "                target_19[:,:,4,:,:] = 0\n",
    "                \n",
    "                for image_no in image_set:\n",
    "                    image_no_current_batch = image_no % batch_size\n",
    "\n",
    "                    for item in All_image[image_pt_name]:\n",
    "                        #print(item)\n",
    "                        anchor_no = item[11] % 3\n",
    "                        grid_x = item[12]\n",
    "                        grid_y = item[13]\n",
    "\n",
    "                        obj_class_no = item[1]\n",
    "                        central_x = (item[2] + item[4]) / 2\n",
    "                        central_y = (item[3] + item[5]) / 2\n",
    "                        width_x = item[9]\n",
    "                        height_y = item[10]\n",
    "\n",
    "                        position_x = item[14]\n",
    "                        position_y = item[15]\n",
    "                        w_expanded_time = item[16]\n",
    "                        h_expanded_time = item[17]\n",
    "\n",
    "                        if item[11] < 3:\n",
    "                            #target_76[anchor_no,image_no_current_batch,4,:,:] = 0\n",
    "                            target_76[anchor_no,image_no_current_batch,0,grid_y,grid_x] = position_x\n",
    "                            target_76[anchor_no,image_no_current_batch,1,grid_y,grid_x] = position_y\n",
    "                            target_76[anchor_no,image_no_current_batch,2,grid_y,grid_x] = w_expanded_time\n",
    "                            target_76[anchor_no,image_no_current_batch,3,grid_y,grid_x] = h_expanded_time\n",
    "                            target_76[anchor_no,image_no_current_batch,4,grid_y,grid_x] = 1\n",
    "\n",
    "                            target_76[anchor_no,image_no_current_batch,5:9,grid_y,grid_x] = 0\n",
    "                            target_76[anchor_no,image_no_current_batch,5 + obj_class_no,grid_y,grid_x] = 1\n",
    "                        elif item[11] < 6:\n",
    "                            #target_38[anchor_no,image_no_current_batch,4,:,:] = 0\n",
    "                            target_38[anchor_no,image_no_current_batch,0,grid_y,grid_x] = position_x\n",
    "                            target_38[anchor_no,image_no_current_batch,1,grid_y,grid_x] = position_y\n",
    "                            target_38[anchor_no,image_no_current_batch,2,grid_y,grid_x] = w_expanded_time\n",
    "                            target_38[anchor_no,image_no_current_batch,3,grid_y,grid_x] = h_expanded_time\n",
    "                            target_38[anchor_no,image_no_current_batch,4,grid_y,grid_x] = 1\n",
    "\n",
    "                            target_38[anchor_no,image_no_current_batch,5:9,grid_y,grid_x] = 0\n",
    "                            target_38[anchor_no,image_no_current_batch,5 + obj_class_no,grid_y,grid_x] = 1\n",
    "                        elif item[11] < 9:\n",
    "                            #target_19[anchor_no,image_no_current_batch,4,:,:] = 0\n",
    "                            target_19[anchor_no,image_no_current_batch,0,grid_y,grid_x] = position_x\n",
    "                            target_19[anchor_no,image_no_current_batch,1,grid_y,grid_x] = position_y\n",
    "                            target_19[anchor_no,image_no_current_batch,2,grid_y,grid_x] = w_expanded_time\n",
    "                            target_19[anchor_no,image_no_current_batch,3,grid_y,grid_x] = h_expanded_time\n",
    "                            target_19[anchor_no,image_no_current_batch,4,grid_y,grid_x] = 1\n",
    "\n",
    "                            target_19[anchor_no,image_no_current_batch,5:9,grid_y,grid_x] = 0\n",
    "                            target_19[anchor_no,image_no_current_batch,5 + obj_class_no,grid_y,grid_x] = 1\n",
    "\n",
    "                #global output_tensor\n",
    "                #output_tensor = output\n",
    "                #print(output_76.shape)\n",
    "                #print(output_76)\n",
    "\n",
    "                target_file_name = 'F:/FlyAI/TL_output_data/target_' + str(image_pt_name) + '.pt'\n",
    "                torch.save({'target_76': target_76, 'target_38': target_38, 'target_19': target_19}, target_file_name)\n",
    "\n",
    "\n",
    "\n",
    "                target_76 = Variable(target_76, requires_grad=False)\n",
    "                target_38 = Variable(target_38, requires_grad=False)\n",
    "                target_19 = Variable(target_19, requires_grad=False)\n",
    "\n",
    "                #print(target_76[0])\n",
    "\n",
    "                loss_w_h = loss_function_MSE(output_76[:, :, 2 : 4, :, :], target_76[:, :, 2 : 4, :, :])\\\n",
    "                 + loss_function_MSE(output_38[:, :, 2 : 4, :, :], target_38[:, :, 2 : 4, :, :])\\\n",
    "                 + loss_function_MSE(output_19[:, :, 2 : 4, :, :], target_19[:, :, 2 : 4, :, :])\n",
    "\n",
    "                loss_w_h = loss_w_h / 2\n",
    "\n",
    "                loss_class = loss_function_BCE(output_76[:, :, 5 : 9, :, :], target_76[:, :, 5 : 9, :, :])\\\n",
    "                 + loss_function_BCE(output_38[:, :, 5 : 9, :, :], target_38[:, :, 5 : 9, :, :])\\\n",
    "                 + loss_function_BCE(output_19[:, :, 5 : 9, :, :], target_19[:, :, 5 : 9, :, :])\n",
    "\n",
    "                loss_obj_p = loss_function_BCE(output_76[:, :, 4, :, :], target_76[:, :, 4, :, :])\\\n",
    "                 + loss_function_BCE(output_38[:, :, 4, :, :], target_38[:, :, 4, :, :])\\\n",
    "                 + loss_function_BCE(output_19[:, :, 4, :, :], target_19[:, :, 4, :, :])\n",
    "\n",
    "                loss_x_y = loss_function_BCE(output_76[:, :, 0 : 2, :, :], target_76[:, :, 0 : 2, :, :])\\\n",
    "                 + loss_function_BCE(output_38[:, :, 0 : 2, :, :], target_38[:, :, 0 : 2, :, :])\\\n",
    "                 + loss_function_BCE(output_19[:, :, 0 : 2, :, :], target_19[:, :, 0 : 2, :, :])\n",
    "\n",
    "                loss = loss_w_h + loss_class + loss_obj_p + loss_x_y\n",
    "                #loss = loss_function_MSE(output_76, target_76) + loss_function_MSE(output_38, target_38) + loss_function_MSE(output_19, target_19)\n",
    "\n",
    "                total_loss += loss\n",
    "                total_loss_w_h += loss_w_h\n",
    "                total_loss_class += loss_class\n",
    "                total_loss_obj_p += loss_obj_p\n",
    "                total_loss_x_y += loss_x_y\n",
    "                gc.collect()\n",
    "                print('Max memory allocated: {0:.2f} MB'\n",
    "                      .format(torch.cuda.max_memory_allocated() / 1e6))\n",
    "                print('Max memory cached: {0:.2f} MB'\n",
    "                      .format(torch.cuda.max_memory_cached() / 1e6))\n",
    "                loss.backward()\n",
    "\n",
    "            #loss = loss_function(output_76, target_76)\n",
    "\n",
    "            optimizer.step()\n",
    "            if i % (steps_for_printing_out_loss) == 0:\n",
    "                print('Loss (epoch: ' + str(i) + '): ' + str(total_loss.cpu().detach().numpy()))\n",
    "                print(total_loss_w_h)\n",
    "                print(total_loss_class)\n",
    "                print(total_loss_obj_p)\n",
    "                print(total_loss_x_y)\n",
    "                TL_model_file_path_by_epoch = \"Model/TL_model\"+ str(i) + \".pt\" \n",
    "                torch.save({'state_dict': TL_model.state_dict(),'optimizer': optimizer.state_dict()}, TL_model_file_path_by_epoch)\n",
    "            \n",
    "    torch.save({'state_dict': TL_model.state_dict(),'optimizer': optimizer.state_dict()}, TL_model_file_path)\n",
    "\n",
    "TL_model_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(image_pt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_into_csv(image_pt_name):\n",
    "    target_file_name = 'F:/FlyAI/TL_output_data/target_' + str(image_pt_name) + '.pt'\n",
    "\n",
    "    target_data = torch.load(target_file_name)\n",
    "    target_76 = target_data['target_76']\n",
    "    target_38 = target_data['target_38']\n",
    "    target_19 = target_data['target_19']\n",
    "\n",
    "    target_76 = torch.transpose(target_76, 2, 4)\n",
    "    target_38 = torch.transpose(target_38, 2, 4)\n",
    "    target_19 = torch.transpose(target_19, 2, 4)\n",
    "\n",
    "    target_76 = target_76.reshape(3 * 76**2, 9)\n",
    "    target_38 = target_38.reshape(3 * 38**2, 9)\n",
    "    target_19 = target_19.reshape(3 * 19**2, 9)\n",
    "\n",
    "    #print(target_38[:,:,5,:,:])\n",
    "    savetxt('19_data_t.csv', target_19.cpu().detach().numpy(), delimiter=',')\n",
    "    savetxt('38_data_t.csv', target_38.cpu().detach().numpy(), delimiter=',')\n",
    "    savetxt('76_data_t.csv', target_76.cpu().detach().numpy(), delimiter=',')\n",
    "    \n",
    "target_into_csv(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_input_into_csv(image_pt_name):\n",
    "    input_file_name = 'F:/FlyAI/TL_input_data/' + str(image_pt_name) + '.pt'\n",
    "\n",
    "    input_data = torch.load(input_file_name)\n",
    "    input_76 = input_data['output_137'].cuda()\n",
    "    input_38 = input_data['output_148'].cuda()\n",
    "    input_19 = input_data['output_159'].cuda()\n",
    "\n",
    "    input_76 = torch.transpose(input_76, 1, 3)\n",
    "    input_38 = torch.transpose(input_38, 1, 3)\n",
    "    input_19 = torch.transpose(input_19, 1, 3)\n",
    "\n",
    "    input_76 = input_76.reshape(76**2, 256)\n",
    "    input_38 = input_38.reshape(38**2, 512)\n",
    "    input_19 = input_19.reshape(19**2, 1024)\n",
    "\n",
    "    savetxt('76_data_i.csv', input_76.cpu().detach().numpy(), delimiter=',')\n",
    "    savetxt('38_data_i.csv', input_38.cpu().detach().numpy(), delimiter=',')\n",
    "    savetxt('19_data_i.csv', input_19.cpu().detach().numpy(), delimiter=',')\n",
    "    \n",
    "training_input_into_csv(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_output_into_csv(image_pt_name):\n",
    "    output_file_name = 'F:/FlyAI/TL_output_data/' + str(image_pt_name) + '.pt'\n",
    "\n",
    "    output_data = torch.load(output_file_name)\n",
    "    output_76 = output_data['output_76'].cuda()\n",
    "    output_38 = output_data['output_38'].cuda()\n",
    "    output_19 = output_data['output_19'].cuda()\n",
    "\n",
    "    output_76 = torch.transpose(output_76, 2, 4)\n",
    "    output_38 = torch.transpose(output_38, 2, 4)\n",
    "    output_19 = torch.transpose(output_19, 2, 4)\n",
    "\n",
    "    output_76 = output_76.reshape(3 * 76**2, 9)\n",
    "    output_38 = output_38.reshape(3 * 38**2, 9)\n",
    "    output_19 = output_19.reshape(3 * 19**2, 9)\n",
    "\n",
    "    savetxt('19_data_o.csv', output_19.cpu().detach().numpy(), delimiter=',')\n",
    "    savetxt('38_data_o.csv', output_38.cpu().detach().numpy(), delimiter=',')\n",
    "    savetxt('76_data_o.csv', output_76.cpu().detach().numpy(), delimiter=',')\n",
    "\n",
    "training_output_into_csv(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(x**2 for x in (19, 38, 76))\n",
    "def x():\n",
    "    output = torch.cat((output_76, output_38, output_19), 0)\n",
    "    print(output.shape)\n",
    "    obj_threshold = 0.53\n",
    "    updated_outcome = []\n",
    "    for item in output:\n",
    "        if item[4] >= obj_threshold:\n",
    "            updated_outcome.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(updated_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#len(updated_outcome)\n",
    "#updated_outcome = updated_outcome[0:9]\n",
    "#print(updated_outcome.shape)\n",
    "def sort_column(elem):\n",
    "    return elem[4]\n",
    "#updated_outcome.sort(key = sort_column, reverse=True)\n",
    "\n",
    "#updated_outcome = updated_outcome[updated_outcome[:,0].argsort()]\n",
    "\n",
    "#print(updated_outcome)\n",
    "\n",
    "def x():\n",
    "    #sort by obj possibility\n",
    "    IoU_threshold = 0.5\n",
    "    final_list = []\n",
    "    #print(updated_outcome)\n",
    "    #updated_outcome[0][3] = 6\n",
    "\n",
    "    for origin_item in updated_outcome:\n",
    "        #print(final_list)\n",
    "        if final_list == []:\n",
    "            final_list.append(origin_item)\n",
    "            continue\n",
    "        indicator = 1\n",
    "        for new_item in final_list:\n",
    "            #print(IoU(new_item[0] - new_item[2]/2, new_item[1] - new_item[3]/2, new_item[2], new_item[3], origin_item[0] - origin_item[2]/2, origin_item[1] - origin_item[3]/2, origin_item[2], origin_item[3]))\n",
    "            if IoU(new_item[0] - new_item[2]/2, new_item[1] - new_item[3]/2, new_item[2], new_item[3], origin_item[0] - origin_item[2]/2, origin_item[1] - origin_item[3]/2, origin_item[2], origin_item[3]) < IoU_threshold:\n",
    "                indicator *= 1\n",
    "            else:\n",
    "                indicator *= 0\n",
    "                break\n",
    "        if indicator == 1:\n",
    "            final_list.append(origin_item)\n",
    "\n",
    "    final_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class transfer_learning_model_prediction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.anchors = [12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401]\n",
    "        self.mask_a = [0, 1, 2]\n",
    "        self.mask_b = [3, 4, 5]\n",
    "        self.mask_c = [6, 7, 8]\n",
    "        #self.TL_model_list = nn.ModuleList()\n",
    "        #TL_model_list.append(Conv_Layer_box(in_channel = 256, out_channel = 255, kernel_size = 1, stride = 1, activation_func = 'linear', batch_normalize = False))\n",
    "        self.Conv_Layer_76_a = Conv_Layer_box(in_channel = 256, out_channel = 255, kernel_size = 1, stride = 1, activation_func = 'linear', batch_normalization = False)\n",
    "        #\n",
    "        self.norm_76 = nn.BatchNorm2d(255)\n",
    "        self.Sigmoid_layer_76 = nn.Sigmoid()\n",
    "        self.Conv_Layer_76_b = Conv_Layer_box(in_channel = 255, out_channel = 27, kernel_size = 1, stride = 1, activation_func = 'linear', batch_normalization = False)\n",
    "        #self.Yolo_Layer_76 = Yolo(self.anchors, self.mask_a, classes = 4, input_image_size = 608)\n",
    "        self.Yolo_Layer_76 = Yolo_TL_prediction(self.anchors, self.mask_a, classes = 4, input_image_size = 608)\n",
    "        \n",
    "        self.Conv_Layer_38_a = Conv_Layer_box(in_channel = 512, out_channel = 255, kernel_size = 1, stride = 1, activation_func = 'linear', batch_normalization = False)\n",
    "        #\n",
    "        self.norm_38 = nn.BatchNorm2d(255)\n",
    "        self.Sigmoid_layer_38 = nn.Sigmoid()\n",
    "        self.Conv_Layer_38_b = Conv_Layer_box(in_channel = 255, out_channel = 27, kernel_size = 1, stride = 1, activation_func = 'linear', batch_normalization = False)\n",
    "        #self.Yolo_Layer_38 = Yolo(self.anchors, self.mask_b, classes = 4, input_image_size = 608)\n",
    "        self.Yolo_Layer_38 = Yolo_TL_prediction(self.anchors, self.mask_b, classes = 4, input_image_size = 608)\n",
    "        \n",
    "        self.Conv_Layer_19_a = Conv_Layer_box(in_channel = 1024, out_channel = 255, kernel_size = 1, stride = 1, activation_func = 'linear', batch_normalization = False)\n",
    "        #\n",
    "        self.norm_19 = nn.BatchNorm2d(255)\n",
    "        self.Sigmoid_layer_19 = nn.Sigmoid()\n",
    "        self.Conv_Layer_19_b = Conv_Layer_box(in_channel = 255, out_channel = 27, kernel_size = 1, stride = 1, activation_func = 'linear', batch_normalization = False)\n",
    "        #self.Yolo_Layer_19 = Yolo(self.anchors, self.mask_c, classes = 4, input_image_size = 608)\n",
    "        self.Yolo_Layer_19 = Yolo_TL_prediction(self.anchors, self.mask_c, classes = 4, input_image_size = 608)\n",
    "        \n",
    "    def forward(self, layer_137_out, layer_148_out, layer_159_out, image_no):\n",
    "        out_76_1 = self.Conv_Layer_76_a(layer_137_out)\n",
    "        \n",
    "        #out_76_1 = self.norm_76(out_76_1)\n",
    "        out_76_1 = self.Sigmoid_layer_76(out_76_1)\n",
    "        out_76_1 = self.Conv_Layer_76_b(out_76_1)\n",
    "        out_76_1 = self.Yolo_Layer_76(out_76_1)\n",
    "        \n",
    "        out_38_1 = self.Conv_Layer_38_a(layer_148_out)\n",
    "        \n",
    "        #out_38_1 = self.norm_76(out_38_1)\n",
    "        out_38_1 = self.Sigmoid_layer_38(out_38_1)\n",
    "        out_38_1 = self.Conv_Layer_38_b(out_38_1)\n",
    "        out_38_1 = self.Yolo_Layer_38(out_38_1)\n",
    "        \n",
    "        out_19_1 = self.Conv_Layer_19_a(layer_159_out)\n",
    "        \n",
    "        #out_19_1 = self.norm_76(out_19_1)\n",
    "        out_19_1 = self.Sigmoid_layer_19(out_19_1)\n",
    "        out_19_1 = self.Conv_Layer_19_b(out_19_1)\n",
    "        out_19_1 = self.Yolo_Layer_19(out_19_1)\n",
    "        #another option: only use 1 conv layer b, and Yolo layer\n",
    "        \n",
    "        output_76 = torch.transpose(out_76_1, 2, 4)\n",
    "        output_38 = torch.transpose(out_38_1, 2, 4)\n",
    "        output_19 = torch.transpose(out_19_1, 2, 4)\n",
    "\n",
    "        output_76 = output_76.reshape(3 * 76**2, 9)\n",
    "        output_38 = output_38.reshape(3 * 38**2, 9)\n",
    "        output_19 = output_19.reshape(3 * 19**2, 9)\n",
    "        \n",
    "        output = torch.cat((output_76, output_38, output_19), 0)\n",
    "        print(output.shape)\n",
    "        #print(output)\n",
    "        #savetxt('abc.csv', output.cpu().detach().numpy(), delimiter=',')\n",
    "        def sort_column(elem):\n",
    "            return elem[4]\n",
    "        output = output.tolist()\n",
    "        output.sort(key = sort_column, reverse=True)\n",
    "        \n",
    "        obj_threshold = 0.8\n",
    "        updated_outcome = []\n",
    "        for item in output:\n",
    "            if item[4] >= obj_threshold:\n",
    "                updated_outcome.append(item)\n",
    "            else:\n",
    "                break\n",
    "        IoU_threshold = 0.7\n",
    "        final_list = []\n",
    "        #print(updated_outcome)\n",
    "        #updated_outcome[0][3] = 6\n",
    "        FlyAI_result = []\n",
    "        for origin_item in updated_outcome:\n",
    "            #print(final_list)\n",
    "            if final_list == []:\n",
    "                final_list.append(origin_item)\n",
    "                continue\n",
    "            indicator = 1\n",
    "            for new_item in final_list:\n",
    "                #print(IoU(new_item[0] - new_item[2]/2, new_item[1] - new_item[3]/2, new_item[2], new_item[3], origin_item[0] - origin_item[2]/2, origin_item[1] - origin_item[3]/2, origin_item[2], origin_item[3]))\n",
    "                if IoU(new_item[0] - new_item[2]/2, new_item[1] - new_item[3]/2, new_item[2], new_item[3], origin_item[0] - origin_item[2]/2, origin_item[1] - origin_item[3]/2, origin_item[2], origin_item[3]) < IoU_threshold:\n",
    "                    indicator *= 1\n",
    "                else:\n",
    "                    indicator *= 0\n",
    "                    break\n",
    "            if indicator == 1:\n",
    "                final_list.append(origin_item)\n",
    "        self.mrr = []\n",
    "        self.result = []\n",
    "        for item in final_list:\n",
    "            a = new_item[5:9]\n",
    "            name = a.index(max(a))\n",
    "            image_id = image_no\n",
    "            confidence = new_item[4]\n",
    "            xmin = round(new_item[0] - new_item[2]/2)\n",
    "            ymin = round(new_item[1] - new_item[3]/2)\n",
    "            xmax = round(new_item[0] + new_item[2]/2)\n",
    "            ymax = round(new_item[1] + new_item[3]/2)\n",
    "            \n",
    "            self.mrr = [name, image_id, confidence, xmin, ymin, xmax, ymax]\n",
    "            self.result.append(self.mrr)\n",
    "        return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TL_model_prediction = transfer_learning_model_prediction().cuda()\n",
    "\n",
    "TL_model_prediction.load_state_dict(torch.load('Model/TL_model2.pt')['state_dict'])\n",
    "TL_model_prediction.eval()\n",
    "\n",
    "submission_list = []\n",
    "for image_pt_name in range(1):\n",
    "    torch.cuda.empty_cache()\n",
    "    file_name = 'F:/FlyAI/UnderwaterDetection_roundB/TL_input_data/' + str(image_pt_name) + '.pt'\n",
    "    input_data = torch.load(file_name)\n",
    "    layer_137_out = input_data['output_137'].cuda()\n",
    "    layer_148_out = input_data['output_148'].cuda()\n",
    "    layer_159_out = input_data['output_159'].cuda()\n",
    "    kkkk = TL_model_prediction(layer_137_out, layer_148_out, layer_159_out, str(image_pt_name))\n",
    "    submission_list.append(kkkk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3 * (19**2) * (1 + 4 + 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function - Done\n",
    "\n",
    "\"\"\"\n",
    "- objective possibility: except GT grids are 1, all others are 0\n",
    "- class: 0, 1 only for GT grids, rest should be ignore (equal to predicted result)\n",
    "- x, y, w, h only for GT grids, rest should be ignore (equal to predicted result)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#grab yolov4 weight into TL - Done\n",
    "#convert training data into 60GB mid input data\n",
    "##########NMS (possibility threshold, IOU threshold, max 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#block 1 layer's parameter\n",
    "\n",
    "\n",
    "\n",
    "#convert result into FlyAI format\n",
    "#change back to normal pic size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_details[160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_layers\n",
    "n = [76, 38, 19]\n",
    "m = [608 / i for i in n for m in range(6)] \n",
    "print(m)\n",
    "\n",
    "\n",
    "anchor = [12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401]\n",
    "[anchor[i] / m[i] for i in range(len(m))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLO_v4_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.YOLO_v4_layers = nn.ModuleList()\n",
    "        \n",
    "            self.YOLO_v4_layers.append(Conv_Layer_box(in_channel[i], out_channel[i], kernel_size= kernel_size[i], stride = stride[i], activation_func = activation_func[i], batch_normalization = batch_normalization[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class abc():\n",
    "    def __init__(self, qwe, out):\n",
    "        print(qwe)\n",
    "        \n",
    "abc(4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = \"1,2,3\"\n",
    "m = abc.split(\",\")\n",
    "m\n",
    "abc = \"1\"\n",
    "m = abc.split(\",\")\n",
    "m\n",
    "k = 4\n",
    "\n",
    "HX = []\n",
    "\n",
    "for r in k:\n",
    "    print(r)\n",
    "    print(k[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightfile = \"D:/Installation/yolov4.weights\"\n",
    "fp = open(weightfile, 'rb')\n",
    "header = np.fromfile(fp, count=5, dtype=np.int32)\n",
    "header = torch.from_numpy(header)\n",
    "seen = header[3]\n",
    "buf = np.fromfile(fp, dtype=np.float32)\n",
    "fp.close()\n",
    "\n",
    "start = 0\n",
    "ind = -2\n",
    "buf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HX_weight = YOLO_v4_Module_WIP.state_dict()\n",
    "i = 0\n",
    "HX = []\n",
    "\n",
    "for kk in HX_weight:\n",
    "    i += 1\n",
    "    print(kk)\n",
    "    print(HX_weight[kk].size())\n",
    "    HX.append(HX_weight[kk].size())\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.load(\"D:/Installation/yolov4.pt\")\n",
    "#d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = d['model']\n",
    "weight_bank = []\n",
    "i = 0\n",
    "yolo_v4_size = []\n",
    "for kk in mm:\n",
    "    i += 1\n",
    "    print(kk)\n",
    "    print(mm[kk].size())\n",
    "    weight_bank.append(mm[kk])\n",
    "    yolo_v4_size.append(mm[kk].size())\n",
    "print(weight_bank)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_bank[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(weight_bank)\n",
    "#update weight into model\n",
    "\n",
    "i = 0\n",
    "HX = []\n",
    "\n",
    "kk_list = []\n",
    "for kk in YOLO_v4_Module_WIP.state_dict():\n",
    "    kk_list.append(kk)\n",
    "#print(kk_list)\n",
    "\n",
    "for kk in kk_list:\n",
    "    #print(weight_bank[i])\n",
    "    #print(kk)\n",
    "    #print(YOLO_v4_Module_WIP.state_dict()[kk])\n",
    "    YOLO_v4_Module_WIP['state_dict'].update({kk:  weight_bank[i]})\n",
    "    \n",
    "    #print(\"------------------------------\")\n",
    "    print(YOLO_v4_Module_WIP['state_dict'][kk])\n",
    "    i += 1\n",
    "    #HX.append(HX_weight[kk].size())\n",
    "print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOLO_v4_Module_WIP.state_dict()[kk_list[0]]\n",
    "#YOLO_v4_Module_WIP['state_dict']\n",
    "\n",
    "YOLOv4_wieght = torch.load(\"C:/Users/HX/Desktop/model.pt\")\n",
    "i = 0\n",
    "fff = YOLOv4_wieght['state_dict']\n",
    "\n",
    "for kk in kk_list:\n",
    "    #print(weight_bank[i])\n",
    "    #print(kk)\n",
    "    #print(YOLO_v4_Module_WIP.state_dict()[kk])\n",
    "    fff.update({kk:  weight_bank[i]})\n",
    "    i += 1\n",
    "    #print(\"------------------------------\")\n",
    "print(fff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'state_dict': fff,'optimizer': optimizer.state_dict()}, \"C:/Users/HX/Desktop/model_YOLOv4.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(yolo_v4_size)):\n",
    "    #print(yolo_v4_size[i] == HX[i])\n",
    "    if (yolo_v4_size[i] == HX[i]) == False:\n",
    "        print(yolo_v4_size[i])\n",
    "        print(HX[i])\n",
    "        print(i)\n",
    "    print(yolo_v4_size[i] == HX[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOLO_v4_Module_WIP.state_dict()\n",
    "torch.save({'state_dict': YOLO_v4_Module_WIP.state_dict(),'optimizer': optimizer.state_dict()}, model_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TL parameter:\n",
    "d = torch.load(\"D:/Installation/yolov4.pt\")\n",
    "mm = d['model']\n",
    "weight_bank = []\n",
    "i = 0\n",
    "yolo_v4_size = []\n",
    "for kk in mm:\n",
    "    i += 1\n",
    "    print(kk)\n",
    "    print(mm[kk])\n",
    "    weight_bank.append(mm[kk])\n",
    "    yolo_v4_size.append(mm[kk].size())\n",
    "print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TL parameter:\n",
    "d = torch.load(\"C:/Users/HX/Desktop/model.pt\")\n",
    "mm = d['state_dict']\n",
    "weight_bank = []\n",
    "i = 0\n",
    "yolo_v4_size = []\n",
    "for kk in mm:\n",
    "    i += 1\n",
    "    print(kk)\n",
    "    print(mm[kk])\n",
    "    weight_bank.append(mm[kk])\n",
    "    yolo_v4_size.append(mm[kk].size())\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TL parameter:\n",
    "d = torch.load(TL_model_file_path)\n",
    "mm = d['state_dict']\n",
    "weight_bank = []\n",
    "i = 0\n",
    "yolo_v4_size = []\n",
    "for kk in mm:\n",
    "    i += 1\n",
    "    print(kk)\n",
    "    print(mm[kk].size())\n",
    "    weight_bank.append(mm[kk])\n",
    "    yolo_v4_size.append(mm[kk].size())\n",
    "print(i)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(weight_bank)\n",
    "#update weight into model\n",
    "TL_model_weight = torch.load(TL_model_file_path)\n",
    "YOLO_v4_weight = torch.load(\"D:/Installation/yolov4.pt\")['model']\n",
    "i = 0\n",
    "HX = []\n",
    "\n",
    "TL_model_weight = TL_model_weight['state_dict']\n",
    "\n",
    "#print(TL_model_weight['state_dict']['Conv_Layer_76_a.conv_box.0.weight'])\n",
    "\n",
    "TL_model_weight.update({'Conv_Layer_76_a.conv_box.0.weight': YOLO_v4_weight['module_list.138.Conv2d.weight']})\n",
    "TL_model_weight.update({'Conv_Layer_76_a.conv_box.0.bias': YOLO_v4_weight['module_list.138.Conv2d.bias']})\n",
    "TL_model_weight.update({'Conv_Layer_38_a.conv_box.0.weight': YOLO_v4_weight['module_list.149.Conv2d.weight']})\n",
    "TL_model_weight.update({'Conv_Layer_38_a.conv_box.0.bias': YOLO_v4_weight['module_list.149.Conv2d.bias']})\n",
    "TL_model_weight.update({'Conv_Layer_19_a.conv_box.0.weight': YOLO_v4_weight['module_list.160.Conv2d.weight']})\n",
    "TL_model_weight.update({'Conv_Layer_19_a.conv_box.0.bias': YOLO_v4_weight['module_list.160.Conv2d.bias']})\n",
    "\n",
    "print(TL_model_weight['Conv_Layer_76_a.conv_box.0.weight'])\n",
    "\n",
    "torch.save({'state_dict': TL_model_weight}, 'Model/TL_model_starting_point.pt')\n",
    "#YOLO_v4_weight['module_list.138.Conv2d.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = TL_model_weight['state_dict']\n",
    "weight_bank = []\n",
    "i = 0\n",
    "yolo_v4_size = []\n",
    "for kk in mm:\n",
    "    i += 1\n",
    "    print(kk)\n",
    "    print(mm[kk].size())\n",
    "    weight_bank.append(mm[kk])\n",
    "    yolo_v4_size.append(mm[kk].size())\n",
    "print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    if i == 2:\n",
    "        continue\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
