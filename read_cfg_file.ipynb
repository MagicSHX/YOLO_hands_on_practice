{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from __future__ import division\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "cfgfile = \"C:/Users/HX/Desktop/yolov4.cfg\"\n",
    "model_file_path = \"Model/model.pt\"\n",
    "\n",
    "def read_cfg_file(cfgfile):\n",
    "    file = open(cfgfile, 'r')\n",
    "    lines = file.read().split('\\n')\n",
    "\n",
    "    layer_type = []\n",
    "    layer_details = []\n",
    "    current_layer_details = {}\n",
    "    for line in lines:\n",
    "        #print(line)\n",
    "        if line == '':\n",
    "            continue\n",
    "        elif line[0] == '#':\n",
    "            continue\n",
    "        else:\n",
    "            if (line[0] == '['):\n",
    "                layer_type.append(line[1 : -1])\n",
    "                if current_layer_details != {}:\n",
    "                    layer_details.append(current_layer_details)\n",
    "                    current_layer_details = {}\n",
    "            else:\n",
    "                current_layer_details.update([(line.split(\"=\")[0].rstrip(), line.split(\"=\")[1].lstrip())])\n",
    "    layer_details.append(current_layer_details)\n",
    "    return layer_type, layer_details\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mish(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * torch.tanh(F.softplus(x))\n",
    "        return x\n",
    "\n",
    "class Conv_Layer_box(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size, stride, activation_func, batch_normalization):\n",
    "        super().__init__()\n",
    "        padding = (int((kernel_size - 1)/2), int((kernel_size - 1)/2))\n",
    "        #TBC: linear\n",
    "        dict_activation_func = {\"ReLU\": nn.ReLU(inplace=False),\n",
    "                                \"linear\": nn.ReLU(inplace=False),\n",
    "                                \"leaky\": nn.LeakyReLU(0.1, inplace=False),\n",
    "                                \"mish\": Mish()\n",
    "                               }\n",
    "        \n",
    "        if batch_normalization == True:\n",
    "            bias = False\n",
    "        else:\n",
    "            bias = True\n",
    "        self.conv_box = nn.ModuleList()\n",
    "        self.conv_box.append(nn.Conv2d(in_channel, out_channel, kernel_size, stride, padding, bias = bias))\n",
    "        if batch_normalization == True:\n",
    "            self.conv_box.append(nn.BatchNorm2d(out_channel))\n",
    "        self.conv_box.append(dict_activation_func[activation_func])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.conv_box:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class Maxpool_pad_Layer_box(nn.Module):\n",
    "    def __init__(self, maxpool_size):\n",
    "        super().__init__()\n",
    "        self.maxpool_size = maxpool_size\n",
    "        #why there are 2 padding??????????????\n",
    "        self.pad_1 = int((self.maxpool_size - 1) / 2)\n",
    "        self.pad_2 = self.pad_1\n",
    "    def forward(self, x):\n",
    "        x = F.pad(x, (self.pad_1, self.pad_2, self.pad_1, self.pad_2), mode='replicate')\n",
    "        x = F.max_pool2d(x, self.maxpool_size, stride=1)\n",
    "        return x\n",
    "    \n",
    "class Upsample_layer(nn.Module):\n",
    "    def __init__(self, stride):\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch, channel, height, width = x.data.size()\n",
    "        x = x.view(batch, channel, height, 1, width, 1).expand(batch, channel, height, self.stride, width, self.stride).clone()\n",
    "        x = x.contiguous().view(batch, channel, height * self.stride, width * self.stride).clone()\n",
    "        return x\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "class shortcut(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "class route(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_length(a_1, a_2, b_1, b_2):\n",
    "    if a_1 <=b_1 and a_2 >= b_1:\n",
    "        return (min(a_2, b_2) - b_1)\n",
    "    elif a_1 <=b_1 and a_2 <= b_1:\n",
    "        return 0\n",
    "    else:\n",
    "        return cross_length(b_1, b_2, a_1, a_2)\n",
    "\n",
    "def IoU(x_GT, y_GT, h_GT, w_GT, x_PD, y_PD, h_PD, w_PD):\n",
    "    area_of_I = cross_length(x_GT, x_GT + h_GT, x_PD, x_PD + h_PD) * cross_length(y_GT, y_GT + h_GT, y_PD, y_PD + h_PD)\n",
    "    area_of_U = h_GT * w_GT + h_PD * w_PD - area_of_I\n",
    "    return area_of_I / area_of_U\n",
    "\n",
    "\n",
    "\n",
    "def axis_conversion(x_centre, y_centre, h, w):\n",
    "    return (x_centre - h / 2, y_centre - w / 2, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.5, 4.5, 3, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "25/(64)\n",
    "axis_conversion(6, 6, 3,3)\n",
    "#IoU(11,11,8,8,12,12,5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yolo_input = np.array([1.0 for i in range(255 * 76 * 76 * 2)]).reshape(2, 255, 76, 76)\n",
    "Yolo_input = torch.from_numpy(Yolo_input)\n",
    "#input[:,:,0,0] = 2\n",
    "#input[:,:,0,0]\n",
    "anchors = [12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401]\n",
    "mask = [0, 1, 2]\n",
    "classes = 80\n",
    "input_image_size = 608"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_on_Matrix_x = torch.from_numpy(np.array([[i for j in range(19)] for i in range(19)]))\n",
    "#add_on_Matrix_y = [[i for i in range(19)] for j in range(19)]\n",
    "#add_on_Matrix_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n#Yolo_run_layer = Yolo(anchors, mask, classes, input_image_size)\\n\\n#b_x, b_y, b_w, b_h, objective_p, class_p = Yolo_run_layer(Yolo_input)\\n#combined_yolo_output = torch.cat((b_x, b_y, b_w, b_h, objective_p, class_p), 2)\\n\\nYolo_input = np.array([1.0 for i in range(255 * 76 * 76 * 2)]).reshape(2, 255, 76, 76)\\nYolo_input = torch.from_numpy(Yolo_input)\\ntarget = np.array([0 for i in range(3 * 76 * 76 * 85)])\\ninput_tensor = Yolo_input\\noutput_tensor = torch.Tensor(target)\\n\\nlearning_rate = 0.08\\nepoch_size = 5\\nsteps_for_printing_out_loss = 1\\n\\nYOLO_Module_WIP = Yolo(anchors, mask, classes, input_image_size)\\nYOLO_Module_WIP.cuda()\\n#Model_WIP.to(device)\\nloss_functioin = nn.MSELoss()\\noptimizer = optim.SGD(YOLO_Module_WIP.parameters(), lr = learning_rate)\\n\\ninput = input_tensor.cuda()\\ntarget = output_tensor.cuda()\\n\\ndef training_model():\\n    for i in range(1, epoch_size + 1):\\n        optimizer.zero_grad()\\n        output = YOLO_Module_WIP(input.cuda())\\n        print(output.size())\\n        #b_x, b_y, b_w, b_h, objective_p, class_p = YOLO_v4_Module_WIP(input.cuda())\\n        #output = b_x\\n        loss = loss_functioin(output, target.reshape(output.size(0), output.size(1), output.size(2), output.size(3), output.size(4)))\\n        loss.backward()\\n        optimizer.step()\\n        if i % (steps_for_printing_out_loss) == 0:\\n            print('Loss (epoch: ' + str(i) + '): ' + str(loss.cpu().detach().numpy()))\\n    torch.save({'state_dict': YOLO_v4_Module_WIP.state_dict(),'optimizer': optimizer.state_dict()}, model_file_path)\\n\\ntraining_model()\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Yolo(nn.Module):\n",
    "    def __init__(self, anchors, mask, classes, input_image_size):\n",
    "        super().__init__()\n",
    "        self.anchors = anchors\n",
    "        self.mask = mask\n",
    "        self.classes = classes\n",
    "        self.number_of_mask = len(mask)\n",
    "        self.input_image_size = input_image_size\n",
    "        #self.Sigmoid_layer = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        mask = self.mask\n",
    "        anchors = self.anchors\n",
    "        classes = self.classes\n",
    "        number_of_mask = self.number_of_mask\n",
    "        input_image_size = self.input_image_size\n",
    "        grid_size = int(input_image_size / x.size(2))\n",
    "        #print(grid_size)\n",
    "        #t_x = torch.from_numpy(np.array([0.0 for i in range(number_of_mask * x.size(0) * x.size(2) * x.size(3))]).reshape(number_of_mask * x.size(0), 1, x.size(2), x.size(3)))\n",
    "        t_x = [None for i in range(number_of_mask)]\n",
    "        t_y = [None for i in range(number_of_mask)]\n",
    "        t_w = [None for i in range(number_of_mask)]\n",
    "        t_h = [None for i in range(number_of_mask)]\n",
    "        objective_p = [None for i in range(number_of_mask)]\n",
    "        class_p = [None for i in range(number_of_mask)]\n",
    "        \n",
    "        #c_x = [i for i in range(x.size(2))]\n",
    "        #c_y = [i for i in range(x.size(2))]\n",
    "        \n",
    "        add_on_Matrix_x = torch.from_numpy(np.array([[i for j in range(x.size(2))] for i in range(x.size(2))])).cuda()\n",
    "        add_on_Matrix_y = torch.from_numpy(np.array([[i for i in range(x.size(2))] for j in range(x.size(2))])).cuda()\n",
    "        \n",
    "        b_x = [None for i in range(number_of_mask)]\n",
    "        b_y = [None for i in range(number_of_mask)]\n",
    "        b_w = [None for i in range(number_of_mask)]\n",
    "        b_h = [None for i in range(number_of_mask)]\n",
    "        \n",
    "        \n",
    "        anchor_shape_1 = int(len(anchors) / 2)\n",
    "        anchors = np.array(anchors).reshape(anchor_shape_1, 2)\n",
    "        \n",
    "        print(anchors)\n",
    "        \n",
    "        p_w = [None for i in range(number_of_mask)]\n",
    "        p_h = [None for i in range(number_of_mask)]\n",
    "        \n",
    "        \n",
    "        for i in range(number_of_mask):\n",
    "            start_point = i * (5 + classes)\n",
    "            end_point = (i + 1) * (5 + classes)\n",
    "            p_w[i], p_h[i] = anchors[mask[i]]\n",
    "            print(p_w)\n",
    "            print(p_h)\n",
    "            \n",
    "            t_x[i] = x[:, (start_point + 0) : (start_point + 1), :, :].clone()\n",
    "            \n",
    "            t_y[i] = x[:, (start_point + 1) : (start_point + 2), :, :].clone()\n",
    "            t_w[i] = x[:, (start_point + 2) : (start_point + 3), :, :].clone()\n",
    "            t_h[i] = x[:, (start_point + 3) : (start_point + 4), :, :].clone()\n",
    "            objective_p[i] = x[:, (start_point + 4) : (start_point + 5), :, :].clone()\n",
    "            class_p[i] = x[:, (start_point + 5) : end_point, :, :].clone()\n",
    "            #print(type(t_x[i]))\n",
    "            \n",
    "            print(F.sigmoid(t_x[i].clone()))\n",
    "            b_x[i] = F.sigmoid(t_x[i].clone()) + add_on_Matrix_x\n",
    "            print(b_x[i])\n",
    "            b_y[i] = F.sigmoid(t_y[i].clone()) + add_on_Matrix_y\n",
    "            \n",
    "            #b_x[i][0, 0, :, :] = b_x[i][0, 0, :, :].clone()\n",
    "            #b_y[i][0, 0, :, :] = b_y[i][0, 0, :, :].clone()\n",
    "            \n",
    "            \"\"\"\n",
    "            print(t_x[i].size())\n",
    "            for m in range(x.size(2)):\n",
    "                for n in range(x.size(2)):\n",
    "                    b_x[i][:, :, c_x[m], c_y[n]] = c_x[m] + b_x[i][:, :, c_x[m], c_y[n]].clone()\n",
    "                    b_y[i][:, :, c_x[m], c_y[n]] = c_y[n] + b_y[i][:, :, c_x[m], c_y[n]].clone()\n",
    "            \"\"\"\n",
    "            #need to think whether need to use below 2 lines\n",
    "            b_x[i] = grid_size * b_x[i].clone()\n",
    "            b_y[i] = grid_size * b_y[i].clone()\n",
    "            b_w[i] = p_w[i] * torch.exp(t_w[i].clone())\n",
    "            b_h[i] = p_h[i] * torch.exp(t_h[i].clone())\n",
    "            \n",
    "            objective_p[i] = F.sigmoid(objective_p[i].clone())\n",
    "            class_p[i] = F.sigmoid(class_p[i].clone())\n",
    "            #torch.reshape(t_x[i])\n",
    "        \n",
    "        b_x = torch.stack(b_x).clone()\n",
    "        b_y = torch.stack(b_y).clone()\n",
    "        b_w = torch.stack(b_w).clone()\n",
    "        b_h = torch.stack(b_h).clone()\n",
    "        objective_p = torch.stack(objective_p).clone()\n",
    "        class_p = torch.stack(class_p).clone()\n",
    "        combined_yolo_output = torch.cat((b_x, b_y, b_w, b_h, objective_p, class_p), 2)\n",
    "        #return b_x, b_y, b_w, b_h, objective_p, class_p\n",
    "        #return combined_yolo_output\n",
    "        \n",
    "        \n",
    "        #b_x = torch.stack(b_x).clone()\n",
    "        return combined_yolo_output\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#Yolo_run_layer = Yolo(anchors, mask, classes, input_image_size)\n",
    "\n",
    "#b_x, b_y, b_w, b_h, objective_p, class_p = Yolo_run_layer(Yolo_input)\n",
    "#combined_yolo_output = torch.cat((b_x, b_y, b_w, b_h, objective_p, class_p), 2)\n",
    "\n",
    "Yolo_input = np.array([1.0 for i in range(255 * 76 * 76 * 2)]).reshape(2, 255, 76, 76)\n",
    "Yolo_input = torch.from_numpy(Yolo_input)\n",
    "target = np.array([0 for i in range(3 * 76 * 76 * 85)])\n",
    "input_tensor = Yolo_input\n",
    "output_tensor = torch.Tensor(target)\n",
    "\n",
    "learning_rate = 0.08\n",
    "epoch_size = 5\n",
    "steps_for_printing_out_loss = 1\n",
    "\n",
    "YOLO_Module_WIP = Yolo(anchors, mask, classes, input_image_size)\n",
    "YOLO_Module_WIP.cuda()\n",
    "#Model_WIP.to(device)\n",
    "loss_functioin = nn.MSELoss()\n",
    "optimizer = optim.SGD(YOLO_Module_WIP.parameters(), lr = learning_rate)\n",
    "\n",
    "input = input_tensor.cuda()\n",
    "target = output_tensor.cuda()\n",
    "\n",
    "def training_model():\n",
    "    for i in range(1, epoch_size + 1):\n",
    "        optimizer.zero_grad()\n",
    "        output = YOLO_Module_WIP(input.cuda())\n",
    "        print(output.size())\n",
    "        #b_x, b_y, b_w, b_h, objective_p, class_p = YOLO_v4_Module_WIP(input.cuda())\n",
    "        #output = b_x\n",
    "        loss = loss_functioin(output, target.reshape(output.size(0), output.size(1), output.size(2), output.size(3), output.size(4)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % (steps_for_printing_out_loss) == 0:\n",
    "            print('Loss (epoch: ' + str(i) + '): ' + str(loss.cpu().detach().numpy()))\n",
    "    torch.save({'state_dict': YOLO_v4_Module_WIP.state_dict(),'optimizer': optimizer.state_dict()}, model_file_path)\n",
    "\n",
    "training_model()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_yolo_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n",
      "162\n",
      "{'batch_normalize': '1', 'filters': '64', 'size': '3', 'stride': '2', 'pad': '1', 'activation': 'mish'}\n"
     ]
    }
   ],
   "source": [
    "layer_type, layer_details = read_cfg_file(cfgfile)\n",
    "\n",
    "net_layer = layer_details[0]\n",
    "\n",
    "layer_type = layer_type[1:]\n",
    "layer_details = layer_details[1:]\n",
    "\n",
    "print(len(layer_type))\n",
    "print(len(layer_details))\n",
    "print(layer_details[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nanchor = 3\\n#yolo_layer = 3\\noutput = (19 ** 2) * (1 + 4 + 16)\\nanchor * output\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "anchor = 3\n",
    "#yolo_layer = 3\n",
    "output = (19 ** 2) * (1 + 4 + 16)\n",
    "anchor * output\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build module for entire YOLO\n",
    "class YOLO_v4_model(nn.Module):\n",
    "    def __init__(self, layer_details, layer_type):\n",
    "        super(YOLO_v4_model, self).__init__()\n",
    "        self.all_layers = nn.ModuleList()\n",
    "        all_layers = self.all_layers\n",
    "        self.layer_details = layer_details\n",
    "        self.layer_type = layer_type\n",
    "\n",
    "        for i in range(len(layer_type)):\n",
    "            if layer_type[i] == 'convolutional':\n",
    "                #print(layer_details[i])\n",
    "                #print(i)\n",
    "                try:\n",
    "                    if int(layer_details[i]['batch_normalize']) == 1:\n",
    "                        batch_normalize = True\n",
    "                    else:\n",
    "                        batch_normalize = False\n",
    "                except:\n",
    "                    batch_normalize = False\n",
    "                if i == 0:\n",
    "                    in_channel = 3\n",
    "                else:\n",
    "                    if layer_type[i - 1] == 'convolutional':\n",
    "                        skip_step = 0\n",
    "                    elif layer_type[i - 1] == 'shortcut':\n",
    "                        skip_step = int(layer_details[i - 1]['from'])\n",
    "                    elif layer_type[i - 1] == 'route':\n",
    "                        try:\n",
    "                            skip_step = int(layer_details[i - 1]['layers'].split(\",\")[0])\n",
    "                        except:\n",
    "                            skip_step = int(layer_details[i - 1]['layers'])\n",
    "                    #print(layer_details[i - 1 + skip_step])\n",
    "                    if skip_step > 0:\n",
    "                        in_channel = int(layer_details[skip_step]['filters'])\n",
    "                    else:\n",
    "                        in_channel = int(layer_details[i - 1 + skip_step]['filters'])\n",
    "                out_channel = int(layer_details[i]['filters'])\n",
    "                kernel_size = int(layer_details[i]['size'])\n",
    "                stride = int(layer_details[i]['stride'])\n",
    "                pad = int(layer_details[i]['pad'])\n",
    "                activation_func = layer_details[i]['activation']\n",
    "                layer = Conv_Layer_box(in_channel, out_channel, kernel_size, stride, activation_func, batch_normalize)\n",
    "                #print(layer)\n",
    "            elif layer_type[i] == 'maxpool':\n",
    "                layer_details[i].update([('filters', layer_details[i - 1]['filters'])])\n",
    "                maxpool_size = int(layer_details[i]['size'])\n",
    "                #print(maxpool_size)\n",
    "                layer = Maxpool_pad_Layer_box(maxpool_size)\n",
    "                #print(layer)\n",
    "            elif layer_type[i] == 'upsample':\n",
    "                layer_details[i].update([('filters', layer_details[i - 1]['filters'])])\n",
    "                stride = int(layer_details[i]['stride'])\n",
    "                layer = Upsample_layer(stride)\n",
    "            elif layer_type[i] == 'yolo':\n",
    "                #print(\"yolo\")\n",
    "                anchors = [int(x) for x in layer_details[i]['anchors'].split(\",\")]\n",
    "                \n",
    "                mask = [int(x) for x in layer_details[i]['mask'].split(\",\")]\n",
    "                classes = int(layer_details[i]['classes'])\n",
    "                #input image size = 608 for now\n",
    "                layer = Yolo(anchors, mask, classes, input_image_size)\n",
    "                #print(anchors)\n",
    "                #print(classes)\n",
    "                #print(input_image_size)\n",
    "                #continue\n",
    "            elif layer_type[i] == 'shortcut':\n",
    "                skip_step = int(layer_details[i]['from'])\n",
    "                layer_details[i].update([('filters', layer_details[i + skip_step]['filters'])])\n",
    "                layer = shortcut()\n",
    "            elif layer_type[i] == 'route':\n",
    "                try:\n",
    "                    skip_step = int(layer_details[i]['layers'].split(\",\")[0])\n",
    "                except:\n",
    "                    skip_step = int(layer_details[i]['layers'])\n",
    "                #print(skip_step)\n",
    "                if skip_step > 0:\n",
    "                    layer_details[i].update([('filters', layer_details[skip_step]['filters'])])\n",
    "                else:\n",
    "                    layer_details[i].update([('filters', layer_details[i + skip_step]['filters'])])\n",
    "                layer = route()\n",
    "            elif layer_type[i] == 'net':\n",
    "                #print(\"net\")\n",
    "                continue\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            all_layers.append(layer)\n",
    "        global all_layerrr\n",
    "        all_layerrr = all_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        all_layers = self.all_layers\n",
    "        layers_output = [None for i in range(len(layer_type))]\n",
    "        for i in range(len(layer_type)):\n",
    "            #print(i)\n",
    "            if i == 0:\n",
    "                layers_output[i] = all_layers[i](x)\n",
    "                continue\n",
    "                \n",
    "            elif layer_type[i] == 'yolo':\n",
    "                layers_output[i] = all_layers[i](layers_output[i - 1])\n",
    "                continue\n",
    "            elif layer_type[i] == 'convolutional' or layer_type[i] == 'maxpool' or layer_type[i] == 'upsample' or layer_type[i] == 'yolo':\n",
    "                layers_output[i] = all_layers[i](layers_output[i - 1])\n",
    "                \"\"\"\n",
    "                try:\n",
    "                    print(\"i: \" + str(i) + str(layers_output[i].size()))\n",
    "                except:\n",
    "                    print(\"go\")\n",
    "                \"\"\"\n",
    "                continue\n",
    "            elif layer_type[i] == 'shortcut':\n",
    "                skip_step = [int(layer_details[i]['from'])]\n",
    "            elif layer_type[i] == 'route':\n",
    "                skip_step = layer_details[i]['layers'].split(\",\")\n",
    "            for SS in skip_step:\n",
    "                SS = int(SS)\n",
    "                #print(\"SS\" + str(i) + str(SS))\n",
    "                #print(skip_step)\n",
    "                \n",
    "                #print(SS)\n",
    "                #print(layers_output[i])\n",
    "                #print(layers_output[i - 1 + SS])\n",
    "                \n",
    "                if SS > 0:\n",
    "                    if layers_output[i] == None:\n",
    "                        #print(layers_output[SS].size())\n",
    "                        layers_output[i] = layers_output[SS]\n",
    "                    else:\n",
    "                        #print(layers_output[SS].size())\n",
    "                        layers_output[i] += layers_output[SS]\n",
    "                else:\n",
    "                    #print(i + SS)\n",
    "                    if layers_output[i] == None:\n",
    "                        #print(layers_output[i + SS].size())\n",
    "                        layers_output[i] = layers_output[i + SS]\n",
    "                    else:\n",
    "                        \n",
    "                        #print(layers_output[i + SS].size())\n",
    "                        layers_output[i] += layers_output[i + SS]\n",
    "            \"\"\"\n",
    "            try:\n",
    "                print(\"i: \" + str(i) + str(layers_output[i].size()))\n",
    "            except:\n",
    "                print(\"go\")\n",
    "            \"\"\"\n",
    "        #print(layers_output[138].size())\n",
    "        return layers_output[139]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12  16]\n",
      " [ 19  36]\n",
      " [ 40  28]\n",
      " [ 36  75]\n",
      " [ 76  55]\n",
      " [ 72 146]\n",
      " [142 110]\n",
      " [192 243]\n",
      " [459 401]]\n",
      "[12, None, None]\n",
      "[16, None, None]\n",
      "tensor([[[[0.5000, 0.5000, 0.7773,  ..., 0.5718, 0.8050, 0.7918],\n",
      "          [0.5000, 0.9336, 0.6814,  ..., 0.5000, 0.6900, 0.7743],\n",
      "          [0.6571, 0.8250, 0.8624,  ..., 0.6731, 0.8474, 0.5000],\n",
      "          ...,\n",
      "          [0.5000, 0.6016, 0.7034,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          [0.5819, 0.8405, 0.8190,  ..., 0.6339, 0.9452, 0.5748],\n",
      "          [0.5224, 0.6646, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.5000,  0.5000,  0.7773,  ...,  0.5718,  0.8050,  0.7918],\n",
      "          [ 1.5000,  1.9336,  1.6814,  ...,  1.5000,  1.6900,  1.7743],\n",
      "          [ 2.6571,  2.8250,  2.8624,  ...,  2.6731,  2.8474,  2.5000],\n",
      "          ...,\n",
      "          [73.5000, 73.6016, 73.7034,  ..., 73.5000, 73.5000, 73.5000],\n",
      "          [74.5819, 74.8405, 74.8190,  ..., 74.6339, 74.9452, 74.5748],\n",
      "          [75.5224, 75.6646, 75.5000,  ..., 75.5000, 75.5000, 75.5000]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "[12, 19, None]\n",
      "[16, 36, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.5000, 0.9151, 0.5000,  ..., 0.5461, 0.7693, 0.5000],\n",
      "          [0.5889, 0.8020, 0.5000,  ..., 0.8766, 0.5000, 0.6754],\n",
      "          [0.6200, 0.5000, 0.5000,  ..., 0.5366, 0.5000, 0.5000],\n",
      "          ...,\n",
      "          [0.7430, 0.6894, 0.5113,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          [0.7240, 0.6547, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          [0.9344, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.6976]]]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.5000,  0.9151,  0.5000,  ...,  0.5461,  0.7693,  0.5000],\n",
      "          [ 1.5889,  1.8020,  1.5000,  ...,  1.8766,  1.5000,  1.6754],\n",
      "          [ 2.6200,  2.5000,  2.5000,  ...,  2.5366,  2.5000,  2.5000],\n",
      "          ...,\n",
      "          [73.7430, 73.6894, 73.5113,  ..., 73.5000, 73.5000, 73.5000],\n",
      "          [74.7240, 74.6546, 74.5000,  ..., 74.5000, 74.5000, 74.5000],\n",
      "          [75.9344, 75.5000, 75.5000,  ..., 75.5000, 75.5000, 75.6976]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "[12, 19, 40]\n",
      "[16, 36, 28]\n",
      "tensor([[[[0.8616, 0.8029, 0.5000,  ..., 0.7764, 0.5720, 0.7352],\n",
      "          [0.8106, 0.8909, 0.5000,  ..., 0.7620, 0.5000, 0.6848],\n",
      "          [0.9391, 0.5000, 0.6192,  ..., 0.7666, 0.7313, 0.8826],\n",
      "          ...,\n",
      "          [0.9168, 0.7553, 0.5465,  ..., 0.6856, 0.5000, 0.5000],\n",
      "          [0.8622, 0.8897, 0.9017,  ..., 0.5000, 0.5265, 0.8563],\n",
      "          [0.8156, 0.8336, 0.5000,  ..., 0.5553, 0.5000, 0.5273]]]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.8616,  0.8029,  0.5000,  ...,  0.7764,  0.5720,  0.7352],\n",
      "          [ 1.8106,  1.8909,  1.5000,  ...,  1.7620,  1.5000,  1.6848],\n",
      "          [ 2.9391,  2.5000,  2.6192,  ...,  2.7666,  2.7313,  2.8826],\n",
      "          ...,\n",
      "          [73.9168, 73.7553, 73.5465,  ..., 73.6856, 73.5000, 73.5000],\n",
      "          [74.8622, 74.8897, 74.9017,  ..., 74.5000, 74.5265, 74.8563],\n",
      "          [75.8156, 75.8336, 75.5000,  ..., 75.5553, 75.5000, 75.5273]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "[[ 12  16]\n",
      " [ 19  36]\n",
      " [ 40  28]\n",
      " [ 36  75]\n",
      " [ 76  55]\n",
      " [ 72 146]\n",
      " [142 110]\n",
      " [192 243]\n",
      " [459 401]]\n",
      "[36, None, None]\n",
      "[75, None, None]\n",
      "tensor([[[[0.5000, 0.6038, 0.5426,  ..., 0.5000, 0.5157, 0.6228],\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5006, 0.5000],\n",
      "          [0.6272, 0.8287, 0.5000,  ..., 0.5623, 0.5000, 0.5546],\n",
      "          ...,\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.8178, 0.5000],\n",
      "          [0.5000, 0.5000, 0.8000,  ..., 0.5738, 0.5000, 0.5000],\n",
      "          [0.5440, 0.7644, 0.6175,  ..., 0.5000, 0.6274, 0.5978]]]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.5000,  0.6038,  0.5426,  ...,  0.5000,  0.5157,  0.6228],\n",
      "          [ 1.5000,  1.5000,  1.5000,  ...,  1.5000,  1.5006,  1.5000],\n",
      "          [ 2.6272,  2.8287,  2.5000,  ...,  2.5623,  2.5000,  2.5546],\n",
      "          ...,\n",
      "          [35.5000, 35.5000, 35.5000,  ..., 35.5000, 35.8177, 35.5000],\n",
      "          [36.5000, 36.5000, 36.8000,  ..., 36.5738, 36.5000, 36.5000],\n",
      "          [37.5440, 37.7644, 37.6175,  ..., 37.5000, 37.6274, 37.5978]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "[36, 76, None]\n",
      "[75, 55, None]\n",
      "tensor([[[[0.8870, 0.7889, 0.6915,  ..., 0.6535, 0.7457, 0.5000],\n",
      "          [0.8813, 0.8733, 0.6425,  ..., 0.6048, 0.8771, 0.5000],\n",
      "          [0.8767, 0.8472, 0.5000,  ..., 0.5744, 0.7621, 0.5000],\n",
      "          ...,\n",
      "          [0.8802, 0.6424, 0.5000,  ..., 0.8550, 0.8594, 0.5000],\n",
      "          [0.7848, 0.6301, 0.5000,  ..., 0.8103, 0.7935, 0.5000],\n",
      "          [0.6447, 0.5000, 0.5120,  ..., 0.5698, 0.5000, 0.5000]]]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.8870,  0.7889,  0.6915,  ...,  0.6535,  0.7457,  0.5000],\n",
      "          [ 1.8813,  1.8733,  1.6425,  ...,  1.6048,  1.8771,  1.5000],\n",
      "          [ 2.8767,  2.8472,  2.5000,  ...,  2.5744,  2.7621,  2.5000],\n",
      "          ...,\n",
      "          [35.8802, 35.6424, 35.5000,  ..., 35.8550, 35.8594, 35.5000],\n",
      "          [36.7848, 36.6301, 36.5000,  ..., 36.8103, 36.7935, 36.5000],\n",
      "          [37.6447, 37.5000, 37.5120,  ..., 37.5698, 37.5000, 37.5000]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "[36, 76, 72]\n",
      "[75, 55, 146]\n",
      "tensor([[[[0.7198, 0.6635, 0.6141,  ..., 0.5000, 0.7548, 0.5048],\n",
      "          [0.6696, 0.5000, 0.6754,  ..., 0.7377, 0.5914, 0.7637],\n",
      "          [0.7734, 0.5000, 0.8650,  ..., 0.6763, 0.8029, 0.6740],\n",
      "          ...,\n",
      "          [0.7543, 0.6984, 0.7888,  ..., 0.8249, 0.7387, 0.8609],\n",
      "          [0.7061, 0.7980, 0.8520,  ..., 0.6397, 0.8409, 0.9170],\n",
      "          [0.8746, 0.6611, 0.7184,  ..., 0.5797, 0.5313, 0.7473]]]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.7198,  0.6635,  0.6141,  ...,  0.5000,  0.7548,  0.5048],\n",
      "          [ 1.6696,  1.5000,  1.6754,  ...,  1.7377,  1.5914,  1.7637],\n",
      "          [ 2.7734,  2.5000,  2.8650,  ...,  2.6763,  2.8029,  2.6740],\n",
      "          ...,\n",
      "          [35.7543, 35.6984, 35.7888,  ..., 35.8249, 35.7387, 35.8609],\n",
      "          [36.7061, 36.7980, 36.8520,  ..., 36.6397, 36.8409, 36.9170],\n",
      "          [37.8746, 37.6611, 37.7184,  ..., 37.5797, 37.5313, 37.7473]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "[[ 12  16]\n",
      " [ 19  36]\n",
      " [ 40  28]\n",
      " [ 36  75]\n",
      " [ 76  55]\n",
      " [ 72 146]\n",
      " [142 110]\n",
      " [192 243]\n",
      " [459 401]]\n",
      "[142, None, None]\n",
      "[110, None, None]\n",
      "tensor([[[[0.7328, 0.8002, 0.7976, 0.8822, 0.7515, 0.6891, 0.7443, 0.6876,\n",
      "           0.6754, 0.7122, 0.6729, 0.6188, 0.6296, 0.7222, 0.6817, 0.6752,\n",
      "           0.8551, 0.7629, 0.7790],\n",
      "          [0.7769, 0.8326, 0.7940, 0.8365, 0.8158, 0.6843, 0.7323, 0.6848,\n",
      "           0.6301, 0.7633, 0.7715, 0.7393, 0.6772, 0.6007, 0.8379, 0.6889,\n",
      "           0.6374, 0.8369, 0.7861],\n",
      "          [0.5561, 0.7955, 0.9350, 0.8546, 0.8617, 0.7431, 0.7407, 0.7555,\n",
      "           0.5453, 0.6235, 0.6566, 0.5470, 0.5110, 0.5442, 0.6161, 0.8753,\n",
      "           0.8327, 0.8881, 0.7297],\n",
      "          [0.5000, 0.5000, 0.7142, 0.8320, 0.7485, 0.7988, 0.7488, 0.5199,\n",
      "           0.5281, 0.6971, 0.6325, 0.6186, 0.7250, 0.5513, 0.6010, 0.7929,\n",
      "           0.7342, 0.7955, 0.8651],\n",
      "          [0.6109, 0.5759, 0.8300, 0.7627, 0.7406, 0.7681, 0.6252, 0.5785,\n",
      "           0.5506, 0.5271, 0.6204, 0.7522, 0.6297, 0.5647, 0.5350, 0.7502,\n",
      "           0.7315, 0.7453, 0.7086],\n",
      "          [0.5387, 0.5874, 0.7041, 0.8043, 0.7457, 0.6389, 0.5000, 0.6518,\n",
      "           0.6871, 0.6403, 0.6887, 0.6784, 0.5822, 0.6745, 0.6651, 0.7219,\n",
      "           0.5349, 0.6246, 0.6923],\n",
      "          [0.5916, 0.6499, 0.5411, 0.6259, 0.7031, 0.7237, 0.5966, 0.6154,\n",
      "           0.7095, 0.6381, 0.7134, 0.6810, 0.6106, 0.6997, 0.5412, 0.5904,\n",
      "           0.6125, 0.6736, 0.6050],\n",
      "          [0.5797, 0.5336, 0.6917, 0.8293, 0.7337, 0.6090, 0.6237, 0.5866,\n",
      "           0.7114, 0.7580, 0.6375, 0.5905, 0.6232, 0.7197, 0.6645, 0.6384,\n",
      "           0.6214, 0.6435, 0.6704],\n",
      "          [0.5813, 0.5959, 0.7065, 0.6992, 0.6684, 0.7296, 0.7178, 0.5827,\n",
      "           0.6781, 0.8058, 0.6970, 0.6336, 0.7100, 0.6503, 0.7143, 0.6314,\n",
      "           0.7337, 0.7128, 0.5880],\n",
      "          [0.6339, 0.6974, 0.6514, 0.7602, 0.5877, 0.6200, 0.6442, 0.6577,\n",
      "           0.7082, 0.8164, 0.7500, 0.6135, 0.5771, 0.6669, 0.5534, 0.6737,\n",
      "           0.6296, 0.7227, 0.6829],\n",
      "          [0.6209, 0.7196, 0.6957, 0.5000, 0.5920, 0.6135, 0.5748, 0.7224,\n",
      "           0.8056, 0.6946, 0.6349, 0.6081, 0.6512, 0.6487, 0.6431, 0.6084,\n",
      "           0.6441, 0.6907, 0.5838],\n",
      "          [0.6047, 0.5375, 0.7185, 0.6518, 0.5859, 0.5000, 0.5000, 0.5793,\n",
      "           0.6374, 0.6722, 0.7876, 0.6249, 0.6761, 0.6208, 0.5930, 0.7452,\n",
      "           0.7576, 0.6366, 0.6471],\n",
      "          [0.6219, 0.6529, 0.6992, 0.5728, 0.6176, 0.5111, 0.5544, 0.5000,\n",
      "           0.5542, 0.6558, 0.6492, 0.7177, 0.6330, 0.5305, 0.6257, 0.7432,\n",
      "           0.6926, 0.5511, 0.6370],\n",
      "          [0.6010, 0.5685, 0.7294, 0.6279, 0.6534, 0.6171, 0.6632, 0.6278,\n",
      "           0.6166, 0.6372, 0.6840, 0.7392, 0.6210, 0.6603, 0.6553, 0.6632,\n",
      "           0.6073, 0.7737, 0.6512],\n",
      "          [0.5698, 0.6030, 0.8163, 0.7081, 0.7097, 0.5762, 0.5424, 0.6030,\n",
      "           0.5728, 0.6917, 0.6380, 0.6593, 0.6915, 0.6435, 0.6594, 0.6592,\n",
      "           0.7719, 0.8610, 0.7243],\n",
      "          [0.5000, 0.8256, 0.6901, 0.7034, 0.8070, 0.6099, 0.5660, 0.6187,\n",
      "           0.7172, 0.6529, 0.6411, 0.6010, 0.6562, 0.7027, 0.7101, 0.7476,\n",
      "           0.7726, 0.7749, 0.8816],\n",
      "          [0.6054, 0.8272, 0.8184, 0.7763, 0.6224, 0.6644, 0.6443, 0.6693,\n",
      "           0.7485, 0.7500, 0.5400, 0.6270, 0.5673, 0.6211, 0.7077, 0.6697,\n",
      "           0.7520, 0.8331, 0.8599],\n",
      "          [0.6177, 0.7019, 0.8010, 0.8009, 0.8122, 0.6753, 0.6953, 0.5373,\n",
      "           0.6353, 0.7144, 0.7177, 0.5659, 0.6972, 0.7592, 0.5732, 0.6779,\n",
      "           0.7733, 0.7855, 0.7611],\n",
      "          [0.6895, 0.7440, 0.5880, 0.7127, 0.7724, 0.7547, 0.6484, 0.6114,\n",
      "           0.6641, 0.5960, 0.6437, 0.7026, 0.6620, 0.7079, 0.6965, 0.7824,\n",
      "           0.7512, 0.7434, 0.6183]]]], device='cuda:0',\n",
      "       grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.7328,  0.8002,  0.7976,  0.8822,  0.7515,  0.6891,  0.7443,\n",
      "            0.6876,  0.6754,  0.7122,  0.6729,  0.6188,  0.6296,  0.7222,\n",
      "            0.6817,  0.6752,  0.8551,  0.7629,  0.7790],\n",
      "          [ 1.7769,  1.8326,  1.7940,  1.8365,  1.8158,  1.6843,  1.7323,\n",
      "            1.6848,  1.6301,  1.7633,  1.7715,  1.7393,  1.6772,  1.6007,\n",
      "            1.8379,  1.6889,  1.6374,  1.8369,  1.7861],\n",
      "          [ 2.5561,  2.7955,  2.9350,  2.8546,  2.8617,  2.7431,  2.7407,\n",
      "            2.7555,  2.5453,  2.6235,  2.6566,  2.5470,  2.5110,  2.5442,\n",
      "            2.6161,  2.8753,  2.8327,  2.8881,  2.7297],\n",
      "          [ 3.5000,  3.5000,  3.7142,  3.8320,  3.7485,  3.7988,  3.7488,\n",
      "            3.5199,  3.5281,  3.6971,  3.6325,  3.6186,  3.7250,  3.5513,\n",
      "            3.6010,  3.7929,  3.7342,  3.7955,  3.8651],\n",
      "          [ 4.6109,  4.5759,  4.8300,  4.7627,  4.7406,  4.7681,  4.6252,\n",
      "            4.5785,  4.5506,  4.5271,  4.6204,  4.7522,  4.6297,  4.5647,\n",
      "            4.5350,  4.7502,  4.7315,  4.7453,  4.7086],\n",
      "          [ 5.5387,  5.5874,  5.7041,  5.8043,  5.7457,  5.6389,  5.5000,\n",
      "            5.6518,  5.6871,  5.6403,  5.6887,  5.6784,  5.5822,  5.6745,\n",
      "            5.6651,  5.7219,  5.5349,  5.6246,  5.6923],\n",
      "          [ 6.5916,  6.6499,  6.5411,  6.6259,  6.7031,  6.7237,  6.5966,\n",
      "            6.6154,  6.7095,  6.6381,  6.7134,  6.6810,  6.6106,  6.6997,\n",
      "            6.5412,  6.5904,  6.6125,  6.6736,  6.6050],\n",
      "          [ 7.5797,  7.5336,  7.6917,  7.8293,  7.7337,  7.6090,  7.6237,\n",
      "            7.5866,  7.7114,  7.7580,  7.6375,  7.5905,  7.6232,  7.7197,\n",
      "            7.6645,  7.6384,  7.6214,  7.6435,  7.6704],\n",
      "          [ 8.5813,  8.5959,  8.7065,  8.6992,  8.6684,  8.7296,  8.7178,\n",
      "            8.5827,  8.6781,  8.8058,  8.6970,  8.6336,  8.7100,  8.6503,\n",
      "            8.7143,  8.6314,  8.7337,  8.7128,  8.5880],\n",
      "          [ 9.6339,  9.6974,  9.6514,  9.7602,  9.5877,  9.6200,  9.6442,\n",
      "            9.6577,  9.7082,  9.8164,  9.7500,  9.6135,  9.5771,  9.6669,\n",
      "            9.5534,  9.6737,  9.6296,  9.7227,  9.6829],\n",
      "          [10.6209, 10.7196, 10.6957, 10.5000, 10.5920, 10.6135, 10.5748,\n",
      "           10.7224, 10.8056, 10.6946, 10.6349, 10.6081, 10.6512, 10.6487,\n",
      "           10.6431, 10.6084, 10.6441, 10.6907, 10.5838],\n",
      "          [11.6047, 11.5375, 11.7185, 11.6518, 11.5859, 11.5000, 11.5000,\n",
      "           11.5793, 11.6374, 11.6722, 11.7876, 11.6249, 11.6761, 11.6208,\n",
      "           11.5930, 11.7452, 11.7576, 11.6366, 11.6471],\n",
      "          [12.6219, 12.6529, 12.6992, 12.5728, 12.6176, 12.5111, 12.5544,\n",
      "           12.5000, 12.5542, 12.6558, 12.6492, 12.7177, 12.6330, 12.5305,\n",
      "           12.6257, 12.7432, 12.6926, 12.5511, 12.6370],\n",
      "          [13.6010, 13.5685, 13.7294, 13.6279, 13.6534, 13.6171, 13.6632,\n",
      "           13.6278, 13.6166, 13.6372, 13.6840, 13.7392, 13.6210, 13.6603,\n",
      "           13.6553, 13.6632, 13.6073, 13.7737, 13.6512],\n",
      "          [14.5698, 14.6030, 14.8163, 14.7081, 14.7097, 14.5762, 14.5424,\n",
      "           14.6030, 14.5728, 14.6917, 14.6380, 14.6593, 14.6915, 14.6435,\n",
      "           14.6594, 14.6592, 14.7719, 14.8610, 14.7243],\n",
      "          [15.5000, 15.8256, 15.6901, 15.7034, 15.8070, 15.6099, 15.5660,\n",
      "           15.6187, 15.7172, 15.6529, 15.6411, 15.6010, 15.6562, 15.7027,\n",
      "           15.7101, 15.7476, 15.7726, 15.7749, 15.8816],\n",
      "          [16.6054, 16.8272, 16.8184, 16.7763, 16.6224, 16.6644, 16.6443,\n",
      "           16.6693, 16.7485, 16.7500, 16.5400, 16.6270, 16.5673, 16.6211,\n",
      "           16.7077, 16.6697, 16.7520, 16.8331, 16.8599],\n",
      "          [17.6177, 17.7019, 17.8010, 17.8009, 17.8122, 17.6753, 17.6952,\n",
      "           17.5373, 17.6353, 17.7144, 17.7177, 17.5659, 17.6972, 17.7592,\n",
      "           17.5732, 17.6779, 17.7733, 17.7855, 17.7611],\n",
      "          [18.6895, 18.7440, 18.5880, 18.7127, 18.7724, 18.7547, 18.6484,\n",
      "           18.6114, 18.6641, 18.5960, 18.6437, 18.7026, 18.6620, 18.7079,\n",
      "           18.6965, 18.7824, 18.7512, 18.7434, 18.6183]]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "[142, 192, None]\n",
      "[110, 243, None]\n",
      "tensor([[[[0.6406, 0.6165, 0.5000, 0.5013, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5775, 0.5000, 0.5000, 0.5985, 0.5407,\n",
      "           0.5699, 0.5000, 0.5000],\n",
      "          [0.5834, 0.5821, 0.8067, 0.7683, 0.6800, 0.5000, 0.5437, 0.5000,\n",
      "           0.5000, 0.5000, 0.5297, 0.5831, 0.5000, 0.5002, 0.5446, 0.7222,\n",
      "           0.5000, 0.5418, 0.5000],\n",
      "          [0.6192, 0.5000, 0.8365, 0.7078, 0.7953, 0.5000, 0.5382, 0.5052,\n",
      "           0.6035, 0.6260, 0.6077, 0.7259, 0.5619, 0.5000, 0.7822, 0.7118,\n",
      "           0.6823, 0.5000, 0.5000],\n",
      "          [0.5074, 0.5712, 0.7110, 0.7381, 0.6128, 0.5698, 0.6173, 0.5443,\n",
      "           0.5509, 0.5796, 0.6083, 0.5289, 0.5000, 0.6287, 0.5621, 0.6504,\n",
      "           0.6589, 0.5000, 0.5056],\n",
      "          [0.6579, 0.6111, 0.5837, 0.7109, 0.6906, 0.5100, 0.5058, 0.5000,\n",
      "           0.5000, 0.5624, 0.5000, 0.5524, 0.5000, 0.6176, 0.5718, 0.5000,\n",
      "           0.5725, 0.7360, 0.5000],\n",
      "          [0.5026, 0.5000, 0.5000, 0.5143, 0.5204, 0.5671, 0.5870, 0.5000,\n",
      "           0.5000, 0.5848, 0.5028, 0.5000, 0.5000, 0.5548, 0.5131, 0.6016,\n",
      "           0.5000, 0.6752, 0.5518],\n",
      "          [0.5851, 0.5315, 0.5299, 0.5000, 0.6226, 0.5258, 0.5000, 0.5000,\n",
      "           0.6086, 0.5920, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5104],\n",
      "          [0.5209, 0.5333, 0.7076, 0.5715, 0.6013, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.6481, 0.5000, 0.5345, 0.5000, 0.5080, 0.5849, 0.5094,\n",
      "           0.5531, 0.5000, 0.5000],\n",
      "          [0.5268, 0.5345, 0.5928, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5245, 0.5523, 0.5000, 0.5069, 0.5086, 0.5365, 0.5479,\n",
      "           0.6344, 0.6431, 0.5161],\n",
      "          [0.5567, 0.5602, 0.7664, 0.5549, 0.5000, 0.6287, 0.5475, 0.5531,\n",
      "           0.6418, 0.5449, 0.5405, 0.6044, 0.5000, 0.5943, 0.5640, 0.5185,\n",
      "           0.5300, 0.5799, 0.5721],\n",
      "          [0.5629, 0.5313, 0.5373, 0.5000, 0.5000, 0.5000, 0.5000, 0.5632,\n",
      "           0.5474, 0.6032, 0.5950, 0.5896, 0.5000, 0.6307, 0.6681, 0.5000,\n",
      "           0.6302, 0.5501, 0.5000],\n",
      "          [0.5958, 0.5477, 0.5353, 0.5159, 0.6449, 0.5000, 0.5000, 0.5046,\n",
      "           0.5000, 0.5916, 0.5000, 0.5143, 0.5017, 0.5000, 0.5000, 0.5670,\n",
      "           0.6243, 0.5023, 0.5000],\n",
      "          [0.5967, 0.5000, 0.5523, 0.5000, 0.5336, 0.6254, 0.5390, 0.5000,\n",
      "           0.5000, 0.5161, 0.5000, 0.5128, 0.5000, 0.5173, 0.5000, 0.5074,\n",
      "           0.5000, 0.5261, 0.5282],\n",
      "          [0.6398, 0.5000, 0.5000, 0.5000, 0.5148, 0.5004, 0.5000, 0.5000,\n",
      "           0.5000, 0.5077, 0.5273, 0.5742, 0.5000, 0.5000, 0.5060, 0.6301,\n",
      "           0.5000, 0.6031, 0.5669],\n",
      "          [0.5000, 0.5561, 0.5424, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5168, 0.5862, 0.6035, 0.5744, 0.5078, 0.5584,\n",
      "           0.5463, 0.5905, 0.5000],\n",
      "          [0.6274, 0.5000, 0.5366, 0.5000, 0.6605, 0.5083, 0.5000, 0.5000,\n",
      "           0.5000, 0.5218, 0.5266, 0.5677, 0.5952, 0.6262, 0.5000, 0.5350,\n",
      "           0.5000, 0.7263, 0.5287],\n",
      "          [0.5000, 0.6271, 0.7133, 0.5971, 0.5653, 0.5236, 0.5285, 0.5000,\n",
      "           0.5246, 0.5923, 0.5875, 0.6109, 0.5100, 0.5075, 0.5000, 0.5000,\n",
      "           0.6813, 0.7436, 0.5839],\n",
      "          [0.6769, 0.5690, 0.5986, 0.5000, 0.6293, 0.5327, 0.5000, 0.5022,\n",
      "           0.5000, 0.5192, 0.5000, 0.5605, 0.6373, 0.5480, 0.5800, 0.6081,\n",
      "           0.5738, 0.5026, 0.5000],\n",
      "          [0.5711, 0.6829, 0.5713, 0.5924, 0.5000, 0.5464, 0.5000, 0.5487,\n",
      "           0.5809, 0.5645, 0.5619, 0.5198, 0.5561, 0.5021, 0.5864, 0.5853,\n",
      "           0.6214, 0.6144, 0.5180]]]], device='cuda:0',\n",
      "       grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.6406,  0.6165,  0.5000,  0.5013,  0.5000,  0.5000,  0.5000,\n",
      "            0.5000,  0.5000,  0.5000,  0.5000,  0.5775,  0.5000,  0.5000,\n",
      "            0.5985,  0.5407,  0.5699,  0.5000,  0.5000],\n",
      "          [ 1.5834,  1.5821,  1.8067,  1.7683,  1.6800,  1.5000,  1.5437,\n",
      "            1.5000,  1.5000,  1.5000,  1.5297,  1.5831,  1.5000,  1.5002,\n",
      "            1.5446,  1.7222,  1.5000,  1.5418,  1.5000],\n",
      "          [ 2.6192,  2.5000,  2.8365,  2.7078,  2.7953,  2.5000,  2.5382,\n",
      "            2.5052,  2.6035,  2.6260,  2.6077,  2.7259,  2.5619,  2.5000,\n",
      "            2.7822,  2.7118,  2.6823,  2.5000,  2.5000],\n",
      "          [ 3.5074,  3.5712,  3.7110,  3.7381,  3.6128,  3.5698,  3.6173,\n",
      "            3.5443,  3.5509,  3.5796,  3.6083,  3.5289,  3.5000,  3.6287,\n",
      "            3.5621,  3.6504,  3.6589,  3.5000,  3.5056],\n",
      "          [ 4.6579,  4.6111,  4.5837,  4.7109,  4.6906,  4.5100,  4.5058,\n",
      "            4.5000,  4.5000,  4.5624,  4.5000,  4.5524,  4.5000,  4.6176,\n",
      "            4.5718,  4.5000,  4.5725,  4.7360,  4.5000],\n",
      "          [ 5.5026,  5.5000,  5.5000,  5.5143,  5.5204,  5.5671,  5.5870,\n",
      "            5.5000,  5.5000,  5.5848,  5.5028,  5.5000,  5.5000,  5.5548,\n",
      "            5.5131,  5.6016,  5.5000,  5.6752,  5.5518],\n",
      "          [ 6.5851,  6.5315,  6.5299,  6.5000,  6.6226,  6.5258,  6.5000,\n",
      "            6.5000,  6.6086,  6.5920,  6.5000,  6.5000,  6.5000,  6.5000,\n",
      "            6.5000,  6.5000,  6.5000,  6.5000,  6.5104],\n",
      "          [ 7.5209,  7.5333,  7.7076,  7.5715,  7.6013,  7.5000,  7.5000,\n",
      "            7.5000,  7.5000,  7.6481,  7.5000,  7.5345,  7.5000,  7.5080,\n",
      "            7.5849,  7.5094,  7.5531,  7.5000,  7.5000],\n",
      "          [ 8.5268,  8.5345,  8.5928,  8.5000,  8.5000,  8.5000,  8.5000,\n",
      "            8.5000,  8.5000,  8.5245,  8.5523,  8.5000,  8.5069,  8.5086,\n",
      "            8.5365,  8.5479,  8.6344,  8.6431,  8.5161],\n",
      "          [ 9.5567,  9.5602,  9.7664,  9.5549,  9.5000,  9.6287,  9.5475,\n",
      "            9.5531,  9.6418,  9.5449,  9.5405,  9.6044,  9.5000,  9.5943,\n",
      "            9.5640,  9.5185,  9.5300,  9.5799,  9.5721],\n",
      "          [10.5629, 10.5313, 10.5373, 10.5000, 10.5000, 10.5000, 10.5000,\n",
      "           10.5632, 10.5474, 10.6032, 10.5950, 10.5896, 10.5000, 10.6307,\n",
      "           10.6681, 10.5000, 10.6302, 10.5501, 10.5000],\n",
      "          [11.5958, 11.5477, 11.5353, 11.5159, 11.6449, 11.5000, 11.5000,\n",
      "           11.5046, 11.5000, 11.5916, 11.5000, 11.5143, 11.5017, 11.5000,\n",
      "           11.5000, 11.5670, 11.6243, 11.5023, 11.5000],\n",
      "          [12.5967, 12.5000, 12.5523, 12.5000, 12.5336, 12.6254, 12.5390,\n",
      "           12.5000, 12.5000, 12.5161, 12.5000, 12.5128, 12.5000, 12.5173,\n",
      "           12.5000, 12.5074, 12.5000, 12.5261, 12.5282],\n",
      "          [13.6398, 13.5000, 13.5000, 13.5000, 13.5148, 13.5004, 13.5000,\n",
      "           13.5000, 13.5000, 13.5077, 13.5273, 13.5742, 13.5000, 13.5000,\n",
      "           13.5060, 13.6301, 13.5000, 13.6031, 13.5669],\n",
      "          [14.5000, 14.5561, 14.5424, 14.5000, 14.5000, 14.5000, 14.5000,\n",
      "           14.5000, 14.5000, 14.5000, 14.5168, 14.5862, 14.6035, 14.5744,\n",
      "           14.5078, 14.5584, 14.5463, 14.5905, 14.5000],\n",
      "          [15.6274, 15.5000, 15.5366, 15.5000, 15.6605, 15.5083, 15.5000,\n",
      "           15.5000, 15.5000, 15.5218, 15.5266, 15.5677, 15.5952, 15.6262,\n",
      "           15.5000, 15.5350, 15.5000, 15.7263, 15.5287],\n",
      "          [16.5000, 16.6271, 16.7133, 16.5971, 16.5653, 16.5236, 16.5285,\n",
      "           16.5000, 16.5246, 16.5923, 16.5875, 16.6109, 16.5100, 16.5075,\n",
      "           16.5000, 16.5000, 16.6813, 16.7436, 16.5839],\n",
      "          [17.6769, 17.5690, 17.5986, 17.5000, 17.6293, 17.5327, 17.5000,\n",
      "           17.5022, 17.5000, 17.5192, 17.5000, 17.5605, 17.6373, 17.5480,\n",
      "           17.5800, 17.6081, 17.5738, 17.5026, 17.5000],\n",
      "          [18.5711, 18.6829, 18.5713, 18.5924, 18.5000, 18.5464, 18.5000,\n",
      "           18.5487, 18.5809, 18.5645, 18.5619, 18.5198, 18.5561, 18.5021,\n",
      "           18.5864, 18.5853, 18.6214, 18.6144, 18.5180]]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "[142, 192, 459]\n",
      "[110, 243, 401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.5000, 0.5000, 0.5558, 0.7951, 0.5000, 0.5931, 0.5755, 0.5323,\n",
      "           0.6251, 0.5000, 0.5059, 0.5000, 0.5485, 0.5294, 0.5000, 0.5404,\n",
      "           0.6277, 0.5858, 0.6731],\n",
      "          [0.5000, 0.5937, 0.5492, 0.7992, 0.5553, 0.5000, 0.5000, 0.5724,\n",
      "           0.5000, 0.5101, 0.6314, 0.5706, 0.5000, 0.6885, 0.5339, 0.5537,\n",
      "           0.6752, 0.5000, 0.6041],\n",
      "          [0.5000, 0.5262, 0.6849, 0.8407, 0.6529, 0.5194, 0.5000, 0.5533,\n",
      "           0.5000, 0.5641, 0.5100, 0.6627, 0.6656, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5941, 0.5000],\n",
      "          [0.5000, 0.6948, 0.5695, 0.5000, 0.5000, 0.5756, 0.5622, 0.6243,\n",
      "           0.5539, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5323, 0.5000,\n",
      "           0.5000, 0.5537, 0.5494],\n",
      "          [0.5000, 0.6932, 0.6353, 0.6073, 0.5077, 0.5000, 0.5280, 0.6670,\n",
      "           0.5247, 0.5976, 0.5000, 0.5000, 0.5000, 0.5000, 0.5551, 0.5000,\n",
      "           0.5727, 0.5496, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5571, 0.5695, 0.7033, 0.5000, 0.6544, 0.5000,\n",
      "           0.5709, 0.5359, 0.5000, 0.5172, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5625, 0.5139],\n",
      "          [0.5226, 0.5999, 0.5083, 0.5733, 0.5931, 0.5476, 0.6655, 0.5000,\n",
      "           0.5164, 0.5460, 0.5367, 0.5000, 0.5000, 0.5625, 0.5000, 0.5651,\n",
      "           0.5000, 0.6064, 0.5793],\n",
      "          [0.5000, 0.6049, 0.6513, 0.5248, 0.5126, 0.5273, 0.6140, 0.5521,\n",
      "           0.5114, 0.5409, 0.5339, 0.5849, 0.5517, 0.5000, 0.5010, 0.5108,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5951, 0.5000, 0.6183, 0.5300, 0.5000, 0.5000, 0.6205, 0.5505,\n",
      "           0.6767, 0.5000, 0.5467, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5033, 0.5441, 0.5415],\n",
      "          [0.5180, 0.5452, 0.6372, 0.5152, 0.5000, 0.5000, 0.5000, 0.5802,\n",
      "           0.5797, 0.5213, 0.5472, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5660, 0.5465, 0.5797],\n",
      "          [0.5000, 0.5335, 0.5817, 0.5325, 0.5000, 0.5539, 0.5102, 0.5266,\n",
      "           0.5896, 0.5500, 0.5202, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5124, 0.5145],\n",
      "          [0.5000, 0.5542, 0.5000, 0.5048, 0.5000, 0.6209, 0.5000, 0.5295,\n",
      "           0.5032, 0.5000, 0.5972, 0.5117, 0.5576, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5303, 0.5000],\n",
      "          [0.5000, 0.5175, 0.5473, 0.6504, 0.5000, 0.5077, 0.5175, 0.5000,\n",
      "           0.5000, 0.6080, 0.5108, 0.5044, 0.5000, 0.5038, 0.5000, 0.5000,\n",
      "           0.6238, 0.5184, 0.5333],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5123, 0.5000, 0.5510,\n",
      "           0.5000, 0.6402, 0.5629, 0.5317, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5186, 0.5000],\n",
      "          [0.5000, 0.5703, 0.5000, 0.6535, 0.5000, 0.6779, 0.5680, 0.6565,\n",
      "           0.5000, 0.5980, 0.5163, 0.5000, 0.5000, 0.5000, 0.5727, 0.5000,\n",
      "           0.6421, 0.5000, 0.5560],\n",
      "          [0.5000, 0.6522, 0.5989, 0.5667, 0.5000, 0.5000, 0.5135, 0.5000,\n",
      "           0.5000, 0.5229, 0.5000, 0.5248, 0.5000, 0.5000, 0.6743, 0.5335,\n",
      "           0.5054, 0.5000, 0.5978],\n",
      "          [0.5000, 0.5440, 0.5906, 0.6647, 0.5000, 0.5504, 0.5000, 0.6889,\n",
      "           0.5000, 0.6182, 0.5855, 0.5000, 0.5033, 0.5000, 0.6050, 0.5614,\n",
      "           0.5456, 0.5772, 0.6998],\n",
      "          [0.5000, 0.6771, 0.6204, 0.8454, 0.7215, 0.5000, 0.5000, 0.5000,\n",
      "           0.5098, 0.5257, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5615, 0.5721, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5627, 0.5000, 0.5009, 0.5413, 0.5000, 0.5027,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5209, 0.5000, 0.5081, 0.5000,\n",
      "           0.5000, 0.6929, 0.5433]]]], device='cuda:0',\n",
      "       grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.5000,  0.5000,  0.5558,  0.7951,  0.5000,  0.5931,  0.5755,\n",
      "            0.5323,  0.6251,  0.5000,  0.5059,  0.5000,  0.5485,  0.5294,\n",
      "            0.5000,  0.5404,  0.6277,  0.5858,  0.6731],\n",
      "          [ 1.5000,  1.5937,  1.5492,  1.7992,  1.5553,  1.5000,  1.5000,\n",
      "            1.5724,  1.5000,  1.5101,  1.6314,  1.5706,  1.5000,  1.6885,\n",
      "            1.5339,  1.5537,  1.6752,  1.5000,  1.6041],\n",
      "          [ 2.5000,  2.5262,  2.6849,  2.8407,  2.6529,  2.5194,  2.5000,\n",
      "            2.5533,  2.5000,  2.5641,  2.5100,  2.6627,  2.6656,  2.5000,\n",
      "            2.5000,  2.5000,  2.5000,  2.5941,  2.5000],\n",
      "          [ 3.5000,  3.6948,  3.5695,  3.5000,  3.5000,  3.5756,  3.5622,\n",
      "            3.6243,  3.5539,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,\n",
      "            3.5323,  3.5000,  3.5000,  3.5537,  3.5494],\n",
      "          [ 4.5000,  4.6932,  4.6353,  4.6073,  4.5077,  4.5000,  4.5280,\n",
      "            4.6670,  4.5247,  4.5976,  4.5000,  4.5000,  4.5000,  4.5000,\n",
      "            4.5551,  4.5000,  4.5727,  4.5496,  4.5000],\n",
      "          [ 5.5000,  5.5000,  5.5571,  5.5695,  5.7033,  5.5000,  5.6544,\n",
      "            5.5000,  5.5709,  5.5359,  5.5000,  5.5172,  5.5000,  5.5000,\n",
      "            5.5000,  5.5000,  5.5000,  5.5625,  5.5139],\n",
      "          [ 6.5226,  6.5999,  6.5083,  6.5733,  6.5931,  6.5476,  6.6655,\n",
      "            6.5000,  6.5164,  6.5460,  6.5367,  6.5000,  6.5000,  6.5625,\n",
      "            6.5000,  6.5651,  6.5000,  6.6064,  6.5793],\n",
      "          [ 7.5000,  7.6049,  7.6513,  7.5248,  7.5126,  7.5273,  7.6140,\n",
      "            7.5521,  7.5114,  7.5409,  7.5339,  7.5849,  7.5517,  7.5000,\n",
      "            7.5010,  7.5108,  7.5000,  7.5000,  7.5000],\n",
      "          [ 8.5951,  8.5000,  8.6184,  8.5300,  8.5000,  8.5000,  8.6205,\n",
      "            8.5505,  8.6767,  8.5000,  8.5467,  8.5000,  8.5000,  8.5000,\n",
      "            8.5000,  8.5000,  8.5033,  8.5441,  8.5415],\n",
      "          [ 9.5180,  9.5452,  9.6372,  9.5152,  9.5000,  9.5000,  9.5000,\n",
      "            9.5802,  9.5797,  9.5213,  9.5472,  9.5000,  9.5000,  9.5000,\n",
      "            9.5000,  9.5000,  9.5660,  9.5465,  9.5797],\n",
      "          [10.5000, 10.5335, 10.5817, 10.5325, 10.5000, 10.5539, 10.5102,\n",
      "           10.5266, 10.5896, 10.5500, 10.5202, 10.5000, 10.5000, 10.5000,\n",
      "           10.5000, 10.5000, 10.5000, 10.5124, 10.5145],\n",
      "          [11.5000, 11.5542, 11.5000, 11.5048, 11.5000, 11.6209, 11.5000,\n",
      "           11.5295, 11.5032, 11.5000, 11.5972, 11.5117, 11.5576, 11.5000,\n",
      "           11.5000, 11.5000, 11.5000, 11.5303, 11.5000],\n",
      "          [12.5000, 12.5175, 12.5473, 12.6504, 12.5000, 12.5077, 12.5175,\n",
      "           12.5000, 12.5000, 12.6080, 12.5108, 12.5044, 12.5000, 12.5038,\n",
      "           12.5000, 12.5000, 12.6238, 12.5184, 12.5333],\n",
      "          [13.5000, 13.5000, 13.5000, 13.5000, 13.5000, 13.5123, 13.5000,\n",
      "           13.5510, 13.5000, 13.6402, 13.5629, 13.5317, 13.5000, 13.5000,\n",
      "           13.5000, 13.5000, 13.5000, 13.5186, 13.5000],\n",
      "          [14.5000, 14.5703, 14.5000, 14.6535, 14.5000, 14.6779, 14.5680,\n",
      "           14.6565, 14.5000, 14.5980, 14.5163, 14.5000, 14.5000, 14.5000,\n",
      "           14.5727, 14.5000, 14.6421, 14.5000, 14.5560],\n",
      "          [15.5000, 15.6522, 15.5989, 15.5667, 15.5000, 15.5000, 15.5135,\n",
      "           15.5000, 15.5000, 15.5229, 15.5000, 15.5248, 15.5000, 15.5000,\n",
      "           15.6743, 15.5335, 15.5054, 15.5000, 15.5978],\n",
      "          [16.5000, 16.5440, 16.5906, 16.6647, 16.5000, 16.5504, 16.5000,\n",
      "           16.6889, 16.5000, 16.6182, 16.5855, 16.5000, 16.5033, 16.5000,\n",
      "           16.6050, 16.5614, 16.5456, 16.5772, 16.6998],\n",
      "          [17.5000, 17.6771, 17.6204, 17.8454, 17.7215, 17.5000, 17.5000,\n",
      "           17.5000, 17.5098, 17.5257, 17.5000, 17.5000, 17.5000, 17.5000,\n",
      "           17.5000, 17.5000, 17.5615, 17.5721, 17.5000],\n",
      "          [18.5000, 18.5000, 18.5627, 18.5000, 18.5009, 18.5413, 18.5000,\n",
      "           18.5027, 18.5000, 18.5000, 18.5000, 18.5000, 18.5209, 18.5000,\n",
      "           18.5081, 18.5000, 18.5000, 18.6929, 18.5433]]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([3, 1, 85, 76, 76])\n",
      "Loss (epoch: 1): 2929.7705\n",
      "[[ 12  16]\n",
      " [ 19  36]\n",
      " [ 40  28]\n",
      " [ 36  75]\n",
      " [ 76  55]\n",
      " [ 72 146]\n",
      " [142 110]\n",
      " [192 243]\n",
      " [459 401]]\n",
      "[12, None, None]\n",
      "[16, None, None]\n",
      "tensor([[[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          ...,\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.5000,  0.5000,  0.5000,  ...,  0.5000,  0.5000,  0.5000],\n",
      "          [ 1.5000,  1.5000,  1.5000,  ...,  1.5000,  1.5000,  1.5000],\n",
      "          [ 2.5000,  2.5000,  2.5000,  ...,  2.5000,  2.5000,  2.5000],\n",
      "          ...,\n",
      "          [73.5000, 73.5000, 73.5000,  ..., 73.5000, 73.5000, 73.5000],\n",
      "          [74.5000, 74.5000, 74.5000,  ..., 74.5000, 74.5000, 74.5000],\n",
      "          [75.5000, 75.5000, 75.5000,  ..., 75.5000, 75.5000, 75.5000]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "[12, 19, None]\n",
      "[16, 36, None]\n",
      "tensor([[[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          ...,\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.5000,  0.5000,  0.5000,  ...,  0.5000,  0.5000,  0.5000],\n",
      "          [ 1.5000,  1.5000,  1.5000,  ...,  1.5000,  1.5000,  1.5000],\n",
      "          [ 2.5000,  2.5000,  2.5000,  ...,  2.5000,  2.5000,  2.5000],\n",
      "          ...,\n",
      "          [73.5000, 73.5000, 73.5000,  ..., 73.5000, 73.5000, 73.5000],\n",
      "          [74.5000, 74.5000, 74.5000,  ..., 74.5000, 74.5000, 74.5000],\n",
      "          [75.5000, 75.5000, 75.5000,  ..., 75.5000, 75.5000, 75.5000]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "[12, 19, 40]\n",
      "[16, 36, 28]\n",
      "tensor([[[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          ...,\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.5000,  0.5000,  0.5000,  ...,  0.5000,  0.5000,  0.5000],\n",
      "          [ 1.5000,  1.5000,  1.5000,  ...,  1.5000,  1.5000,  1.5000],\n",
      "          [ 2.5000,  2.5000,  2.5000,  ...,  2.5000,  2.5000,  2.5000],\n",
      "          ...,\n",
      "          [73.5000, 73.5000, 73.5000,  ..., 73.5000, 73.5000, 73.5000],\n",
      "          [74.5000, 74.5000, 74.5000,  ..., 74.5000, 74.5000, 74.5000],\n",
      "          [75.5000, 75.5000, 75.5000,  ..., 75.5000, 75.5000, 75.5000]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "[[ 12  16]\n",
      " [ 19  36]\n",
      " [ 40  28]\n",
      " [ 36  75]\n",
      " [ 76  55]\n",
      " [ 72 146]\n",
      " [142 110]\n",
      " [192 243]\n",
      " [459 401]]\n",
      "[36, None, None]\n",
      "[75, None, None]\n",
      "tensor([[[[0.5000, 0.5000, 0.5229,  ..., 0.5000, 0.5106, 0.5260],\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          [0.5069, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          ...,\n",
      "          [0.6678, 0.5000, 0.5000,  ..., 0.5000, 0.5906, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5318, 0.5000, 0.6136],\n",
      "          [0.5000, 0.7200, 0.5004,  ..., 0.5000, 0.5856, 0.5000]]]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.5000,  0.5000,  0.5229,  ...,  0.5000,  0.5106,  0.5260],\n",
      "          [ 1.5000,  1.5000,  1.5000,  ...,  1.5000,  1.5000,  1.5000],\n",
      "          [ 2.5069,  2.5000,  2.5000,  ...,  2.5000,  2.5000,  2.5000],\n",
      "          ...,\n",
      "          [35.6678, 35.5000, 35.5000,  ..., 35.5000, 35.5906, 35.5000],\n",
      "          [36.5000, 36.5000, 36.5000,  ..., 36.5318, 36.5000, 36.6136],\n",
      "          [37.5000, 37.7200, 37.5004,  ..., 37.5000, 37.5856, 37.5000]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "[36, 76, None]\n",
      "[75, 55, None]\n",
      "tensor([[[[0.5000, 0.5000, 0.5248,  ..., 0.5024, 0.5150, 0.5000],\n",
      "          [0.6221, 0.7081, 0.6215,  ..., 0.5097, 0.5000, 0.5594],\n",
      "          [0.6616, 0.5202, 0.5870,  ..., 0.5000, 0.5000, 0.5525],\n",
      "          ...,\n",
      "          [0.9608, 0.9301, 0.9085,  ..., 0.9474, 0.8260, 0.6599],\n",
      "          [0.9697, 0.6716, 0.7967,  ..., 0.5000, 0.5041, 0.5000],\n",
      "          [0.8560, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.5000,  0.5000,  0.5248,  ...,  0.5024,  0.5150,  0.5000],\n",
      "          [ 1.6221,  1.7081,  1.6215,  ...,  1.5097,  1.5000,  1.5594],\n",
      "          [ 2.6616,  2.5202,  2.5870,  ...,  2.5000,  2.5000,  2.5525],\n",
      "          ...,\n",
      "          [35.9608, 35.9301, 35.9085,  ..., 35.9474, 35.8260, 35.6599],\n",
      "          [36.9697, 36.6716, 36.7967,  ..., 36.5000, 36.5041, 36.5000],\n",
      "          [37.8560, 37.5000, 37.5000,  ..., 37.5000, 37.5000, 37.5000]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "[36, 76, 72]\n",
      "[75, 55, 146]\n",
      "tensor([[[[0.5818, 0.5275, 0.5000,  ..., 0.5399, 0.5191, 0.5000],\n",
      "          [0.5000, 0.5952, 0.5949,  ..., 0.5401, 0.5365, 0.5000],\n",
      "          [0.5000, 0.7345, 0.5000,  ..., 0.5280, 0.5000, 0.5029],\n",
      "          ...,\n",
      "          [0.8765, 0.7824, 0.5000,  ..., 0.6441, 0.6261, 0.5000],\n",
      "          [0.9614, 0.8425, 0.5203,  ..., 0.8322, 0.7660, 0.7917],\n",
      "          [0.8046, 0.5000, 0.5000,  ..., 0.6381, 0.7499, 0.7225]]]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.5818,  0.5275,  0.5000,  ...,  0.5399,  0.5191,  0.5000],\n",
      "          [ 1.5000,  1.5952,  1.5949,  ...,  1.5401,  1.5365,  1.5000],\n",
      "          [ 2.5000,  2.7345,  2.5000,  ...,  2.5280,  2.5000,  2.5029],\n",
      "          ...,\n",
      "          [35.8765, 35.7824, 35.5000,  ..., 35.6441, 35.6261, 35.5000],\n",
      "          [36.9614, 36.8425, 36.5203,  ..., 36.8322, 36.7660, 36.7917],\n",
      "          [37.8046, 37.5000, 37.5000,  ..., 37.6381, 37.7500, 37.7225]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "[[ 12  16]\n",
      " [ 19  36]\n",
      " [ 40  28]\n",
      " [ 36  75]\n",
      " [ 76  55]\n",
      " [ 72 146]\n",
      " [142 110]\n",
      " [192 243]\n",
      " [459 401]]\n",
      "[142, None, None]\n",
      "[110, None, None]\n",
      "tensor([[[[0.5627, 0.6639, 0.5931, 0.5474, 0.5777, 0.5551, 0.5400, 0.5442,\n",
      "           0.5499, 0.5470, 0.5532, 0.5594, 0.5720, 0.5754, 0.5599, 0.5564,\n",
      "           0.5714, 0.6093, 0.5510],\n",
      "          [0.6263, 0.7520, 0.7092, 0.7043, 0.5697, 0.5355, 0.5567, 0.5537,\n",
      "           0.5410, 0.5416, 0.5467, 0.5602, 0.5533, 0.5406, 0.5387, 0.5000,\n",
      "           0.5663, 0.6092, 0.5576],\n",
      "          [0.6159, 0.7337, 0.5800, 0.7500, 0.6326, 0.6261, 0.6189, 0.6286,\n",
      "           0.6347, 0.6404, 0.6384, 0.6314, 0.6232, 0.6054, 0.6054, 0.5601,\n",
      "           0.5498, 0.6458, 0.5740],\n",
      "          [0.6712, 0.7104, 0.5218, 0.5592, 0.5906, 0.5471, 0.5487, 0.5576,\n",
      "           0.5680, 0.5718, 0.5801, 0.5847, 0.6014, 0.6006, 0.5887, 0.5399,\n",
      "           0.5000, 0.6723, 0.5961],\n",
      "          [0.6386, 0.6564, 0.5062, 0.6052, 0.6220, 0.5759, 0.5731, 0.5744,\n",
      "           0.5676, 0.5767, 0.5811, 0.5763, 0.5808, 0.5776, 0.5644, 0.5791,\n",
      "           0.5544, 0.5968, 0.5713],\n",
      "          [0.6349, 0.6718, 0.5000, 0.5912, 0.6063, 0.5905, 0.5656, 0.5655,\n",
      "           0.5589, 0.5595, 0.5652, 0.5641, 0.5696, 0.5636, 0.5780, 0.5555,\n",
      "           0.5577, 0.6285, 0.5934],\n",
      "          [0.6416, 0.6589, 0.5000, 0.6032, 0.6015, 0.5953, 0.5824, 0.5884,\n",
      "           0.5835, 0.5879, 0.6009, 0.5951, 0.5829, 0.5722, 0.5727, 0.5564,\n",
      "           0.5344, 0.6313, 0.5994],\n",
      "          [0.6483, 0.6391, 0.5000, 0.6320, 0.5964, 0.6042, 0.5851, 0.5852,\n",
      "           0.5864, 0.5990, 0.6056, 0.5931, 0.5811, 0.5641, 0.5588, 0.5207,\n",
      "           0.5289, 0.6121, 0.5718],\n",
      "          [0.6365, 0.6308, 0.5058, 0.6309, 0.5600, 0.5436, 0.5421, 0.5534,\n",
      "           0.5540, 0.5528, 0.5511, 0.5527, 0.5674, 0.5692, 0.5744, 0.5479,\n",
      "           0.5274, 0.6605, 0.5752],\n",
      "          [0.6800, 0.6907, 0.5232, 0.6249, 0.6353, 0.5674, 0.5659, 0.5712,\n",
      "           0.5819, 0.5772, 0.5816, 0.5612, 0.5600, 0.5561, 0.5380, 0.5000,\n",
      "           0.5861, 0.6256, 0.5834],\n",
      "          [0.6623, 0.6017, 0.5904, 0.6188, 0.6136, 0.5668, 0.5943, 0.5766,\n",
      "           0.5701, 0.5658, 0.5781, 0.5701, 0.5591, 0.5451, 0.5548, 0.5322,\n",
      "           0.5981, 0.6191, 0.5596],\n",
      "          [0.6722, 0.6308, 0.7243, 0.6604, 0.6674, 0.6585, 0.6343, 0.6533,\n",
      "           0.6429, 0.6442, 0.6512, 0.6379, 0.6167, 0.5855, 0.5578, 0.5946,\n",
      "           0.6058, 0.6050, 0.6307],\n",
      "          [0.7297, 0.6840, 0.6926, 0.6509, 0.6452, 0.6287, 0.6144, 0.6377,\n",
      "           0.6443, 0.6179, 0.6220, 0.6054, 0.5829, 0.5939, 0.5742, 0.5803,\n",
      "           0.6017, 0.6822, 0.5941],\n",
      "          [0.7056, 0.6470, 0.8126, 0.6373, 0.6858, 0.7657, 0.7295, 0.7304,\n",
      "           0.7239, 0.7029, 0.7000, 0.6402, 0.6156, 0.6792, 0.6235, 0.6393,\n",
      "           0.6891, 0.7283, 0.6857],\n",
      "          [0.7912, 0.8324, 0.8179, 0.7899, 0.7054, 0.7462, 0.7021, 0.7140,\n",
      "           0.7090, 0.6886, 0.6673, 0.6503, 0.6596, 0.6538, 0.6981, 0.7991,\n",
      "           0.8084, 0.6775, 0.8105],\n",
      "          [0.7026, 0.9316, 0.8291, 0.9117, 0.7261, 0.7028, 0.7966, 0.7968,\n",
      "           0.7852, 0.7719, 0.7256, 0.6839, 0.7092, 0.7909, 0.7660, 0.8308,\n",
      "           0.7048, 0.8161, 0.9002],\n",
      "          [0.7372, 0.8608, 0.8365, 0.8561, 0.7093, 0.6535, 0.6169, 0.6127,\n",
      "           0.6478, 0.6803, 0.7385, 0.7320, 0.7934, 0.8219, 0.9173, 0.9432,\n",
      "           0.9052, 0.8744, 0.9014],\n",
      "          [0.9326, 0.8742, 0.9498, 0.9451, 0.9463, 0.8823, 0.8636, 0.8619,\n",
      "           0.8464, 0.8245, 0.8350, 0.8699, 0.8634, 0.8837, 0.8673, 0.9080,\n",
      "           0.8653, 0.8700, 0.8891],\n",
      "          [0.7856, 0.9003, 0.8586, 0.9334, 0.7520, 0.6842, 0.6833, 0.7046,\n",
      "           0.6939, 0.6625, 0.6808, 0.7000, 0.7113, 0.7399, 0.7782, 0.9107,\n",
      "           0.8277, 0.9076, 0.8287]]]], device='cuda:0',\n",
      "       grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.5627,  0.6639,  0.5931,  0.5474,  0.5777,  0.5551,  0.5400,\n",
      "            0.5442,  0.5499,  0.5470,  0.5532,  0.5594,  0.5720,  0.5754,\n",
      "            0.5599,  0.5564,  0.5714,  0.6093,  0.5510],\n",
      "          [ 1.6263,  1.7520,  1.7092,  1.7043,  1.5697,  1.5355,  1.5567,\n",
      "            1.5537,  1.5410,  1.5416,  1.5467,  1.5602,  1.5533,  1.5406,\n",
      "            1.5387,  1.5000,  1.5663,  1.6092,  1.5576],\n",
      "          [ 2.6159,  2.7337,  2.5800,  2.7500,  2.6326,  2.6261,  2.6189,\n",
      "            2.6286,  2.6347,  2.6404,  2.6384,  2.6314,  2.6232,  2.6054,\n",
      "            2.6054,  2.5601,  2.5498,  2.6458,  2.5740],\n",
      "          [ 3.6712,  3.7104,  3.5218,  3.5592,  3.5906,  3.5471,  3.5487,\n",
      "            3.5576,  3.5680,  3.5718,  3.5801,  3.5847,  3.6014,  3.6006,\n",
      "            3.5887,  3.5399,  3.5000,  3.6723,  3.5961],\n",
      "          [ 4.6386,  4.6564,  4.5062,  4.6052,  4.6220,  4.5759,  4.5731,\n",
      "            4.5744,  4.5676,  4.5767,  4.5811,  4.5763,  4.5808,  4.5776,\n",
      "            4.5644,  4.5791,  4.5544,  4.5968,  4.5713],\n",
      "          [ 5.6349,  5.6718,  5.5000,  5.5912,  5.6063,  5.5905,  5.5656,\n",
      "            5.5655,  5.5589,  5.5595,  5.5652,  5.5641,  5.5696,  5.5636,\n",
      "            5.5780,  5.5555,  5.5577,  5.6285,  5.5934],\n",
      "          [ 6.6416,  6.6589,  6.5000,  6.6032,  6.6015,  6.5953,  6.5824,\n",
      "            6.5884,  6.5835,  6.5879,  6.6009,  6.5951,  6.5829,  6.5722,\n",
      "            6.5727,  6.5564,  6.5344,  6.6313,  6.5994],\n",
      "          [ 7.6483,  7.6391,  7.5000,  7.6320,  7.5964,  7.6042,  7.5851,\n",
      "            7.5852,  7.5864,  7.5990,  7.6056,  7.5931,  7.5811,  7.5641,\n",
      "            7.5588,  7.5207,  7.5289,  7.6121,  7.5718],\n",
      "          [ 8.6365,  8.6308,  8.5058,  8.6309,  8.5600,  8.5436,  8.5421,\n",
      "            8.5534,  8.5540,  8.5528,  8.5511,  8.5527,  8.5674,  8.5692,\n",
      "            8.5744,  8.5479,  8.5274,  8.6605,  8.5752],\n",
      "          [ 9.6800,  9.6907,  9.5232,  9.6249,  9.6353,  9.5674,  9.5659,\n",
      "            9.5712,  9.5819,  9.5772,  9.5816,  9.5612,  9.5600,  9.5561,\n",
      "            9.5380,  9.5000,  9.5861,  9.6256,  9.5834],\n",
      "          [10.6623, 10.6017, 10.5904, 10.6188, 10.6136, 10.5668, 10.5943,\n",
      "           10.5766, 10.5701, 10.5658, 10.5781, 10.5701, 10.5591, 10.5451,\n",
      "           10.5548, 10.5322, 10.5981, 10.6191, 10.5596],\n",
      "          [11.6722, 11.6308, 11.7243, 11.6604, 11.6674, 11.6585, 11.6343,\n",
      "           11.6533, 11.6429, 11.6442, 11.6512, 11.6379, 11.6167, 11.5855,\n",
      "           11.5578, 11.5946, 11.6058, 11.6050, 11.6307],\n",
      "          [12.7297, 12.6840, 12.6926, 12.6509, 12.6452, 12.6287, 12.6144,\n",
      "           12.6377, 12.6443, 12.6179, 12.6220, 12.6054, 12.5829, 12.5939,\n",
      "           12.5742, 12.5803, 12.6017, 12.6822, 12.5941],\n",
      "          [13.7056, 13.6470, 13.8126, 13.6373, 13.6858, 13.7657, 13.7295,\n",
      "           13.7304, 13.7239, 13.7029, 13.7000, 13.6402, 13.6156, 13.6792,\n",
      "           13.6235, 13.6393, 13.6891, 13.7283, 13.6857],\n",
      "          [14.7912, 14.8324, 14.8179, 14.7899, 14.7054, 14.7462, 14.7021,\n",
      "           14.7140, 14.7090, 14.6886, 14.6673, 14.6503, 14.6596, 14.6538,\n",
      "           14.6981, 14.7991, 14.8084, 14.6775, 14.8105],\n",
      "          [15.7026, 15.9316, 15.8291, 15.9117, 15.7261, 15.7028, 15.7966,\n",
      "           15.7968, 15.7852, 15.7719, 15.7256, 15.6839, 15.7092, 15.7909,\n",
      "           15.7660, 15.8308, 15.7048, 15.8161, 15.9002],\n",
      "          [16.7372, 16.8608, 16.8365, 16.8561, 16.7093, 16.6535, 16.6169,\n",
      "           16.6127, 16.6478, 16.6803, 16.7385, 16.7320, 16.7934, 16.8219,\n",
      "           16.9173, 16.9432, 16.9052, 16.8744, 16.9014],\n",
      "          [17.9326, 17.8742, 17.9498, 17.9451, 17.9463, 17.8823, 17.8636,\n",
      "           17.8619, 17.8464, 17.8245, 17.8350, 17.8699, 17.8634, 17.8837,\n",
      "           17.8673, 17.9080, 17.8653, 17.8700, 17.8891],\n",
      "          [18.7856, 18.9003, 18.8586, 18.9334, 18.7520, 18.6842, 18.6833,\n",
      "           18.7046, 18.6940, 18.6625, 18.6808, 18.7000, 18.7113, 18.7399,\n",
      "           18.7782, 18.9107, 18.8277, 18.9076, 18.8287]]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "[142, 192, None]\n",
      "[110, 243, None]\n",
      "tensor([[[[0.5000, 0.5000, 0.5619, 0.5158, 0.5000, 0.5069, 0.5087, 0.5162,\n",
      "           0.5116, 0.5085, 0.5099, 0.5053, 0.5041, 0.5000, 0.5000, 0.5000,\n",
      "           0.5195, 0.5047, 0.5000],\n",
      "          [0.5536, 0.5351, 0.5000, 0.5256, 0.5000, 0.5624, 0.5556, 0.5664,\n",
      "           0.5708, 0.5666, 0.5636, 0.5530, 0.5498, 0.5362, 0.5069, 0.5270,\n",
      "           0.5771, 0.5000, 0.5260],\n",
      "          [0.5486, 0.5760, 0.5782, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5121, 0.5000],\n",
      "          [0.5806, 0.5475, 0.5000, 0.5000, 0.5000, 0.5199, 0.5183, 0.5051,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5162, 0.5290, 0.5898,\n",
      "           0.5098, 0.5770, 0.5000],\n",
      "          [0.5711, 0.5419, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5711, 0.5003],\n",
      "          [0.5788, 0.5591, 0.5000, 0.5000, 0.5128, 0.5153, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5049,\n",
      "           0.5212, 0.5658, 0.5292],\n",
      "          [0.5694, 0.5739, 0.5000, 0.5000, 0.5241, 0.5290, 0.5117, 0.5023,\n",
      "           0.5010, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5048,\n",
      "           0.5563, 0.5650, 0.5287],\n",
      "          [0.5729, 0.5846, 0.5342, 0.5000, 0.5205, 0.5204, 0.5006, 0.5003,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5080, 0.5054,\n",
      "           0.5541, 0.5549, 0.5013],\n",
      "          [0.5599, 0.5600, 0.5167, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5019, 0.5252,\n",
      "           0.5448, 0.5422, 0.5330],\n",
      "          [0.5127, 0.5196, 0.5000, 0.5000, 0.5976, 0.5631, 0.5575, 0.5557,\n",
      "           0.5515, 0.5513, 0.5486, 0.5503, 0.5560, 0.5519, 0.5593, 0.5757,\n",
      "           0.5841, 0.5890, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5112, 0.5239, 0.5042, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5389, 0.5000, 0.5000],\n",
      "          [0.5414, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5037, 0.5090, 0.5972, 0.6007, 0.5883, 0.5521,\n",
      "           0.5307, 0.5173, 0.5346, 0.5274, 0.5120, 0.5181, 0.5769, 0.6447,\n",
      "           0.6611, 0.5734, 0.5000],\n",
      "          [0.5000, 0.5361, 0.5066, 0.5162, 0.5879, 0.6523, 0.6243, 0.6337,\n",
      "           0.6178, 0.5926, 0.6026, 0.5978, 0.6002, 0.6061, 0.6739, 0.6469,\n",
      "           0.5536, 0.5656, 0.5514],\n",
      "          [0.5000, 0.5000, 0.6655, 0.5000, 0.5035, 0.5358, 0.5573, 0.5430,\n",
      "           0.5000, 0.5000, 0.5112, 0.5572, 0.5705, 0.6166, 0.6090, 0.6451,\n",
      "           0.5000, 0.5000, 0.6095],\n",
      "          [0.5123, 0.7101, 0.5000, 0.5904, 0.5000, 0.6310, 0.5922, 0.5464,\n",
      "           0.5526, 0.5620, 0.5642, 0.5213, 0.5000, 0.5000, 0.5000, 0.6143,\n",
      "           0.6240, 0.5232, 0.5000],\n",
      "          [0.5170, 0.7117, 0.5000, 0.5000, 0.7161, 0.7601, 0.7448, 0.7318,\n",
      "           0.7023, 0.7053, 0.7384, 0.6819, 0.6469, 0.7697, 0.8339, 0.8931,\n",
      "           0.8038, 0.5989, 0.5000],\n",
      "          [0.9264, 0.9218, 0.6659, 0.8701, 0.8523, 0.6652, 0.6997, 0.6889,\n",
      "           0.7125, 0.6751, 0.7057, 0.7342, 0.7325, 0.7608, 0.5563, 0.6309,\n",
      "           0.7626, 0.5000, 0.5155],\n",
      "          [0.7019, 0.6178, 0.6084, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5373,\n",
      "           0.7735, 0.7068, 0.6272]]]], device='cuda:0',\n",
      "       grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.5000,  0.5000,  0.5619,  0.5158,  0.5000,  0.5069,  0.5087,\n",
      "            0.5162,  0.5116,  0.5085,  0.5099,  0.5053,  0.5041,  0.5000,\n",
      "            0.5000,  0.5000,  0.5195,  0.5047,  0.5000],\n",
      "          [ 1.5536,  1.5351,  1.5000,  1.5256,  1.5000,  1.5624,  1.5556,\n",
      "            1.5664,  1.5708,  1.5666,  1.5636,  1.5530,  1.5498,  1.5362,\n",
      "            1.5069,  1.5270,  1.5771,  1.5000,  1.5260],\n",
      "          [ 2.5486,  2.5760,  2.5782,  2.5000,  2.5000,  2.5000,  2.5000,\n",
      "            2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,\n",
      "            2.5000,  2.5000,  2.5000,  2.5121,  2.5000],\n",
      "          [ 3.5806,  3.5475,  3.5000,  3.5000,  3.5000,  3.5199,  3.5183,\n",
      "            3.5051,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5162,\n",
      "            3.5290,  3.5898,  3.5098,  3.5770,  3.5000],\n",
      "          [ 4.5711,  4.5419,  4.5000,  4.5000,  4.5000,  4.5000,  4.5000,\n",
      "            4.5000,  4.5000,  4.5000,  4.5000,  4.5000,  4.5000,  4.5000,\n",
      "            4.5000,  4.5000,  4.5000,  4.5711,  4.5003],\n",
      "          [ 5.5788,  5.5591,  5.5000,  5.5000,  5.5128,  5.5153,  5.5000,\n",
      "            5.5000,  5.5000,  5.5000,  5.5000,  5.5000,  5.5000,  5.5000,\n",
      "            5.5000,  5.5049,  5.5212,  5.5658,  5.5292],\n",
      "          [ 6.5694,  6.5739,  6.5000,  6.5000,  6.5241,  6.5290,  6.5117,\n",
      "            6.5023,  6.5010,  6.5000,  6.5000,  6.5000,  6.5000,  6.5000,\n",
      "            6.5000,  6.5048,  6.5563,  6.5650,  6.5287],\n",
      "          [ 7.5729,  7.5846,  7.5342,  7.5000,  7.5205,  7.5204,  7.5006,\n",
      "            7.5003,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000,\n",
      "            7.5080,  7.5054,  7.5541,  7.5549,  7.5013],\n",
      "          [ 8.5599,  8.5600,  8.5167,  8.5000,  8.5000,  8.5000,  8.5000,\n",
      "            8.5000,  8.5000,  8.5000,  8.5000,  8.5000,  8.5000,  8.5000,\n",
      "            8.5019,  8.5252,  8.5448,  8.5422,  8.5330],\n",
      "          [ 9.5127,  9.5196,  9.5000,  9.5000,  9.5976,  9.5631,  9.5575,\n",
      "            9.5557,  9.5515,  9.5513,  9.5486,  9.5503,  9.5560,  9.5519,\n",
      "            9.5593,  9.5757,  9.5841,  9.5890,  9.5000],\n",
      "          [10.5000, 10.5000, 10.5000, 10.5000, 10.5000, 10.5000, 10.5000,\n",
      "           10.5000, 10.5112, 10.5239, 10.5042, 10.5000, 10.5000, 10.5000,\n",
      "           10.5000, 10.5000, 10.5389, 10.5000, 10.5000],\n",
      "          [11.5414, 11.5000, 11.5000, 11.5000, 11.5000, 11.5000, 11.5000,\n",
      "           11.5000, 11.5000, 11.5000, 11.5000, 11.5000, 11.5000, 11.5000,\n",
      "           11.5000, 11.5000, 11.5000, 11.5000, 11.5000],\n",
      "          [12.5000, 12.5000, 12.5037, 12.5090, 12.5972, 12.6007, 12.5883,\n",
      "           12.5521, 12.5307, 12.5173, 12.5346, 12.5274, 12.5120, 12.5181,\n",
      "           12.5769, 12.6447, 12.6611, 12.5734, 12.5000],\n",
      "          [13.5000, 13.5361, 13.5066, 13.5162, 13.5879, 13.6523, 13.6243,\n",
      "           13.6337, 13.6178, 13.5926, 13.6026, 13.5978, 13.6002, 13.6061,\n",
      "           13.6739, 13.6469, 13.5536, 13.5656, 13.5514],\n",
      "          [14.5000, 14.5000, 14.6655, 14.5000, 14.5035, 14.5358, 14.5573,\n",
      "           14.5430, 14.5000, 14.5000, 14.5112, 14.5572, 14.5705, 14.6166,\n",
      "           14.6090, 14.6451, 14.5000, 14.5000, 14.6095],\n",
      "          [15.5123, 15.7101, 15.5000, 15.5904, 15.5000, 15.6310, 15.5922,\n",
      "           15.5464, 15.5526, 15.5620, 15.5642, 15.5213, 15.5000, 15.5000,\n",
      "           15.5000, 15.6143, 15.6240, 15.5232, 15.5000],\n",
      "          [16.5170, 16.7117, 16.5000, 16.5000, 16.7161, 16.7601, 16.7448,\n",
      "           16.7318, 16.7023, 16.7053, 16.7384, 16.6819, 16.6469, 16.7697,\n",
      "           16.8339, 16.8931, 16.8038, 16.5989, 16.5000],\n",
      "          [17.9264, 17.9218, 17.6659, 17.8701, 17.8523, 17.6652, 17.6997,\n",
      "           17.6889, 17.7125, 17.6751, 17.7057, 17.7342, 17.7325, 17.7608,\n",
      "           17.5563, 17.6309, 17.7626, 17.5000, 17.5155],\n",
      "          [18.7019, 18.6178, 18.6084, 18.5000, 18.5000, 18.5000, 18.5000,\n",
      "           18.5000, 18.5000, 18.5000, 18.5000, 18.5000, 18.5000, 18.5000,\n",
      "           18.5000, 18.5373, 18.7735, 18.7069, 18.6272]]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "[142, 192, 459]\n",
      "[110, 243, 401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5165, 0.5000],\n",
      "          [0.5000, 0.6189, 0.5000, 0.5000, 0.5045, 0.5236, 0.5314, 0.5522,\n",
      "           0.5429, 0.5357, 0.5172, 0.5040, 0.5000, 0.5000, 0.5000, 0.5656,\n",
      "           0.5224, 0.5457, 0.5000],\n",
      "          [0.5000, 0.5750, 0.6113, 0.5543, 0.5000, 0.5053, 0.5268, 0.5417,\n",
      "           0.5369, 0.5273, 0.5237, 0.5272, 0.5346, 0.5309, 0.5334, 0.5281,\n",
      "           0.5385, 0.5576, 0.5000],\n",
      "          [0.5000, 0.6375, 0.5430, 0.5919, 0.5266, 0.5000, 0.5000, 0.5108,\n",
      "           0.5110, 0.5145, 0.5162, 0.5220, 0.5340, 0.5350, 0.5424, 0.5228,\n",
      "           0.5132, 0.5465, 0.5245],\n",
      "          [0.5000, 0.5711, 0.5000, 0.5403, 0.5509, 0.5000, 0.5105, 0.5050,\n",
      "           0.5000, 0.5002, 0.5077, 0.5248, 0.5450, 0.5689, 0.5772, 0.5607,\n",
      "           0.5472, 0.5529, 0.5000],\n",
      "          [0.5000, 0.5683, 0.5000, 0.5298, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5134, 0.5194, 0.5240,\n",
      "           0.5139, 0.5314, 0.5023],\n",
      "          [0.5000, 0.5928, 0.5000, 0.5529, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5040, 0.5077, 0.5242,\n",
      "           0.5160, 0.5342, 0.5000],\n",
      "          [0.5000, 0.6221, 0.5000, 0.5701, 0.5403, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5067, 0.5025, 0.5094, 0.5079,\n",
      "           0.5071, 0.5510, 0.5000],\n",
      "          [0.5000, 0.5703, 0.5000, 0.5724, 0.5106, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5069, 0.5005, 0.5207,\n",
      "           0.5000, 0.5570, 0.5097],\n",
      "          [0.5000, 0.5491, 0.5000, 0.5272, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5458, 0.5000],\n",
      "          [0.5000, 0.5013, 0.5000, 0.5326, 0.5021, 0.5083, 0.5335, 0.5227,\n",
      "           0.5260, 0.5286, 0.5007, 0.5048, 0.5090, 0.5139, 0.5045, 0.5213,\n",
      "           0.5608, 0.6042, 0.5000],\n",
      "          [0.5000, 0.5843, 0.5000, 0.5061, 0.5769, 0.5614, 0.5788, 0.5715,\n",
      "           0.5572, 0.5560, 0.5578, 0.5653, 0.5745, 0.5680, 0.5583, 0.5148,\n",
      "           0.5276, 0.5634, 0.5027],\n",
      "          [0.5000, 0.6309, 0.5000, 0.5022, 0.5074, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5173, 0.5252, 0.5499, 0.5945, 0.5589,\n",
      "           0.5696, 0.5940, 0.5000],\n",
      "          [0.5564, 0.5000, 0.5097, 0.5281, 0.5278, 0.5291, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5022, 0.5054, 0.5000, 0.5030,\n",
      "           0.5000, 0.6528, 0.5178],\n",
      "          [0.5000, 0.7453, 0.6935, 0.5184, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5395, 0.6175, 0.6170,\n",
      "           0.5696, 0.6304, 0.5340],\n",
      "          [0.5000, 0.5000, 0.7033, 0.5634, 0.5508, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5059, 0.5000, 0.5000, 0.5000, 0.6691, 0.7654,\n",
      "           0.5915, 0.6573, 0.5482],\n",
      "          [0.5000, 0.7320, 0.7105, 0.7568, 0.6109, 0.6759, 0.5767, 0.5336,\n",
      "           0.5011, 0.5386, 0.5688, 0.5613, 0.5534, 0.5762, 0.5000, 0.6526,\n",
      "           0.5778, 0.5000, 0.5000],\n",
      "          [0.5956, 0.8109, 0.7193, 0.7691, 0.7272, 0.5235, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5567, 0.5000, 0.5000, 0.5000, 0.5174,\n",
      "           0.6295, 0.5620, 0.8511],\n",
      "          [0.5220, 0.5250, 0.5000, 0.5000, 0.5000, 0.5639, 0.5000, 0.5062,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5373, 0.5773, 0.6037, 0.6675,\n",
      "           0.5652, 0.8018, 0.5000]]]], device='cuda:0',\n",
      "       grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,\n",
      "            0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,\n",
      "            0.5000,  0.5000,  0.5000,  0.5165,  0.5000],\n",
      "          [ 1.5000,  1.6189,  1.5000,  1.5000,  1.5045,  1.5236,  1.5314,\n",
      "            1.5522,  1.5429,  1.5357,  1.5172,  1.5040,  1.5000,  1.5000,\n",
      "            1.5000,  1.5656,  1.5224,  1.5457,  1.5000],\n",
      "          [ 2.5000,  2.5750,  2.6113,  2.5543,  2.5000,  2.5053,  2.5268,\n",
      "            2.5417,  2.5369,  2.5273,  2.5237,  2.5272,  2.5346,  2.5309,\n",
      "            2.5334,  2.5281,  2.5385,  2.5576,  2.5000],\n",
      "          [ 3.5000,  3.6375,  3.5430,  3.5919,  3.5266,  3.5000,  3.5000,\n",
      "            3.5108,  3.5110,  3.5145,  3.5162,  3.5220,  3.5340,  3.5350,\n",
      "            3.5424,  3.5228,  3.5132,  3.5465,  3.5245],\n",
      "          [ 4.5000,  4.5711,  4.5000,  4.5403,  4.5509,  4.5000,  4.5105,\n",
      "            4.5050,  4.5000,  4.5002,  4.5077,  4.5248,  4.5450,  4.5689,\n",
      "            4.5772,  4.5607,  4.5472,  4.5529,  4.5000],\n",
      "          [ 5.5000,  5.5683,  5.5000,  5.5298,  5.5000,  5.5000,  5.5000,\n",
      "            5.5000,  5.5000,  5.5000,  5.5000,  5.5000,  5.5000,  5.5134,\n",
      "            5.5194,  5.5240,  5.5139,  5.5314,  5.5023],\n",
      "          [ 6.5000,  6.5928,  6.5000,  6.5529,  6.5000,  6.5000,  6.5000,\n",
      "            6.5000,  6.5000,  6.5000,  6.5000,  6.5000,  6.5000,  6.5040,\n",
      "            6.5077,  6.5242,  6.5160,  6.5342,  6.5000],\n",
      "          [ 7.5000,  7.6221,  7.5000,  7.5701,  7.5403,  7.5000,  7.5000,\n",
      "            7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5067,  7.5025,\n",
      "            7.5094,  7.5079,  7.5071,  7.5510,  7.5000],\n",
      "          [ 8.5000,  8.5703,  8.5000,  8.5724,  8.5106,  8.5000,  8.5000,\n",
      "            8.5000,  8.5000,  8.5000,  8.5000,  8.5000,  8.5000,  8.5069,\n",
      "            8.5005,  8.5207,  8.5000,  8.5570,  8.5097],\n",
      "          [ 9.5000,  9.5491,  9.5000,  9.5272,  9.5000,  9.5000,  9.5000,\n",
      "            9.5000,  9.5000,  9.5000,  9.5000,  9.5000,  9.5000,  9.5000,\n",
      "            9.5000,  9.5000,  9.5000,  9.5458,  9.5000],\n",
      "          [10.5000, 10.5013, 10.5000, 10.5326, 10.5021, 10.5083, 10.5335,\n",
      "           10.5227, 10.5260, 10.5286, 10.5007, 10.5048, 10.5090, 10.5139,\n",
      "           10.5045, 10.5213, 10.5608, 10.6042, 10.5000],\n",
      "          [11.5000, 11.5843, 11.5000, 11.5061, 11.5769, 11.5614, 11.5788,\n",
      "           11.5715, 11.5572, 11.5560, 11.5578, 11.5653, 11.5745, 11.5680,\n",
      "           11.5583, 11.5148, 11.5276, 11.5634, 11.5027],\n",
      "          [12.5000, 12.6309, 12.5000, 12.5022, 12.5074, 12.5000, 12.5000,\n",
      "           12.5000, 12.5000, 12.5000, 12.5000, 12.5173, 12.5252, 12.5499,\n",
      "           12.5945, 12.5589, 12.5696, 12.5940, 12.5000],\n",
      "          [13.5564, 13.5000, 13.5097, 13.5281, 13.5278, 13.5291, 13.5000,\n",
      "           13.5000, 13.5000, 13.5000, 13.5000, 13.5000, 13.5022, 13.5054,\n",
      "           13.5000, 13.5030, 13.5000, 13.6528, 13.5178],\n",
      "          [14.5000, 14.7453, 14.6935, 14.5184, 14.5000, 14.5000, 14.5000,\n",
      "           14.5000, 14.5000, 14.5000, 14.5000, 14.5000, 14.5000, 14.5395,\n",
      "           14.6175, 14.6170, 14.5696, 14.6304, 14.5340],\n",
      "          [15.5000, 15.5000, 15.7033, 15.5634, 15.5508, 15.5000, 15.5000,\n",
      "           15.5000, 15.5000, 15.5000, 15.5059, 15.5000, 15.5000, 15.5000,\n",
      "           15.6691, 15.7654, 15.5915, 15.6573, 15.5482],\n",
      "          [16.5000, 16.7320, 16.7105, 16.7568, 16.6109, 16.6759, 16.5767,\n",
      "           16.5336, 16.5011, 16.5386, 16.5688, 16.5613, 16.5534, 16.5762,\n",
      "           16.5000, 16.6526, 16.5778, 16.5000, 16.5000],\n",
      "          [17.5956, 17.8109, 17.7193, 17.7691, 17.7272, 17.5235, 17.5000,\n",
      "           17.5000, 17.5000, 17.5000, 17.5000, 17.5567, 17.5000, 17.5000,\n",
      "           17.5000, 17.5174, 17.6295, 17.5620, 17.8511],\n",
      "          [18.5220, 18.5250, 18.5000, 18.5000, 18.5000, 18.5639, 18.5000,\n",
      "           18.5062, 18.5000, 18.5000, 18.5000, 18.5000, 18.5373, 18.5773,\n",
      "           18.6037, 18.6675, 18.5652, 18.8018, 18.5000]]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([3, 1, 85, 76, 76])\n",
      "Loss (epoch: 2): 2916.8774\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input = np.array([1 for i in range(608 * 608 * 3 * 1)]).reshape(1, 3, 608, 608)\n",
    "#target = np.array([0 for i in range(7 * 7 * 30)])\n",
    "target = np.array([0 for i in range(3 * 19 * 19 * 85 * 16)])\n",
    "\n",
    "input_tensor = torch.Tensor(input)\n",
    "output_tensor = torch.Tensor(target)\n",
    "\n",
    "#x = np.array([1 for i in range(608 * 608 * 3)]).reshape(1, 3, 608, 608)\n",
    "#x = torch.tensor(x)\n",
    "\n",
    "learning_rate = 0.08\n",
    "epoch_size = 2\n",
    "steps_for_printing_out_loss = 1\n",
    "\n",
    "YOLO_v4_Module_WIP = YOLO_v4_model(layer_details, layer_type)\n",
    "YOLO_v4_Module_WIP.cuda()\n",
    "#Model_WIP.to(device)\n",
    "loss_functioin = nn.MSELoss()\n",
    "optimizer = optim.SGD(YOLO_v4_Module_WIP.parameters(), lr = learning_rate)\n",
    "\n",
    "input = input_tensor.cuda()\n",
    "target = output_tensor.cuda()\n",
    "\n",
    "def training_model():\n",
    "    for i in range(1, epoch_size + 1):\n",
    "        optimizer.zero_grad()\n",
    "        output = YOLO_v4_Module_WIP(input.cuda())\n",
    "        print(output.size())\n",
    "        #b_x, b_y, b_w, b_h, objective_p, class_p = YOLO_v4_Module_WIP(input.cuda())\n",
    "        #output = b_x\n",
    "        #loss = loss_functioin(output, target.reshape(output.size(0), output.size(1), output.size(2), output.size(3)))\n",
    "        \n",
    "        loss = loss_functioin(output, target.reshape(output.size(0), output.size(1), output.size(2), output.size(3), output.size(4)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % (steps_for_printing_out_loss) == 0:\n",
    "            print('Loss (epoch: ' + str(i) + '): ' + str(loss.cpu().detach().numpy()))\n",
    "    #torch.save({'state_dict': YOLO_v4_Module_WIP.state_dict(),'optimizer': optimizer.state_dict()}, model_file_path)\n",
    "\n",
    "training_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#all_layerrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'size': '1',\n",
       " 'stride': '1',\n",
       " 'pad': '1',\n",
       " 'filters': '255',\n",
       " 'activation': 'linear'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_details[160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5,\n",
       " 2.0,\n",
       " 2.375,\n",
       " 4.5,\n",
       " 5.0,\n",
       " 3.5,\n",
       " 2.25,\n",
       " 4.6875,\n",
       " 4.75,\n",
       " 3.4375,\n",
       " 4.5,\n",
       " 9.125,\n",
       " 4.4375,\n",
       " 3.4375,\n",
       " 6.0,\n",
       " 7.59375,\n",
       " 14.34375,\n",
       " 12.53125]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all_layers\n",
    "n = [76, 38, 19]\n",
    "m = [608 / i for i in n for m in range(6)] \n",
    "print(m)\n",
    "\n",
    "\n",
    "anchor = [12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401]\n",
    "[anchor[i] / m[i] for i in range(len(m))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-16-13f47200f125>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-16-13f47200f125>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    self.YOLO_v4_layers.append(Conv_Layer_box(in_channel[i], out_channel[i], kernel_size= kernel_size[i], stride = stride[i], activation_func = activation_func[i], batch_normalization = batch_normalization[i]))\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "class YOLO_v4_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.YOLO_v4_layers = nn.ModuleList()\n",
    "        \n",
    "            self.YOLO_v4_layers.append(Conv_Layer_box(in_channel[i], out_channel[i], kernel_size= kernel_size[i], stride = stride[i], activation_func = activation_func[i], batch_normalization = batch_normalization[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class abc():\n",
    "    def __init__(self, qwe, out):\n",
    "        print(qwe)\n",
    "        \n",
    "abc(4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = \"1,2,3\"\n",
    "m = abc.split(\",\")\n",
    "m\n",
    "abc = \"1\"\n",
    "m = abc.split(\",\")\n",
    "m\n",
    "k = 4\n",
    "\n",
    "for r in k:\n",
    "    print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
