{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from __future__ import division\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "cfgfile = \"C:/Users/HX/Desktop/yolov4.cfg\"\n",
    "model_file_path = \"Model/model.pt\"\n",
    "\n",
    "def read_cfg_file(cfgfile):\n",
    "    file = open(cfgfile, 'r')\n",
    "    lines = file.read().split('\\n')\n",
    "\n",
    "    layer_type = []\n",
    "    layer_details = []\n",
    "    current_layer_details = {}\n",
    "    for line in lines:\n",
    "        #print(line)\n",
    "        if line == '':\n",
    "            continue\n",
    "        elif line[0] == '#':\n",
    "            continue\n",
    "        else:\n",
    "            if (line[0] == '['):\n",
    "                layer_type.append(line[1 : -1])\n",
    "                if current_layer_details != {}:\n",
    "                    layer_details.append(current_layer_details)\n",
    "                    current_layer_details = {}\n",
    "            else:\n",
    "                current_layer_details.update([(line.split(\"=\")[0].rstrip(), line.split(\"=\")[1].lstrip())])\n",
    "    layer_details.append(current_layer_details)\n",
    "    return layer_type, layer_details\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mish(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * torch.tanh(F.softplus(x))\n",
    "        return x\n",
    "\n",
    "class Conv_Layer_box(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size, stride, activation_func, batch_normalization):\n",
    "        super().__init__()\n",
    "        padding = (int((kernel_size - 1)/2), int((kernel_size - 1)/2))\n",
    "        #TBC: linear\n",
    "        dict_activation_func = {\"ReLU\": nn.ReLU(inplace=False),\n",
    "                                \"linear\": nn.ReLU(inplace=False),\n",
    "                                \"leaky\": nn.LeakyReLU(0.1, inplace=False),\n",
    "                                \"mish\": Mish()\n",
    "                               }\n",
    "        \n",
    "        if batch_normalization == True:\n",
    "            bias = False\n",
    "        else:\n",
    "            bias = True\n",
    "        self.conv_box = nn.ModuleList()\n",
    "        self.conv_box.append(nn.Conv2d(in_channel, out_channel, kernel_size, stride, padding, bias = bias))\n",
    "        if batch_normalization == True:\n",
    "            self.conv_box.append(nn.BatchNorm2d(out_channel))\n",
    "        self.conv_box.append(dict_activation_func[activation_func])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.conv_box:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class Maxpool_pad_Layer_box(nn.Module):\n",
    "    def __init__(self, maxpool_size):\n",
    "        super().__init__()\n",
    "        self.maxpool_size = maxpool_size\n",
    "        #why there are 2 padding??????????????\n",
    "        self.pad_1 = int((self.maxpool_size - 1) / 2)\n",
    "        self.pad_2 = self.pad_1\n",
    "    def forward(self, x):\n",
    "        x = F.pad(x, (self.pad_1, self.pad_2, self.pad_1, self.pad_2), mode='replicate')\n",
    "        x = F.max_pool2d(x, self.maxpool_size, stride=1)\n",
    "        return x\n",
    "    \n",
    "class Upsample_layer(nn.Module):\n",
    "    def __init__(self, stride):\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch, channel, height, width = x.data.size()\n",
    "        x = x.view(batch, channel, height, 1, width, 1).expand(batch, channel, height, self.stride, width, self.stride).clone()\n",
    "        x = x.contiguous().view(batch, channel, height * self.stride, width * self.stride).clone()\n",
    "        return x\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "class shortcut(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "class route(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_length(a_1, a_2, b_1, b_2):\n",
    "    if a_1 <=b_1 and a_2 >= b_1:\n",
    "        return (min(a_2, b_2) - b_1)\n",
    "    elif a_1 <=b_1 and a_2 <= b_1:\n",
    "        return 0\n",
    "    else:\n",
    "        return cross_length(b_1, b_2, a_1, a_2)\n",
    "\n",
    "def IoU(x_GT, y_GT, h_GT, w_GT, x_PD, y_PD, h_PD, w_PD):\n",
    "    area_of_I = cross_length(x_GT, x_GT + h_GT, x_PD, x_PD + h_PD) * cross_length(y_GT, y_GT + h_GT, y_PD, y_PD + h_PD)\n",
    "    area_of_U = h_GT * w_GT + h_PD * w_PD - area_of_I\n",
    "    return area_of_I / area_of_U\n",
    "\n",
    "\n",
    "\n",
    "def axis_conversion(x_centre, y_centre, h, w):\n",
    "    return (x_centre - h / 2, y_centre - w / 2, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.5, 4.5, 3, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "25/(64)\n",
    "axis_conversion(6, 6, 3,3)\n",
    "#IoU(11,11,8,8,12,12,5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yolo_input = np.array([1.0 for i in range(255 * 76 * 76 * 2)]).reshape(2, 255, 76, 76)\n",
    "Yolo_input = torch.from_numpy(Yolo_input)\n",
    "#input[:,:,0,0] = 2\n",
    "#input[:,:,0,0]\n",
    "anchors = [12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401]\n",
    "mask = [0, 1, 2]\n",
    "classes = 80\n",
    "input_image_size = 608"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_on_Matrix_x = torch.from_numpy(np.array([[i for j in range(19)] for i in range(19)]))\n",
    "#add_on_Matrix_y = [[i for i in range(19)] for j in range(19)]\n",
    "#add_on_Matrix_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n#Yolo_run_layer = Yolo(anchors, mask, classes, input_image_size)\\n\\n#b_x, b_y, b_w, b_h, objective_p, class_p = Yolo_run_layer(Yolo_input)\\n#combined_yolo_output = torch.cat((b_x, b_y, b_w, b_h, objective_p, class_p), 2)\\n\\nYolo_input = np.array([1.0 for i in range(255 * 76 * 76 * 2)]).reshape(2, 255, 76, 76)\\nYolo_input = torch.from_numpy(Yolo_input)\\ntarget = np.array([0 for i in range(3 * 76 * 76 * 85)])\\ninput_tensor = Yolo_input\\noutput_tensor = torch.Tensor(target)\\n\\nlearning_rate = 0.08\\nepoch_size = 5\\nsteps_for_printing_out_loss = 1\\n\\nYOLO_Module_WIP = Yolo(anchors, mask, classes, input_image_size)\\nYOLO_Module_WIP.cuda()\\n#Model_WIP.to(device)\\nloss_functioin = nn.MSELoss()\\noptimizer = optim.SGD(YOLO_Module_WIP.parameters(), lr = learning_rate)\\n\\ninput = input_tensor.cuda()\\ntarget = output_tensor.cuda()\\n\\ndef training_model():\\n    for i in range(1, epoch_size + 1):\\n        optimizer.zero_grad()\\n        output = YOLO_Module_WIP(input.cuda())\\n        print(output.size())\\n        #b_x, b_y, b_w, b_h, objective_p, class_p = YOLO_v4_Module_WIP(input.cuda())\\n        #output = b_x\\n        loss = loss_functioin(output, target.reshape(output.size(0), output.size(1), output.size(2), output.size(3), output.size(4)))\\n        loss.backward()\\n        optimizer.step()\\n        if i % (steps_for_printing_out_loss) == 0:\\n            print('Loss (epoch: ' + str(i) + '): ' + str(loss.cpu().detach().numpy()))\\n    torch.save({'state_dict': YOLO_v4_Module_WIP.state_dict(),'optimizer': optimizer.state_dict()}, model_file_path)\\n\\ntraining_model()\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Yolo(nn.Module):\n",
    "    def __init__(self, anchors, mask, classes, input_image_size):\n",
    "        super().__init__()\n",
    "        self.anchors = anchors\n",
    "        self.mask = mask\n",
    "        self.classes = classes\n",
    "        self.number_of_mask = len(mask)\n",
    "        self.input_image_size = input_image_size\n",
    "        #self.Sigmoid_layer = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        mask = self.mask\n",
    "        anchors = self.anchors\n",
    "        classes = self.classes\n",
    "        number_of_mask = self.number_of_mask\n",
    "        input_image_size = self.input_image_size\n",
    "        grid_size = int(input_image_size / x.size(2))\n",
    "        #print(grid_size)\n",
    "        #t_x = torch.from_numpy(np.array([0.0 for i in range(number_of_mask * x.size(0) * x.size(2) * x.size(3))]).reshape(number_of_mask * x.size(0), 1, x.size(2), x.size(3)))\n",
    "        t_x = [None for i in range(number_of_mask)]\n",
    "        t_y = [None for i in range(number_of_mask)]\n",
    "        t_w = [None for i in range(number_of_mask)]\n",
    "        t_h = [None for i in range(number_of_mask)]\n",
    "        objective_p = [None for i in range(number_of_mask)]\n",
    "        class_p = [None for i in range(number_of_mask)]\n",
    "        \n",
    "        #c_x = [i for i in range(x.size(2))]\n",
    "        #c_y = [i for i in range(x.size(2))]\n",
    "        \n",
    "        add_on_Matrix_x = torch.from_numpy(np.array([[i for j in range(x.size(2))] for i in range(x.size(2))])).cuda()\n",
    "        add_on_Matrix_y = torch.from_numpy(np.array([[i for i in range(x.size(2))] for j in range(x.size(2))])).cuda()\n",
    "        \n",
    "        b_x = [None for i in range(number_of_mask)]\n",
    "        b_y = [None for i in range(number_of_mask)]\n",
    "        b_w = [None for i in range(number_of_mask)]\n",
    "        b_h = [None for i in range(number_of_mask)]\n",
    "        \n",
    "        \n",
    "        anchor_shape_1 = int(len(anchors) / 2)\n",
    "        anchors = np.array(anchors).reshape(anchor_shape_1, 2)\n",
    "        \n",
    "        print(anchors)\n",
    "        \n",
    "        p_w = [None for i in range(number_of_mask)]\n",
    "        p_h = [None for i in range(number_of_mask)]\n",
    "        \n",
    "        \n",
    "        for i in range(number_of_mask):\n",
    "            start_point = i * (5 + classes)\n",
    "            end_point = (i + 1) * (5 + classes)\n",
    "            p_w[i], p_h[i] = anchors[mask[i]]\n",
    "            print(p_w)\n",
    "            print(p_h)\n",
    "            \n",
    "            t_x[i] = x[:, (start_point + 0) : (start_point + 1), :, :].clone()\n",
    "            \n",
    "            t_y[i] = x[:, (start_point + 1) : (start_point + 2), :, :].clone()\n",
    "            t_w[i] = x[:, (start_point + 2) : (start_point + 3), :, :].clone()\n",
    "            t_h[i] = x[:, (start_point + 3) : (start_point + 4), :, :].clone()\n",
    "            objective_p[i] = x[:, (start_point + 4) : (start_point + 5), :, :].clone()\n",
    "            class_p[i] = x[:, (start_point + 5) : end_point, :, :].clone()\n",
    "            #print(type(t_x[i]))\n",
    "            \n",
    "            print(F.sigmoid(t_x[i].clone()))\n",
    "            b_x[i] = F.sigmoid(t_x[i].clone()) + add_on_Matrix_x\n",
    "            print(b_x[i])\n",
    "            b_y[i] = F.sigmoid(t_y[i].clone()) + add_on_Matrix_y\n",
    "            \n",
    "            #b_x[i][0, 0, :, :] = b_x[i][0, 0, :, :].clone()\n",
    "            #b_y[i][0, 0, :, :] = b_y[i][0, 0, :, :].clone()\n",
    "            \n",
    "            \"\"\"\n",
    "            print(t_x[i].size())\n",
    "            for m in range(x.size(2)):\n",
    "                for n in range(x.size(2)):\n",
    "                    b_x[i][:, :, c_x[m], c_y[n]] = c_x[m] + b_x[i][:, :, c_x[m], c_y[n]].clone()\n",
    "                    b_y[i][:, :, c_x[m], c_y[n]] = c_y[n] + b_y[i][:, :, c_x[m], c_y[n]].clone()\n",
    "            \"\"\"\n",
    "            #need to think whether need to use below 2 lines\n",
    "            b_x[i] = grid_size * b_x[i].clone()\n",
    "            b_y[i] = grid_size * b_y[i].clone()\n",
    "            b_w[i] = p_w[i] * torch.exp(t_w[i].clone())\n",
    "            b_h[i] = p_h[i] * torch.exp(t_h[i].clone())\n",
    "            \n",
    "            objective_p[i] = F.sigmoid(objective_p[i].clone())\n",
    "            class_p[i] = F.sigmoid(class_p[i].clone())\n",
    "            #torch.reshape(t_x[i])\n",
    "        \n",
    "        b_x = torch.stack(b_x).clone()\n",
    "        b_y = torch.stack(b_y).clone()\n",
    "        b_w = torch.stack(b_w).clone()\n",
    "        b_h = torch.stack(b_h).clone()\n",
    "        objective_p = torch.stack(objective_p).clone()\n",
    "        class_p = torch.stack(class_p).clone()\n",
    "        combined_yolo_output = torch.cat((b_x, b_y, b_w, b_h, objective_p, class_p), 2)\n",
    "        #return b_x, b_y, b_w, b_h, objective_p, class_p\n",
    "        #return combined_yolo_output\n",
    "        \n",
    "        \n",
    "        #b_x = torch.stack(b_x).clone()\n",
    "        return combined_yolo_output\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#Yolo_run_layer = Yolo(anchors, mask, classes, input_image_size)\n",
    "\n",
    "#b_x, b_y, b_w, b_h, objective_p, class_p = Yolo_run_layer(Yolo_input)\n",
    "#combined_yolo_output = torch.cat((b_x, b_y, b_w, b_h, objective_p, class_p), 2)\n",
    "\n",
    "Yolo_input = np.array([1.0 for i in range(255 * 76 * 76 * 2)]).reshape(2, 255, 76, 76)\n",
    "Yolo_input = torch.from_numpy(Yolo_input)\n",
    "target = np.array([0 for i in range(3 * 76 * 76 * 85)])\n",
    "input_tensor = Yolo_input\n",
    "output_tensor = torch.Tensor(target)\n",
    "\n",
    "learning_rate = 0.08\n",
    "epoch_size = 5\n",
    "steps_for_printing_out_loss = 1\n",
    "\n",
    "YOLO_Module_WIP = Yolo(anchors, mask, classes, input_image_size)\n",
    "YOLO_Module_WIP.cuda()\n",
    "#Model_WIP.to(device)\n",
    "loss_functioin = nn.MSELoss()\n",
    "optimizer = optim.SGD(YOLO_Module_WIP.parameters(), lr = learning_rate)\n",
    "\n",
    "input = input_tensor.cuda()\n",
    "target = output_tensor.cuda()\n",
    "\n",
    "def training_model():\n",
    "    for i in range(1, epoch_size + 1):\n",
    "        optimizer.zero_grad()\n",
    "        output = YOLO_Module_WIP(input.cuda())\n",
    "        print(output.size())\n",
    "        #b_x, b_y, b_w, b_h, objective_p, class_p = YOLO_v4_Module_WIP(input.cuda())\n",
    "        #output = b_x\n",
    "        loss = loss_functioin(output, target.reshape(output.size(0), output.size(1), output.size(2), output.size(3), output.size(4)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % (steps_for_printing_out_loss) == 0:\n",
    "            print('Loss (epoch: ' + str(i) + '): ' + str(loss.cpu().detach().numpy()))\n",
    "    torch.save({'state_dict': YOLO_v4_Module_WIP.state_dict(),'optimizer': optimizer.state_dict()}, model_file_path)\n",
    "\n",
    "training_model()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_yolo_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n",
      "162\n",
      "{'batch_normalize': '1', 'filters': '64', 'size': '3', 'stride': '2', 'pad': '1', 'activation': 'mish'}\n"
     ]
    }
   ],
   "source": [
    "layer_type, layer_details = read_cfg_file(cfgfile)\n",
    "\n",
    "net_layer = layer_details[0]\n",
    "\n",
    "layer_type = layer_type[1:]\n",
    "layer_details = layer_details[1:]\n",
    "\n",
    "print(len(layer_type))\n",
    "print(len(layer_details))\n",
    "print(layer_details[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nanchor = 3\\n#yolo_layer = 3\\noutput = (19 ** 2) * (1 + 4 + 16)\\nanchor * output\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "anchor = 3\n",
    "#yolo_layer = 3\n",
    "output = (19 ** 2) * (1 + 4 + 16)\n",
    "anchor * output\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build module for entire YOLO\n",
    "class YOLO_v4_model(nn.Module):\n",
    "    def __init__(self, layer_details, layer_type):\n",
    "        super(YOLO_v4_model, self).__init__()\n",
    "        self.all_layers = nn.ModuleList()\n",
    "        all_layers = self.all_layers\n",
    "        self.layer_details = layer_details\n",
    "        self.layer_type = layer_type\n",
    "\n",
    "        for i in range(len(layer_type)):\n",
    "            if layer_type[i] == 'convolutional':\n",
    "                #print(layer_details[i])\n",
    "                #print(i)\n",
    "                try:\n",
    "                    if int(layer_details[i]['batch_normalize']) == 1:\n",
    "                        batch_normalize = True\n",
    "                    else:\n",
    "                        batch_normalize = False\n",
    "                except:\n",
    "                    batch_normalize = False\n",
    "                if i == 0:\n",
    "                    in_channel = 3\n",
    "                else:\n",
    "                    if layer_type[i - 1] == 'convolutional':\n",
    "                        skip_step = 0\n",
    "                    elif layer_type[i - 1] == 'shortcut':\n",
    "                        skip_step = int(layer_details[i - 1]['from'])\n",
    "                    elif layer_type[i - 1] == 'route':\n",
    "                        try:\n",
    "                            skip_step = int(layer_details[i - 1]['layers'].split(\",\")[0])\n",
    "                        except:\n",
    "                            skip_step = int(layer_details[i - 1]['layers'])\n",
    "                    #print(layer_details[i - 1 + skip_step])\n",
    "                    if skip_step > 0:\n",
    "                        in_channel = int(layer_details[skip_step]['filters'])\n",
    "                    else:\n",
    "                        in_channel = int(layer_details[i - 1 + skip_step]['filters'])\n",
    "                out_channel = int(layer_details[i]['filters'])\n",
    "                kernel_size = int(layer_details[i]['size'])\n",
    "                stride = int(layer_details[i]['stride'])\n",
    "                pad = int(layer_details[i]['pad'])\n",
    "                activation_func = layer_details[i]['activation']\n",
    "                layer = Conv_Layer_box(in_channel, out_channel, kernel_size, stride, activation_func, batch_normalize)\n",
    "                #print(layer)\n",
    "            elif layer_type[i] == 'maxpool':\n",
    "                layer_details[i].update([('filters', layer_details[i - 1]['filters'])])\n",
    "                maxpool_size = int(layer_details[i]['size'])\n",
    "                #print(maxpool_size)\n",
    "                layer = Maxpool_pad_Layer_box(maxpool_size)\n",
    "                #print(layer)\n",
    "            elif layer_type[i] == 'upsample':\n",
    "                layer_details[i].update([('filters', layer_details[i - 1]['filters'])])\n",
    "                stride = int(layer_details[i]['stride'])\n",
    "                layer = Upsample_layer(stride)\n",
    "            elif layer_type[i] == 'yolo':\n",
    "                #print(\"yolo\")\n",
    "                anchors = [int(x) for x in layer_details[i]['anchors'].split(\",\")]\n",
    "                \n",
    "                mask = [int(x) for x in layer_details[i]['mask'].split(\",\")]\n",
    "                classes = int(layer_details[i]['classes'])\n",
    "                #input image size = 608 for now\n",
    "                layer = Yolo(anchors, mask, classes, input_image_size)\n",
    "                #print(anchors)\n",
    "                #print(classes)\n",
    "                #print(input_image_size)\n",
    "                #continue\n",
    "            elif layer_type[i] == 'shortcut':\n",
    "                skip_step = int(layer_details[i]['from'])\n",
    "                layer_details[i].update([('filters', layer_details[i + skip_step]['filters'])])\n",
    "                layer = shortcut()\n",
    "            elif layer_type[i] == 'route':\n",
    "                try:\n",
    "                    skip_step = int(layer_details[i]['layers'].split(\",\")[0])\n",
    "                except:\n",
    "                    skip_step = int(layer_details[i]['layers'])\n",
    "                #print(skip_step)\n",
    "                if skip_step > 0:\n",
    "                    layer_details[i].update([('filters', layer_details[skip_step]['filters'])])\n",
    "                else:\n",
    "                    layer_details[i].update([('filters', layer_details[i + skip_step]['filters'])])\n",
    "                layer = route()\n",
    "            elif layer_type[i] == 'net':\n",
    "                #print(\"net\")\n",
    "                continue\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            all_layers.append(layer)\n",
    "        global all_layerrr\n",
    "        all_layerrr = all_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        all_layers = self.all_layers\n",
    "        layers_output = [None for i in range(len(layer_type))]\n",
    "        for i in range(len(layer_type)):\n",
    "            #print(i)\n",
    "            if i == 0:\n",
    "                layers_output[i] = all_layers[i](x)\n",
    "                continue\n",
    "                \n",
    "            elif layer_type[i] == 'yolo':\n",
    "                layers_output[i] = all_layers[i](layers_output[i - 1])\n",
    "                continue\n",
    "            elif layer_type[i] == 'convolutional' or layer_type[i] == 'maxpool' or layer_type[i] == 'upsample' or layer_type[i] == 'yolo':\n",
    "                layers_output[i] = all_layers[i](layers_output[i - 1])\n",
    "                \"\"\"\n",
    "                try:\n",
    "                    print(\"i: \" + str(i) + str(layers_output[i].size()))\n",
    "                except:\n",
    "                    print(\"go\")\n",
    "                \"\"\"\n",
    "                continue\n",
    "            elif layer_type[i] == 'shortcut':\n",
    "                skip_step = [int(layer_details[i]['from'])]\n",
    "            elif layer_type[i] == 'route':\n",
    "                skip_step = layer_details[i]['layers'].split(\",\")\n",
    "            for SS in skip_step:\n",
    "                SS = int(SS)\n",
    "                #print(\"SS\" + str(i) + str(SS))\n",
    "                #print(skip_step)\n",
    "                \n",
    "                #print(SS)\n",
    "                #print(layers_output[i])\n",
    "                #print(layers_output[i - 1 + SS])\n",
    "                \n",
    "                if SS > 0:\n",
    "                    if layers_output[i] == None:\n",
    "                        #print(layers_output[SS].size())\n",
    "                        layers_output[i] = layers_output[SS]\n",
    "                    else:\n",
    "                        #print(layers_output[SS].size())\n",
    "                        layers_output[i] += layers_output[SS]\n",
    "                else:\n",
    "                    #print(i + SS)\n",
    "                    if layers_output[i] == None:\n",
    "                        #print(layers_output[i + SS].size())\n",
    "                        layers_output[i] = layers_output[i + SS]\n",
    "                    else:\n",
    "                        \n",
    "                        #print(layers_output[i + SS].size())\n",
    "                        layers_output[i] += layers_output[i + SS]\n",
    "            \"\"\"\n",
    "            try:\n",
    "                print(\"i: \" + str(i) + str(layers_output[i].size()))\n",
    "            except:\n",
    "                print(\"go\")\n",
    "            \"\"\"\n",
    "        #print(layers_output[138].size())\n",
    "        return layers_output[161]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12  16]\n",
      " [ 19  36]\n",
      " [ 40  28]\n",
      " [ 36  75]\n",
      " [ 76  55]\n",
      " [ 72 146]\n",
      " [142 110]\n",
      " [192 243]\n",
      " [459 401]]\n",
      "[12, None, None]\n",
      "[16, None, None]\n",
      "tensor([[[[0.6661, 0.5000, 0.5728,  ..., 0.6531, 0.5000, 0.5000],\n",
      "          [0.7666, 0.8081, 0.8431,  ..., 0.5512, 0.9344, 0.9773],\n",
      "          [0.6206, 0.8418, 0.7212,  ..., 0.5005, 0.5000, 0.7480],\n",
      "          ...,\n",
      "          [0.7601, 0.5000, 0.5000,  ..., 0.6086, 0.8753, 0.7279],\n",
      "          [0.5677, 0.5000, 0.5000,  ..., 0.5742, 0.5000, 0.7921],\n",
      "          [0.5760, 0.9037, 0.6374,  ..., 0.6776, 0.7177, 0.8499]]]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.6661,  0.5000,  0.5728,  ...,  0.6531,  0.5000,  0.5000],\n",
      "          [ 1.7666,  1.8081,  1.8431,  ...,  1.5512,  1.9344,  1.9773],\n",
      "          [ 2.6206,  2.8418,  2.7212,  ...,  2.5005,  2.5000,  2.7480],\n",
      "          ...,\n",
      "          [73.7601, 73.5000, 73.5000,  ..., 73.6086, 73.8753, 73.7279],\n",
      "          [74.5677, 74.5000, 74.5000,  ..., 74.5742, 74.5000, 74.7921],\n",
      "          [75.5760, 75.9037, 75.6374,  ..., 75.6776, 75.7177, 75.8499]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "[12, 19, None]\n",
      "[16, 36, None]\n",
      "tensor([[[[0.5000, 0.7657, 0.6406,  ..., 0.5237, 0.9440, 0.6809],\n",
      "          [0.8772, 0.7908, 0.8713,  ..., 0.9393, 0.7584, 0.9545],\n",
      "          [0.5000, 0.5588, 0.8570,  ..., 0.6545, 0.6399, 0.7636],\n",
      "          ...,\n",
      "          [0.5780, 0.9341, 0.7996,  ..., 0.6894, 0.5000, 0.5000],\n",
      "          [0.5974, 0.9418, 0.8153,  ..., 0.9533, 0.7178, 0.6736],\n",
      "          [0.9796, 0.9930, 0.9716,  ..., 0.9316, 0.7849, 0.6748]]]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.5000,  0.7657,  0.6406,  ...,  0.5237,  0.9440,  0.6809],\n",
      "          [ 1.8772,  1.7908,  1.8713,  ...,  1.9393,  1.7584,  1.9545],\n",
      "          [ 2.5000,  2.5588,  2.8570,  ...,  2.6545,  2.6399,  2.7636],\n",
      "          ...,\n",
      "          [73.5780, 73.9341, 73.7996,  ..., 73.6894, 73.5000, 73.5000],\n",
      "          [74.5974, 74.9418, 74.8153,  ..., 74.9533, 74.7178, 74.6736],\n",
      "          [75.9796, 75.9930, 75.9716,  ..., 75.9316, 75.7849, 75.6748]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "[12, 19, 40]\n",
      "[16, 36, 28]\n",
      "tensor([[[[0.5280, 0.7953, 0.8243,  ..., 0.5000, 0.9116, 0.6053],\n",
      "          [0.7046, 0.5000, 0.6896,  ..., 0.8442, 0.5000, 0.9038],\n",
      "          [0.8809, 0.8136, 0.8573,  ..., 0.6080, 0.8484, 0.9047],\n",
      "          ...,\n",
      "          [0.7447, 0.8207, 0.5624,  ..., 0.7413, 0.5000, 0.5943],\n",
      "          [0.7999, 0.9125, 0.7925,  ..., 0.8826, 0.7631, 0.5786],\n",
      "          [0.5000, 0.6390, 0.8574,  ..., 0.7246, 0.5000, 0.7353]]]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.5280,  0.7953,  0.8243,  ...,  0.5000,  0.9116,  0.6053],\n",
      "          [ 1.7046,  1.5000,  1.6896,  ...,  1.8442,  1.5000,  1.9038],\n",
      "          [ 2.8809,  2.8136,  2.8573,  ...,  2.6080,  2.8484,  2.9047],\n",
      "          ...,\n",
      "          [73.7447, 73.8207, 73.5624,  ..., 73.7413, 73.5000, 73.5943],\n",
      "          [74.7999, 74.9125, 74.7925,  ..., 74.8826, 74.7631, 74.5786],\n",
      "          [75.5000, 75.6390, 75.8574,  ..., 75.7246, 75.5000, 75.7353]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "[[ 12  16]\n",
      " [ 19  36]\n",
      " [ 40  28]\n",
      " [ 36  75]\n",
      " [ 76  55]\n",
      " [ 72 146]\n",
      " [142 110]\n",
      " [192 243]\n",
      " [459 401]]\n",
      "[36, None, None]\n",
      "[75, None, None]\n",
      "tensor([[[[0.6353, 0.5000, 0.5000,  ..., 0.5601, 0.5000, 0.6704],\n",
      "          [0.6456, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.6609],\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.7608],\n",
      "          ...,\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.7699],\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.7262],\n",
      "          [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.6353,  0.5000,  0.5000,  ...,  0.5601,  0.5000,  0.6704],\n",
      "          [ 1.6456,  1.5000,  1.5000,  ...,  1.5000,  1.5000,  1.6609],\n",
      "          [ 2.5000,  2.5000,  2.5000,  ...,  2.5000,  2.5000,  2.7608],\n",
      "          ...,\n",
      "          [35.5000, 35.5000, 35.5000,  ..., 35.5000, 35.5000, 35.7699],\n",
      "          [36.5000, 36.5000, 36.5000,  ..., 36.5000, 36.5000, 36.7262],\n",
      "          [37.5000, 37.5000, 37.5000,  ..., 37.5000, 37.5000, 37.5000]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "[36, 76, None]\n",
      "[75, 55, None]\n",
      "tensor([[[[0.6649, 0.5169, 0.7295,  ..., 0.5000, 0.5324, 0.8066],\n",
      "          [0.5000, 0.7456, 0.6279,  ..., 0.5079, 0.8229, 0.6526],\n",
      "          [0.5000, 0.6963, 0.7187,  ..., 0.5000, 0.5000, 0.6337],\n",
      "          ...,\n",
      "          [0.5971, 0.5000, 0.5335,  ..., 0.7120, 0.5475, 0.5000],\n",
      "          [0.5035, 0.7335, 0.5748,  ..., 0.5000, 0.6790, 0.5000],\n",
      "          [0.7103, 0.7252, 0.7935,  ..., 0.5000, 0.6824, 0.6470]]]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.6649,  0.5169,  0.7295,  ...,  0.5000,  0.5324,  0.8066],\n",
      "          [ 1.5000,  1.7456,  1.6279,  ...,  1.5079,  1.8229,  1.6526],\n",
      "          [ 2.5000,  2.6963,  2.7187,  ...,  2.5000,  2.5000,  2.6337],\n",
      "          ...,\n",
      "          [35.5971, 35.5000, 35.5335,  ..., 35.7120, 35.5475, 35.5000],\n",
      "          [36.5035, 36.7335, 36.5748,  ..., 36.5000, 36.6790, 36.5000],\n",
      "          [37.7103, 37.7252, 37.7935,  ..., 37.5000, 37.6824, 37.6470]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "[36, 76, 72]\n",
      "[75, 55, 146]\n",
      "tensor([[[[0.5000, 0.7584, 0.5000,  ..., 0.5000, 0.5125, 0.6795],\n",
      "          [0.5000, 0.6227, 0.6331,  ..., 0.5500, 0.5000, 0.5000],\n",
      "          [0.7221, 0.5000, 0.5891,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          ...,\n",
      "          [0.5000, 0.7365, 0.5000,  ..., 0.5554, 0.5642, 0.5729],\n",
      "          [0.5000, 0.6721, 0.5000,  ..., 0.7875, 0.6040, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5393,  ..., 0.5000, 0.5000, 0.6591]]]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.5000,  0.7584,  0.5000,  ...,  0.5000,  0.5125,  0.6795],\n",
      "          [ 1.5000,  1.6227,  1.6331,  ...,  1.5500,  1.5000,  1.5000],\n",
      "          [ 2.7221,  2.5000,  2.5891,  ...,  2.5000,  2.5000,  2.5000],\n",
      "          ...,\n",
      "          [35.5000, 35.7365, 35.5000,  ..., 35.5554, 35.5642, 35.5729],\n",
      "          [36.5000, 36.6721, 36.5000,  ..., 36.7875, 36.6040, 36.5000],\n",
      "          [37.5000, 37.5000, 37.5393,  ..., 37.5000, 37.5000, 37.6591]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "[[ 12  16]\n",
      " [ 19  36]\n",
      " [ 40  28]\n",
      " [ 36  75]\n",
      " [ 76  55]\n",
      " [ 72 146]\n",
      " [142 110]\n",
      " [192 243]\n",
      " [459 401]]\n",
      "[142, None, None]\n",
      "[110, None, None]\n",
      "tensor([[[[0.6534, 0.7209, 0.5133, 0.5000, 0.6202, 0.5298, 0.5829, 0.6005,\n",
      "           0.5000, 0.5000, 0.5000, 0.5876, 0.6035, 0.5298, 0.5322, 0.6069,\n",
      "           0.5332, 0.6863, 0.6130],\n",
      "          [0.6498, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5104, 0.5000, 0.6871, 0.6697, 0.7043, 0.6562, 0.7172, 0.6337,\n",
      "           0.6377, 0.5000, 0.6651],\n",
      "          [0.7375, 0.5000, 0.6561, 0.5000, 0.5000, 0.7064, 0.6241, 0.5224,\n",
      "           0.5331, 0.5103, 0.5432, 0.6806, 0.6860, 0.5264, 0.8012, 0.7137,\n",
      "           0.6199, 0.7338, 0.5000],\n",
      "          [0.7543, 0.7105, 0.6711, 0.5000, 0.5178, 0.5000, 0.6481, 0.5249,\n",
      "           0.5000, 0.5180, 0.5000, 0.6743, 0.6900, 0.6721, 0.7194, 0.7379,\n",
      "           0.5588, 0.6064, 0.5000],\n",
      "          [0.6136, 0.6135, 0.5834, 0.5421, 0.5657, 0.5000, 0.5049, 0.6112,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5661, 0.5633, 0.7116, 0.5864,\n",
      "           0.6780, 0.5650, 0.5000],\n",
      "          [0.6151, 0.5845, 0.5617, 0.5000, 0.5000, 0.5774, 0.5000, 0.5000,\n",
      "           0.5000, 0.6113, 0.5580, 0.5000, 0.5284, 0.5000, 0.7117, 0.5884,\n",
      "           0.7540, 0.5000, 0.5000],\n",
      "          [0.5298, 0.6365, 0.5801, 0.6269, 0.5421, 0.5929, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5240, 0.5775, 0.5000, 0.5495, 0.5000, 0.5745,\n",
      "           0.5607, 0.5489, 0.5000],\n",
      "          [0.5000, 0.5094, 0.5000, 0.7367, 0.5900, 0.6245, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5958, 0.5000, 0.5000, 0.5000, 0.5831,\n",
      "           0.5998, 0.5837, 0.5000],\n",
      "          [0.5868, 0.5306, 0.5000, 0.5427, 0.5000, 0.5191, 0.5000, 0.5000,\n",
      "           0.5000, 0.5382, 0.5862, 0.5853, 0.5425, 0.5000, 0.5706, 0.5848,\n",
      "           0.6367, 0.5279, 0.5598],\n",
      "          [0.5325, 0.5145, 0.6011, 0.5201, 0.5000, 0.5719, 0.5000, 0.5321,\n",
      "           0.5000, 0.5393, 0.5207, 0.5186, 0.5588, 0.5702, 0.5429, 0.6303,\n",
      "           0.6537, 0.6011, 0.5056],\n",
      "          [0.5897, 0.7140, 0.5627, 0.5867, 0.5865, 0.5000, 0.5358, 0.5382,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5104, 0.5000, 0.5046,\n",
      "           0.5125, 0.5000, 0.5000],\n",
      "          [0.6813, 0.5754, 0.5716, 0.5000, 0.6140, 0.5130, 0.5338, 0.5634,\n",
      "           0.5252, 0.5322, 0.5675, 0.5000, 0.5000, 0.5537, 0.5000, 0.5785,\n",
      "           0.6532, 0.5693, 0.6049],\n",
      "          [0.5000, 0.6349, 0.5472, 0.6162, 0.6308, 0.5545, 0.5578, 0.5757,\n",
      "           0.5696, 0.5238, 0.5195, 0.5000, 0.5782, 0.6045, 0.5656, 0.6206,\n",
      "           0.6715, 0.5457, 0.5000],\n",
      "          [0.5000, 0.6925, 0.5668, 0.5898, 0.5892, 0.5795, 0.6389, 0.5141,\n",
      "           0.6005, 0.5952, 0.5000, 0.5162, 0.5000, 0.5000, 0.5405, 0.5000,\n",
      "           0.5354, 0.6433, 0.5000],\n",
      "          [0.5805, 0.5000, 0.5000, 0.5000, 0.5763, 0.6064, 0.5485, 0.6237,\n",
      "           0.5454, 0.5000, 0.5492, 0.5000, 0.6010, 0.6017, 0.5879, 0.6394,\n",
      "           0.5168, 0.5244, 0.5000],\n",
      "          [0.6117, 0.5483, 0.5438, 0.5000, 0.6371, 0.5279, 0.6423, 0.5345,\n",
      "           0.5000, 0.5819, 0.5945, 0.5907, 0.6462, 0.6982, 0.5000, 0.6108,\n",
      "           0.6015, 0.5000, 0.5000],\n",
      "          [0.5485, 0.5536, 0.5388, 0.5000, 0.5686, 0.6177, 0.5678, 0.5000,\n",
      "           0.5974, 0.5000, 0.5852, 0.5070, 0.5000, 0.6649, 0.6129, 0.5575,\n",
      "           0.6764, 0.6495, 0.5000],\n",
      "          [0.6179, 0.5452, 0.7305, 0.5075, 0.5753, 0.6257, 0.6066, 0.5036,\n",
      "           0.5402, 0.5743, 0.5793, 0.6493, 0.5000, 0.7307, 0.5000, 0.6712,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.7281, 0.5679, 0.5547, 0.5414, 0.5824, 0.5000, 0.5000, 0.5380,\n",
      "           0.5000, 0.5857, 0.5000, 0.5445, 0.5012, 0.5800, 0.5340, 0.5522,\n",
      "           0.5204, 0.6649, 0.5360]]]], device='cuda:0',\n",
      "       grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.6534,  0.7209,  0.5133,  0.5000,  0.6202,  0.5298,  0.5829,\n",
      "            0.6005,  0.5000,  0.5000,  0.5000,  0.5876,  0.6035,  0.5298,\n",
      "            0.5322,  0.6069,  0.5332,  0.6863,  0.6130],\n",
      "          [ 1.6498,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,\n",
      "            1.5000,  1.5104,  1.5000,  1.6871,  1.6697,  1.7043,  1.6562,\n",
      "            1.7172,  1.6337,  1.6377,  1.5000,  1.6651],\n",
      "          [ 2.7375,  2.5000,  2.6561,  2.5000,  2.5000,  2.7064,  2.6241,\n",
      "            2.5224,  2.5331,  2.5103,  2.5432,  2.6806,  2.6860,  2.5264,\n",
      "            2.8012,  2.7137,  2.6199,  2.7338,  2.5000],\n",
      "          [ 3.7543,  3.7105,  3.6711,  3.5000,  3.5178,  3.5000,  3.6481,\n",
      "            3.5249,  3.5000,  3.5180,  3.5000,  3.6743,  3.6900,  3.6721,\n",
      "            3.7194,  3.7379,  3.5588,  3.6064,  3.5000],\n",
      "          [ 4.6136,  4.6135,  4.5834,  4.5421,  4.5657,  4.5000,  4.5049,\n",
      "            4.6112,  4.5000,  4.5000,  4.5000,  4.5000,  4.5661,  4.5633,\n",
      "            4.7116,  4.5864,  4.6780,  4.5650,  4.5000],\n",
      "          [ 5.6151,  5.5845,  5.5617,  5.5000,  5.5000,  5.5774,  5.5000,\n",
      "            5.5000,  5.5000,  5.6113,  5.5580,  5.5000,  5.5284,  5.5000,\n",
      "            5.7117,  5.5884,  5.7540,  5.5000,  5.5000],\n",
      "          [ 6.5298,  6.6365,  6.5801,  6.6269,  6.5421,  6.5929,  6.5000,\n",
      "            6.5000,  6.5000,  6.5000,  6.5240,  6.5775,  6.5000,  6.5495,\n",
      "            6.5000,  6.5745,  6.5607,  6.5489,  6.5000],\n",
      "          [ 7.5000,  7.5094,  7.5000,  7.7367,  7.5900,  7.6245,  7.5000,\n",
      "            7.5000,  7.5000,  7.5000,  7.5000,  7.5958,  7.5000,  7.5000,\n",
      "            7.5000,  7.5831,  7.5998,  7.5837,  7.5000],\n",
      "          [ 8.5868,  8.5306,  8.5000,  8.5427,  8.5000,  8.5191,  8.5000,\n",
      "            8.5000,  8.5000,  8.5382,  8.5862,  8.5853,  8.5425,  8.5000,\n",
      "            8.5706,  8.5848,  8.6367,  8.5279,  8.5598],\n",
      "          [ 9.5325,  9.5145,  9.6011,  9.5201,  9.5000,  9.5719,  9.5000,\n",
      "            9.5321,  9.5000,  9.5393,  9.5207,  9.5186,  9.5588,  9.5702,\n",
      "            9.5429,  9.6303,  9.6537,  9.6011,  9.5056],\n",
      "          [10.5897, 10.7140, 10.5627, 10.5867, 10.5865, 10.5000, 10.5358,\n",
      "           10.5382, 10.5000, 10.5000, 10.5000, 10.5000, 10.5000, 10.5104,\n",
      "           10.5000, 10.5046, 10.5125, 10.5000, 10.5000],\n",
      "          [11.6813, 11.5754, 11.5716, 11.5000, 11.6140, 11.5130, 11.5338,\n",
      "           11.5634, 11.5252, 11.5322, 11.5675, 11.5000, 11.5000, 11.5537,\n",
      "           11.5000, 11.5785, 11.6532, 11.5693, 11.6049],\n",
      "          [12.5000, 12.6349, 12.5472, 12.6162, 12.6308, 12.5545, 12.5578,\n",
      "           12.5757, 12.5696, 12.5238, 12.5195, 12.5000, 12.5782, 12.6045,\n",
      "           12.5656, 12.6206, 12.6715, 12.5457, 12.5000],\n",
      "          [13.5000, 13.6925, 13.5668, 13.5898, 13.5892, 13.5795, 13.6389,\n",
      "           13.5141, 13.6005, 13.5952, 13.5000, 13.5162, 13.5000, 13.5000,\n",
      "           13.5405, 13.5000, 13.5354, 13.6433, 13.5000],\n",
      "          [14.5805, 14.5000, 14.5000, 14.5000, 14.5763, 14.6064, 14.5485,\n",
      "           14.6237, 14.5454, 14.5000, 14.5492, 14.5000, 14.6010, 14.6017,\n",
      "           14.5879, 14.6394, 14.5168, 14.5244, 14.5000],\n",
      "          [15.6117, 15.5483, 15.5438, 15.5000, 15.6371, 15.5279, 15.6423,\n",
      "           15.5345, 15.5000, 15.5819, 15.5945, 15.5907, 15.6462, 15.6982,\n",
      "           15.5000, 15.6108, 15.6015, 15.5000, 15.5000],\n",
      "          [16.5485, 16.5536, 16.5388, 16.5000, 16.5686, 16.6177, 16.5678,\n",
      "           16.5000, 16.5974, 16.5000, 16.5852, 16.5070, 16.5000, 16.6649,\n",
      "           16.6129, 16.5575, 16.6764, 16.6495, 16.5000],\n",
      "          [17.6179, 17.5452, 17.7305, 17.5075, 17.5753, 17.6257, 17.6066,\n",
      "           17.5036, 17.5402, 17.5743, 17.5793, 17.6493, 17.5000, 17.7307,\n",
      "           17.5000, 17.6712, 17.5000, 17.5000, 17.5000],\n",
      "          [18.7281, 18.5679, 18.5547, 18.5414, 18.5824, 18.5000, 18.5000,\n",
      "           18.5380, 18.5000, 18.5857, 18.5000, 18.5445, 18.5012, 18.5800,\n",
      "           18.5340, 18.5522, 18.5204, 18.6649, 18.5360]]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "[142, 192, None]\n",
      "[110, 243, None]\n",
      "tensor([[[[0.5000, 0.5000, 0.5562, 0.5000, 0.5000, 0.6421, 0.5096, 0.5000,\n",
      "           0.5000, 0.5529, 0.6422, 0.6050, 0.5343, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.6455],\n",
      "          [0.5722, 0.5000, 0.7988, 0.5157, 0.5905, 0.6175, 0.5736, 0.5000,\n",
      "           0.5441, 0.5000, 0.6682, 0.5828, 0.5777, 0.5236, 0.5000, 0.5000,\n",
      "           0.6083, 0.6176, 0.5043],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.6061, 0.5773, 0.5850,\n",
      "           0.5643, 0.5000, 0.5000, 0.5690, 0.5358, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.6135, 0.5000],\n",
      "          [0.5000, 0.5878, 0.5000, 0.6130, 0.5000, 0.6852, 0.5962, 0.6554,\n",
      "           0.5000, 0.5000, 0.5000, 0.5242, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5227, 0.5199],\n",
      "          [0.5716, 0.5437, 0.5000, 0.5000, 0.5000, 0.6073, 0.5194, 0.5000,\n",
      "           0.5719, 0.5000, 0.5000, 0.5021, 0.5000, 0.5000, 0.5183, 0.5048,\n",
      "           0.5169, 0.5000, 0.6127],\n",
      "          [0.5074, 0.5000, 0.5316, 0.5557, 0.5000, 0.5000, 0.5967, 0.5431,\n",
      "           0.5000, 0.5362, 0.5053, 0.5852, 0.5417, 0.5000, 0.5610, 0.5000,\n",
      "           0.5000, 0.5000, 0.5162],\n",
      "          [0.6398, 0.5000, 0.5000, 0.6259, 0.5368, 0.5000, 0.5068, 0.5000,\n",
      "           0.5800, 0.5270, 0.6084, 0.5000, 0.6270, 0.5000, 0.5000, 0.5167,\n",
      "           0.5000, 0.5451, 0.5813],\n",
      "          [0.5282, 0.5069, 0.6943, 0.5000, 0.6400, 0.5000, 0.5555, 0.5000,\n",
      "           0.5479, 0.5246, 0.5000, 0.5000, 0.6083, 0.5479, 0.5000, 0.6147,\n",
      "           0.5288, 0.6146, 0.6327],\n",
      "          [0.5241, 0.5000, 0.5145, 0.5105, 0.5000, 0.5000, 0.5000, 0.5127,\n",
      "           0.5477, 0.5459, 0.5528, 0.5420, 0.5998, 0.5040, 0.5000, 0.6294,\n",
      "           0.5689, 0.5000, 0.6417],\n",
      "          [0.5000, 0.6223, 0.5000, 0.5193, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.6144, 0.6240, 0.5433, 0.5000,\n",
      "           0.5000, 0.5000, 0.5156],\n",
      "          [0.5000, 0.5127, 0.5346, 0.5000, 0.5762, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5094, 0.5000, 0.5000, 0.5100, 0.5000,\n",
      "           0.6042, 0.5000, 0.6088],\n",
      "          [0.5000, 0.5267, 0.5000, 0.5586, 0.5637, 0.5602, 0.5000, 0.5125,\n",
      "           0.5000, 0.5000, 0.5000, 0.5209, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5234, 0.5525],\n",
      "          [0.5000, 0.5326, 0.5000, 0.5200, 0.5000, 0.5513, 0.5484, 0.5000,\n",
      "           0.5260, 0.5000, 0.5702, 0.5438, 0.5864, 0.5000, 0.5207, 0.5000,\n",
      "           0.6507, 0.5000, 0.5000],\n",
      "          [0.5347, 0.5000, 0.6305, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.6435, 0.6341, 0.5780, 0.5991, 0.5000, 0.5164, 0.5000,\n",
      "           0.5000, 0.5000, 0.5317],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.6072, 0.5000, 0.5000, 0.5000,\n",
      "           0.5838, 0.5000, 0.5713, 0.5781, 0.5067, 0.6118, 0.5568, 0.5664,\n",
      "           0.5463, 0.5392, 0.5760],\n",
      "          [0.5000, 0.5000, 0.5556, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5439, 0.5246, 0.6088, 0.5000, 0.5906, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5637],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5544, 0.5000, 0.5000, 0.5000, 0.5055,\n",
      "           0.5494, 0.5000, 0.6163, 0.5136, 0.5925, 0.5230, 0.5000, 0.5371,\n",
      "           0.5000, 0.5000, 0.5083],\n",
      "          [0.5000, 0.6415, 0.5000, 0.5000, 0.5279, 0.5000, 0.5823, 0.5535,\n",
      "           0.5000, 0.5000, 0.5000, 0.5451, 0.5733, 0.5109, 0.5000, 0.5000,\n",
      "           0.5216, 0.5000, 0.5659],\n",
      "          [0.5025, 0.5709, 0.5000, 0.5710, 0.5000, 0.5578, 0.5005, 0.5000,\n",
      "           0.5694, 0.5000, 0.5000, 0.5000, 0.5000, 0.5184, 0.5000, 0.5381,\n",
      "           0.5000, 0.5000, 0.5000]]]], device='cuda:0',\n",
      "       grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.5000,  0.5000,  0.5562,  0.5000,  0.5000,  0.6421,  0.5096,\n",
      "            0.5000,  0.5000,  0.5529,  0.6422,  0.6050,  0.5343,  0.5000,\n",
      "            0.5000,  0.5000,  0.5000,  0.5000,  0.6455],\n",
      "          [ 1.5722,  1.5000,  1.7988,  1.5157,  1.5905,  1.6175,  1.5736,\n",
      "            1.5000,  1.5441,  1.5000,  1.6682,  1.5828,  1.5777,  1.5236,\n",
      "            1.5000,  1.5000,  1.6083,  1.6176,  1.5043],\n",
      "          [ 2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.6061,  2.5773,\n",
      "            2.5850,  2.5643,  2.5000,  2.5000,  2.5690,  2.5358,  2.5000,\n",
      "            2.5000,  2.5000,  2.5000,  2.6135,  2.5000],\n",
      "          [ 3.5000,  3.5878,  3.5000,  3.6130,  3.5000,  3.6852,  3.5962,\n",
      "            3.6554,  3.5000,  3.5000,  3.5000,  3.5242,  3.5000,  3.5000,\n",
      "            3.5000,  3.5000,  3.5000,  3.5227,  3.5199],\n",
      "          [ 4.5716,  4.5437,  4.5000,  4.5000,  4.5000,  4.6073,  4.5194,\n",
      "            4.5000,  4.5719,  4.5000,  4.5000,  4.5021,  4.5000,  4.5000,\n",
      "            4.5183,  4.5048,  4.5169,  4.5000,  4.6127],\n",
      "          [ 5.5074,  5.5000,  5.5316,  5.5557,  5.5000,  5.5000,  5.5967,\n",
      "            5.5431,  5.5000,  5.5362,  5.5053,  5.5852,  5.5417,  5.5000,\n",
      "            5.5610,  5.5000,  5.5000,  5.5000,  5.5162],\n",
      "          [ 6.6398,  6.5000,  6.5000,  6.6259,  6.5368,  6.5000,  6.5068,\n",
      "            6.5000,  6.5800,  6.5270,  6.6084,  6.5000,  6.6270,  6.5000,\n",
      "            6.5000,  6.5167,  6.5000,  6.5451,  6.5813],\n",
      "          [ 7.5282,  7.5069,  7.6943,  7.5000,  7.6400,  7.5000,  7.5555,\n",
      "            7.5000,  7.5479,  7.5246,  7.5000,  7.5000,  7.6083,  7.5479,\n",
      "            7.5000,  7.6147,  7.5288,  7.6146,  7.6327],\n",
      "          [ 8.5241,  8.5000,  8.5145,  8.5105,  8.5000,  8.5000,  8.5000,\n",
      "            8.5127,  8.5477,  8.5459,  8.5528,  8.5420,  8.5998,  8.5040,\n",
      "            8.5000,  8.6294,  8.5689,  8.5000,  8.6417],\n",
      "          [ 9.5000,  9.6223,  9.5000,  9.5193,  9.5000,  9.5000,  9.5000,\n",
      "            9.5000,  9.5000,  9.5000,  9.5000,  9.5000,  9.6144,  9.6240,\n",
      "            9.5433,  9.5000,  9.5000,  9.5000,  9.5156],\n",
      "          [10.5000, 10.5127, 10.5346, 10.5000, 10.5763, 10.5000, 10.5000,\n",
      "           10.5000, 10.5000, 10.5000, 10.5000, 10.5094, 10.5000, 10.5000,\n",
      "           10.5100, 10.5000, 10.6042, 10.5000, 10.6088],\n",
      "          [11.5000, 11.5267, 11.5000, 11.5586, 11.5637, 11.5602, 11.5000,\n",
      "           11.5125, 11.5000, 11.5000, 11.5000, 11.5209, 11.5000, 11.5000,\n",
      "           11.5000, 11.5000, 11.5000, 11.5234, 11.5525],\n",
      "          [12.5000, 12.5326, 12.5000, 12.5200, 12.5000, 12.5513, 12.5484,\n",
      "           12.5000, 12.5260, 12.5000, 12.5702, 12.5438, 12.5864, 12.5000,\n",
      "           12.5207, 12.5000, 12.6507, 12.5000, 12.5000],\n",
      "          [13.5347, 13.5000, 13.6305, 13.5000, 13.5000, 13.5000, 13.5000,\n",
      "           13.5000, 13.5000, 13.6435, 13.6341, 13.5780, 13.5991, 13.5000,\n",
      "           13.5164, 13.5000, 13.5000, 13.5000, 13.5317],\n",
      "          [14.5000, 14.5000, 14.5000, 14.5000, 14.6072, 14.5000, 14.5000,\n",
      "           14.5000, 14.5838, 14.5000, 14.5713, 14.5781, 14.5067, 14.6118,\n",
      "           14.5568, 14.5664, 14.5463, 14.5392, 14.5760],\n",
      "          [15.5000, 15.5000, 15.5556, 15.5000, 15.5000, 15.5000, 15.5000,\n",
      "           15.5000, 15.5439, 15.5246, 15.6088, 15.5000, 15.5906, 15.5000,\n",
      "           15.5000, 15.5000, 15.5000, 15.5000, 15.5637],\n",
      "          [16.5000, 16.5000, 16.5000, 16.5543, 16.5000, 16.5000, 16.5000,\n",
      "           16.5055, 16.5494, 16.5000, 16.6163, 16.5136, 16.5925, 16.5230,\n",
      "           16.5000, 16.5371, 16.5000, 16.5000, 16.5083],\n",
      "          [17.5000, 17.6415, 17.5000, 17.5000, 17.5279, 17.5000, 17.5823,\n",
      "           17.5535, 17.5000, 17.5000, 17.5000, 17.5451, 17.5733, 17.5109,\n",
      "           17.5000, 17.5000, 17.5216, 17.5000, 17.5659],\n",
      "          [18.5025, 18.5709, 18.5000, 18.5710, 18.5000, 18.5578, 18.5005,\n",
      "           18.5000, 18.5694, 18.5000, 18.5000, 18.5000, 18.5000, 18.5184,\n",
      "           18.5000, 18.5381, 18.5000, 18.5000, 18.5000]]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "[142, 192, 459]\n",
      "[110, 243, 401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.5000, 0.6038, 0.6476, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5365, 0.5194, 0.5000, 0.5000, 0.5000, 0.5000, 0.5137,\n",
      "           0.5000, 0.5902, 0.5970],\n",
      "          [0.6394, 0.5000, 0.7759, 0.6099, 0.5000, 0.5164, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5700, 0.5337, 0.5126, 0.5000, 0.5000, 0.5972,\n",
      "           0.5419, 0.7080, 0.6082],\n",
      "          [0.6102, 0.5000, 0.5000, 0.5000, 0.5000, 0.6135, 0.5125, 0.5855,\n",
      "           0.5961, 0.6285, 0.5099, 0.5000, 0.5000, 0.5000, 0.7026, 0.6365,\n",
      "           0.6989, 0.5000, 0.5000],\n",
      "          [0.6343, 0.5973, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5099,\n",
      "           0.5000, 0.5553, 0.5283, 0.6755, 0.6568, 0.6109, 0.5146, 0.5107,\n",
      "           0.7231, 0.5000, 0.5306],\n",
      "          [0.6820, 0.6091, 0.5534, 0.6082, 0.5130, 0.5000, 0.5021, 0.5818,\n",
      "           0.5000, 0.5000, 0.5000, 0.6432, 0.5876, 0.5293, 0.6433, 0.6484,\n",
      "           0.5000, 0.5796, 0.5867],\n",
      "          [0.6668, 0.5000, 0.6585, 0.6598, 0.5867, 0.5000, 0.5783, 0.5000,\n",
      "           0.5709, 0.6154, 0.5982, 0.5000, 0.5373, 0.5589, 0.6276, 0.5000,\n",
      "           0.5402, 0.7096, 0.5999],\n",
      "          [0.6063, 0.6788, 0.6072, 0.5061, 0.5462, 0.5153, 0.5000, 0.5121,\n",
      "           0.5200, 0.5330, 0.5000, 0.5322, 0.5000, 0.5433, 0.6051, 0.6600,\n",
      "           0.6211, 0.5757, 0.5280],\n",
      "          [0.6204, 0.7518, 0.6449, 0.6228, 0.6212, 0.6475, 0.5983, 0.5088,\n",
      "           0.5097, 0.5235, 0.5357, 0.5771, 0.6228, 0.5000, 0.6264, 0.5107,\n",
      "           0.5777, 0.6151, 0.5316],\n",
      "          [0.6400, 0.5319, 0.5628, 0.6712, 0.5000, 0.5000, 0.5594, 0.5640,\n",
      "           0.6171, 0.5291, 0.6990, 0.5591, 0.5832, 0.5000, 0.5206, 0.5920,\n",
      "           0.5232, 0.5344, 0.5378],\n",
      "          [0.5281, 0.5546, 0.6374, 0.6623, 0.5189, 0.5533, 0.5759, 0.6089,\n",
      "           0.6226, 0.5302, 0.5128, 0.6552, 0.5164, 0.6173, 0.5059, 0.5077,\n",
      "           0.5347, 0.5971, 0.5000],\n",
      "          [0.5672, 0.5525, 0.5983, 0.6049, 0.5000, 0.5531, 0.5204, 0.5639,\n",
      "           0.5629, 0.5031, 0.5000, 0.5349, 0.5746, 0.5000, 0.5000, 0.5738,\n",
      "           0.5731, 0.6051, 0.5246],\n",
      "          [0.5696, 0.5758, 0.6778, 0.5962, 0.5069, 0.6569, 0.5688, 0.5000,\n",
      "           0.5429, 0.5203, 0.5977, 0.5675, 0.5000, 0.5925, 0.5000, 0.5328,\n",
      "           0.5575, 0.5165, 0.5132],\n",
      "          [0.6810, 0.5443, 0.5000, 0.6174, 0.6757, 0.5826, 0.6927, 0.5333,\n",
      "           0.5423, 0.6460, 0.5000, 0.5379, 0.5522, 0.5265, 0.5324, 0.6493,\n",
      "           0.6653, 0.5286, 0.6129],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5504, 0.5995, 0.5000,\n",
      "           0.5867, 0.5978, 0.5000, 0.5997, 0.5490, 0.5000, 0.5000, 0.5000,\n",
      "           0.5580, 0.5114, 0.5000],\n",
      "          [0.6094, 0.5861, 0.5000, 0.5376, 0.5144, 0.6233, 0.6211, 0.7442,\n",
      "           0.6428, 0.6281, 0.5952, 0.5455, 0.5648, 0.5614, 0.5000, 0.6159,\n",
      "           0.6407, 0.5000, 0.5894],\n",
      "          [0.5388, 0.5750, 0.5000, 0.5000, 0.5454, 0.6095, 0.5000, 0.5077,\n",
      "           0.7372, 0.5060, 0.5468, 0.5380, 0.6206, 0.5000, 0.5180, 0.5000,\n",
      "           0.5000, 0.5000, 0.5699],\n",
      "          [0.5948, 0.7215, 0.7138, 0.6007, 0.6296, 0.5000, 0.5847, 0.5881,\n",
      "           0.5381, 0.5000, 0.5268, 0.5000, 0.5000, 0.6216, 0.5000, 0.5000,\n",
      "           0.5719, 0.5975, 0.6329],\n",
      "          [0.6982, 0.5586, 0.5670, 0.5099, 0.5872, 0.5000, 0.5000, 0.5241,\n",
      "           0.5000, 0.5557, 0.5000, 0.5921, 0.5565, 0.5058, 0.5406, 0.6406,\n",
      "           0.5437, 0.5000, 0.5000],\n",
      "          [0.5246, 0.6619, 0.5000, 0.6013, 0.5000, 0.6107, 0.5211, 0.5400,\n",
      "           0.5807, 0.5000, 0.5000, 0.5000, 0.5000, 0.5911, 0.5060, 0.5511,\n",
      "           0.5000, 0.5000, 0.5000]]]], device='cuda:0',\n",
      "       grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.5000,  0.6038,  0.6476,  0.5000,  0.5000,  0.5000,  0.5000,\n",
      "            0.5000,  0.5000,  0.5365,  0.5194,  0.5000,  0.5000,  0.5000,\n",
      "            0.5000,  0.5137,  0.5000,  0.5902,  0.5970],\n",
      "          [ 1.6394,  1.5000,  1.7759,  1.6099,  1.5000,  1.5164,  1.5000,\n",
      "            1.5000,  1.5000,  1.5000,  1.5700,  1.5337,  1.5126,  1.5000,\n",
      "            1.5000,  1.5972,  1.5419,  1.7080,  1.6082],\n",
      "          [ 2.6102,  2.5000,  2.5000,  2.5000,  2.5000,  2.6135,  2.5125,\n",
      "            2.5855,  2.5961,  2.6285,  2.5099,  2.5000,  2.5000,  2.5000,\n",
      "            2.7026,  2.6365,  2.6989,  2.5000,  2.5000],\n",
      "          [ 3.6343,  3.5973,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,\n",
      "            3.5099,  3.5000,  3.5553,  3.5283,  3.6755,  3.6568,  3.6109,\n",
      "            3.5146,  3.5107,  3.7231,  3.5000,  3.5306],\n",
      "          [ 4.6820,  4.6091,  4.5534,  4.6082,  4.5130,  4.5000,  4.5021,\n",
      "            4.5818,  4.5000,  4.5000,  4.5000,  4.6432,  4.5876,  4.5293,\n",
      "            4.6433,  4.6484,  4.5000,  4.5796,  4.5867],\n",
      "          [ 5.6668,  5.5000,  5.6585,  5.6598,  5.5867,  5.5000,  5.5783,\n",
      "            5.5000,  5.5709,  5.6154,  5.5982,  5.5000,  5.5373,  5.5589,\n",
      "            5.6276,  5.5000,  5.5402,  5.7096,  5.5999],\n",
      "          [ 6.6063,  6.6788,  6.6072,  6.5061,  6.5462,  6.5153,  6.5000,\n",
      "            6.5121,  6.5200,  6.5330,  6.5000,  6.5322,  6.5000,  6.5433,\n",
      "            6.6051,  6.6600,  6.6211,  6.5757,  6.5280],\n",
      "          [ 7.6204,  7.7518,  7.6449,  7.6228,  7.6212,  7.6475,  7.5983,\n",
      "            7.5088,  7.5097,  7.5235,  7.5357,  7.5771,  7.6228,  7.5000,\n",
      "            7.6264,  7.5107,  7.5777,  7.6151,  7.5316],\n",
      "          [ 8.6400,  8.5319,  8.5628,  8.6712,  8.5000,  8.5000,  8.5594,\n",
      "            8.5640,  8.6171,  8.5291,  8.6990,  8.5591,  8.5832,  8.5000,\n",
      "            8.5206,  8.5920,  8.5232,  8.5344,  8.5378],\n",
      "          [ 9.5281,  9.5546,  9.6374,  9.6623,  9.5189,  9.5533,  9.5759,\n",
      "            9.6089,  9.6226,  9.5302,  9.5128,  9.6552,  9.5164,  9.6173,\n",
      "            9.5059,  9.5077,  9.5347,  9.5971,  9.5000],\n",
      "          [10.5672, 10.5525, 10.5983, 10.6049, 10.5000, 10.5531, 10.5204,\n",
      "           10.5639, 10.5629, 10.5031, 10.5000, 10.5349, 10.5746, 10.5000,\n",
      "           10.5000, 10.5738, 10.5731, 10.6051, 10.5246],\n",
      "          [11.5696, 11.5758, 11.6778, 11.5962, 11.5069, 11.6569, 11.5688,\n",
      "           11.5000, 11.5429, 11.5203, 11.5977, 11.5675, 11.5000, 11.5925,\n",
      "           11.5000, 11.5328, 11.5575, 11.5165, 11.5132],\n",
      "          [12.6810, 12.5443, 12.5000, 12.6174, 12.6757, 12.5826, 12.6927,\n",
      "           12.5333, 12.5423, 12.6460, 12.5000, 12.5379, 12.5522, 12.5265,\n",
      "           12.5324, 12.6493, 12.6653, 12.5286, 12.6129],\n",
      "          [13.5000, 13.5000, 13.5000, 13.5000, 13.5000, 13.5504, 13.5995,\n",
      "           13.5000, 13.5867, 13.5978, 13.5000, 13.5997, 13.5490, 13.5000,\n",
      "           13.5000, 13.5000, 13.5580, 13.5114, 13.5000],\n",
      "          [14.6094, 14.5861, 14.5000, 14.5376, 14.5144, 14.6233, 14.6211,\n",
      "           14.7442, 14.6428, 14.6281, 14.5952, 14.5455, 14.5648, 14.5614,\n",
      "           14.5000, 14.6159, 14.6407, 14.5000, 14.5894],\n",
      "          [15.5388, 15.5750, 15.5000, 15.5000, 15.5454, 15.6095, 15.5000,\n",
      "           15.5077, 15.7372, 15.5059, 15.5468, 15.5380, 15.6206, 15.5000,\n",
      "           15.5180, 15.5000, 15.5000, 15.5000, 15.5700],\n",
      "          [16.5948, 16.7215, 16.7138, 16.6007, 16.6296, 16.5000, 16.5847,\n",
      "           16.5881, 16.5381, 16.5000, 16.5268, 16.5000, 16.5000, 16.6216,\n",
      "           16.5000, 16.5000, 16.5719, 16.5975, 16.6329],\n",
      "          [17.6982, 17.5586, 17.5670, 17.5099, 17.5872, 17.5000, 17.5000,\n",
      "           17.5241, 17.5000, 17.5557, 17.5000, 17.5921, 17.5565, 17.5058,\n",
      "           17.5406, 17.6406, 17.5437, 17.5000, 17.5000],\n",
      "          [18.5246, 18.6619, 18.5000, 18.6013, 18.5000, 18.6107, 18.5211,\n",
      "           18.5400, 18.5807, 18.5000, 18.5000, 18.5000, 18.5000, 18.5911,\n",
      "           18.5060, 18.5511, 18.5000, 18.5000, 18.5000]]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([3, 1, 85, 19, 19])\n",
      "Loss (epoch: 1): 5931.2783\n",
      "[[ 12  16]\n",
      " [ 19  36]\n",
      " [ 40  28]\n",
      " [ 36  75]\n",
      " [ 76  55]\n",
      " [ 72 146]\n",
      " [142 110]\n",
      " [192 243]\n",
      " [459 401]]\n",
      "[12, None, None]\n",
      "[16, None, None]\n",
      "tensor([[[[0.6438, 0.5000, 0.5215,  ..., 0.5958, 0.5000, 0.6229],\n",
      "          [0.6290, 0.5600, 0.7607,  ..., 0.8440, 0.5493, 0.6940],\n",
      "          [0.5168, 0.5000, 0.6385,  ..., 0.8469, 0.5621, 0.7058],\n",
      "          ...,\n",
      "          [0.5772, 0.5458, 0.6407,  ..., 0.7101, 0.5000, 0.8170],\n",
      "          [0.6514, 0.7160, 0.7924,  ..., 0.7807, 0.5000, 0.8266],\n",
      "          [0.6901, 0.7910, 0.8334,  ..., 0.6975, 0.5000, 0.8125]]]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.6438,  0.5000,  0.5215,  ...,  0.5958,  0.5000,  0.6229],\n",
      "          [ 1.6290,  1.5600,  1.7607,  ...,  1.8440,  1.5493,  1.6940],\n",
      "          [ 2.5168,  2.5000,  2.6385,  ...,  2.8469,  2.5621,  2.7058],\n",
      "          ...,\n",
      "          [73.5772, 73.5458, 73.6407,  ..., 73.7101, 73.5000, 73.8170],\n",
      "          [74.6514, 74.7160, 74.7924,  ..., 74.7807, 74.5000, 74.8266],\n",
      "          [75.6901, 75.7910, 75.8334,  ..., 75.6975, 75.5000, 75.8125]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "[12, 19, None]\n",
      "[16, 36, None]\n",
      "tensor([[[[0.5945, 0.6355, 0.6205,  ..., 0.5675, 0.6156, 0.5939],\n",
      "          [0.7175, 0.6533, 0.5475,  ..., 0.6886, 0.6066, 0.5000],\n",
      "          [0.7162, 0.7313, 0.6413,  ..., 0.6243, 0.6330, 0.5000],\n",
      "          ...,\n",
      "          [0.7032, 0.6860, 0.5455,  ..., 0.5000, 0.5415, 0.6124],\n",
      "          [0.8176, 0.7402, 0.6982,  ..., 0.7113, 0.6374, 0.7645],\n",
      "          [0.7223, 0.6473, 0.6188,  ..., 0.7719, 0.7190, 0.7711]]]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.5945,  0.6355,  0.6205,  ...,  0.5675,  0.6156,  0.5939],\n",
      "          [ 1.7175,  1.6533,  1.5475,  ...,  1.6886,  1.6066,  1.5000],\n",
      "          [ 2.7162,  2.7313,  2.6413,  ...,  2.6243,  2.6330,  2.5000],\n",
      "          ...,\n",
      "          [73.7032, 73.6860, 73.5455,  ..., 73.5000, 73.5415, 73.6124],\n",
      "          [74.8176, 74.7402, 74.6982,  ..., 74.7113, 74.6374, 74.7645],\n",
      "          [75.7223, 75.6473, 75.6188,  ..., 75.7719, 75.7190, 75.7711]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "[12, 19, 40]\n",
      "[16, 36, 28]\n",
      "tensor([[[[0.5300, 0.5171, 0.5160,  ..., 0.5305, 0.5000, 0.6198],\n",
      "          [0.5156, 0.5719, 0.5596,  ..., 0.5353, 0.5000, 0.6600],\n",
      "          [0.5201, 0.6072, 0.5000,  ..., 0.5177, 0.5000, 0.6436],\n",
      "          ...,\n",
      "          [0.5497, 0.8363, 0.7066,  ..., 0.9034, 0.7222, 0.9162],\n",
      "          [0.5679, 0.7836, 0.5610,  ..., 0.7271, 0.5888, 0.8779],\n",
      "          [0.5000, 0.7049, 0.6825,  ..., 0.5801, 0.7315, 0.9118]]]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.5300,  0.5171,  0.5160,  ...,  0.5305,  0.5000,  0.6198],\n",
      "          [ 1.5156,  1.5719,  1.5596,  ...,  1.5353,  1.5000,  1.6600],\n",
      "          [ 2.5201,  2.6072,  2.5000,  ...,  2.5177,  2.5000,  2.6436],\n",
      "          ...,\n",
      "          [73.5497, 73.8363, 73.7066,  ..., 73.9034, 73.7222, 73.9162],\n",
      "          [74.5679, 74.7836, 74.5610,  ..., 74.7271, 74.5888, 74.8779],\n",
      "          [75.5000, 75.7049, 75.6825,  ..., 75.5801, 75.7315, 75.9118]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "[[ 12  16]\n",
      " [ 19  36]\n",
      " [ 40  28]\n",
      " [ 36  75]\n",
      " [ 76  55]\n",
      " [ 72 146]\n",
      " [142 110]\n",
      " [192 243]\n",
      " [459 401]]\n",
      "[36, None, None]\n",
      "[75, None, None]\n",
      "tensor([[[[0.5200, 0.5000, 0.5438,  ..., 0.6333, 0.5515, 0.5159],\n",
      "          [0.5112, 0.5000, 0.5000,  ..., 0.6254, 0.5961, 0.5000],\n",
      "          [0.5281, 0.5000, 0.5000,  ..., 0.6330, 0.5000, 0.5000],\n",
      "          ...,\n",
      "          [0.5374, 0.5291, 0.5000,  ..., 0.5766, 0.7509, 0.7311],\n",
      "          [0.5672, 0.5175, 0.5097,  ..., 0.5839, 0.6293, 0.7377],\n",
      "          [0.5000, 0.5000, 0.5103,  ..., 0.5244, 0.6008, 0.7583]]]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.5200,  0.5000,  0.5438,  ...,  0.6333,  0.5515,  0.5159],\n",
      "          [ 1.5112,  1.5000,  1.5000,  ...,  1.6254,  1.5961,  1.5000],\n",
      "          [ 2.5281,  2.5000,  2.5000,  ...,  2.6330,  2.5000,  2.5000],\n",
      "          ...,\n",
      "          [35.5374, 35.5291, 35.5000,  ..., 35.5766, 35.7509, 35.7311],\n",
      "          [36.5672, 36.5175, 36.5097,  ..., 36.5839, 36.6293, 36.7377],\n",
      "          [37.5000, 37.5000, 37.5103,  ..., 37.5244, 37.6008, 37.7583]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "[36, 76, None]\n",
      "[75, 55, None]\n",
      "tensor([[[[0.5000, 0.6133, 0.5675,  ..., 0.5256, 0.5270, 0.5523],\n",
      "          [0.5151, 0.5852, 0.5109,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          [0.5334, 0.6166, 0.5348,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          ...,\n",
      "          [0.5651, 0.5585, 0.5663,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          [0.5332, 0.5791, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          [0.5224, 0.5747, 0.5223,  ..., 0.6441, 0.6294, 0.5000]]]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.5000,  0.6133,  0.5675,  ...,  0.5256,  0.5270,  0.5523],\n",
      "          [ 1.5151,  1.5852,  1.5109,  ...,  1.5000,  1.5000,  1.5000],\n",
      "          [ 2.5334,  2.6166,  2.5348,  ...,  2.5000,  2.5000,  2.5000],\n",
      "          ...,\n",
      "          [35.5651, 35.5585, 35.5663,  ..., 35.5000, 35.5000, 35.5000],\n",
      "          [36.5332, 36.5791, 36.5000,  ..., 36.5000, 36.5000, 36.5000],\n",
      "          [37.5224, 37.5747, 37.5223,  ..., 37.6441, 37.6294, 37.5000]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "[36, 76, 72]\n",
      "[75, 55, 146]\n",
      "tensor([[[[0.5000, 0.5477, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "          [0.5092, 0.5367, 0.5637,  ..., 0.5000, 0.5939, 0.5291],\n",
      "          [0.5339, 0.5348, 0.5522,  ..., 0.5000, 0.5281, 0.5000],\n",
      "          ...,\n",
      "          [0.5000, 0.5000, 0.5268,  ..., 0.7393, 0.9885, 0.9830],\n",
      "          [0.5000, 0.5307, 0.5334,  ..., 0.7096, 0.9683, 0.9687],\n",
      "          [0.5151, 0.5000, 0.5293,  ..., 0.7021, 0.8585, 0.8168]]]],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.5000,  0.5477,  0.5000,  ...,  0.5000,  0.5000,  0.5000],\n",
      "          [ 1.5092,  1.5367,  1.5637,  ...,  1.5000,  1.5939,  1.5291],\n",
      "          [ 2.5339,  2.5348,  2.5522,  ...,  2.5000,  2.5281,  2.5000],\n",
      "          ...,\n",
      "          [35.5000, 35.5000, 35.5268,  ..., 35.7393, 35.9885, 35.9830],\n",
      "          [36.5000, 36.5307, 36.5334,  ..., 36.7096, 36.9683, 36.9687],\n",
      "          [37.5151, 37.5000, 37.5293,  ..., 37.7021, 37.8585, 37.8168]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "[[ 12  16]\n",
      " [ 19  36]\n",
      " [ 40  28]\n",
      " [ 36  75]\n",
      " [ 76  55]\n",
      " [ 72 146]\n",
      " [142 110]\n",
      " [192 243]\n",
      " [459 401]]\n",
      "[142, None, None]\n",
      "[110, None, None]\n",
      "tensor([[[[0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000]]]], device='cuda:0',\n",
      "       grad_fn=<SigmoidBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,\n",
      "            0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,\n",
      "            0.5000,  0.5000,  0.5000,  0.5000,  0.5000],\n",
      "          [ 1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,\n",
      "            1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,\n",
      "            1.5000,  1.5000,  1.5000,  1.5000,  1.5000],\n",
      "          [ 2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,\n",
      "            2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,\n",
      "            2.5000,  2.5000,  2.5000,  2.5000,  2.5000],\n",
      "          [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,\n",
      "            3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,\n",
      "            3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n",
      "          [ 4.5000,  4.5000,  4.5000,  4.5000,  4.5000,  4.5000,  4.5000,\n",
      "            4.5000,  4.5000,  4.5000,  4.5000,  4.5000,  4.5000,  4.5000,\n",
      "            4.5000,  4.5000,  4.5000,  4.5000,  4.5000],\n",
      "          [ 5.5000,  5.5000,  5.5000,  5.5000,  5.5000,  5.5000,  5.5000,\n",
      "            5.5000,  5.5000,  5.5000,  5.5000,  5.5000,  5.5000,  5.5000,\n",
      "            5.5000,  5.5000,  5.5000,  5.5000,  5.5000],\n",
      "          [ 6.5000,  6.5000,  6.5000,  6.5000,  6.5000,  6.5000,  6.5000,\n",
      "            6.5000,  6.5000,  6.5000,  6.5000,  6.5000,  6.5000,  6.5000,\n",
      "            6.5000,  6.5000,  6.5000,  6.5000,  6.5000],\n",
      "          [ 7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000,\n",
      "            7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000,\n",
      "            7.5000,  7.5000,  7.5000,  7.5000,  7.5000],\n",
      "          [ 8.5000,  8.5000,  8.5000,  8.5000,  8.5000,  8.5000,  8.5000,\n",
      "            8.5000,  8.5000,  8.5000,  8.5000,  8.5000,  8.5000,  8.5000,\n",
      "            8.5000,  8.5000,  8.5000,  8.5000,  8.5000],\n",
      "          [ 9.5000,  9.5000,  9.5000,  9.5000,  9.5000,  9.5000,  9.5000,\n",
      "            9.5000,  9.5000,  9.5000,  9.5000,  9.5000,  9.5000,  9.5000,\n",
      "            9.5000,  9.5000,  9.5000,  9.5000,  9.5000],\n",
      "          [10.5000, 10.5000, 10.5000, 10.5000, 10.5000, 10.5000, 10.5000,\n",
      "           10.5000, 10.5000, 10.5000, 10.5000, 10.5000, 10.5000, 10.5000,\n",
      "           10.5000, 10.5000, 10.5000, 10.5000, 10.5000],\n",
      "          [11.5000, 11.5000, 11.5000, 11.5000, 11.5000, 11.5000, 11.5000,\n",
      "           11.5000, 11.5000, 11.5000, 11.5000, 11.5000, 11.5000, 11.5000,\n",
      "           11.5000, 11.5000, 11.5000, 11.5000, 11.5000],\n",
      "          [12.5000, 12.5000, 12.5000, 12.5000, 12.5000, 12.5000, 12.5000,\n",
      "           12.5000, 12.5000, 12.5000, 12.5000, 12.5000, 12.5000, 12.5000,\n",
      "           12.5000, 12.5000, 12.5000, 12.5000, 12.5000],\n",
      "          [13.5000, 13.5000, 13.5000, 13.5000, 13.5000, 13.5000, 13.5000,\n",
      "           13.5000, 13.5000, 13.5000, 13.5000, 13.5000, 13.5000, 13.5000,\n",
      "           13.5000, 13.5000, 13.5000, 13.5000, 13.5000],\n",
      "          [14.5000, 14.5000, 14.5000, 14.5000, 14.5000, 14.5000, 14.5000,\n",
      "           14.5000, 14.5000, 14.5000, 14.5000, 14.5000, 14.5000, 14.5000,\n",
      "           14.5000, 14.5000, 14.5000, 14.5000, 14.5000],\n",
      "          [15.5000, 15.5000, 15.5000, 15.5000, 15.5000, 15.5000, 15.5000,\n",
      "           15.5000, 15.5000, 15.5000, 15.5000, 15.5000, 15.5000, 15.5000,\n",
      "           15.5000, 15.5000, 15.5000, 15.5000, 15.5000],\n",
      "          [16.5000, 16.5000, 16.5000, 16.5000, 16.5000, 16.5000, 16.5000,\n",
      "           16.5000, 16.5000, 16.5000, 16.5000, 16.5000, 16.5000, 16.5000,\n",
      "           16.5000, 16.5000, 16.5000, 16.5000, 16.5000],\n",
      "          [17.5000, 17.5000, 17.5000, 17.5000, 17.5000, 17.5000, 17.5000,\n",
      "           17.5000, 17.5000, 17.5000, 17.5000, 17.5000, 17.5000, 17.5000,\n",
      "           17.5000, 17.5000, 17.5000, 17.5000, 17.5000],\n",
      "          [18.5000, 18.5000, 18.5000, 18.5000, 18.5000, 18.5000, 18.5000,\n",
      "           18.5000, 18.5000, 18.5000, 18.5000, 18.5000, 18.5000, 18.5000,\n",
      "           18.5000, 18.5000, 18.5000, 18.5000, 18.5000]]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "[142, 192, None]\n",
      "[110, 243, None]\n",
      "tensor([[[[0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000]]]], device='cuda:0',\n",
      "       grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,\n",
      "            0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,\n",
      "            0.5000,  0.5000,  0.5000,  0.5000,  0.5000],\n",
      "          [ 1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,\n",
      "            1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,\n",
      "            1.5000,  1.5000,  1.5000,  1.5000,  1.5000],\n",
      "          [ 2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,\n",
      "            2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,\n",
      "            2.5000,  2.5000,  2.5000,  2.5000,  2.5000],\n",
      "          [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,\n",
      "            3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,\n",
      "            3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n",
      "          [ 4.5000,  4.5000,  4.5000,  4.5000,  4.5000,  4.5000,  4.5000,\n",
      "            4.5000,  4.5000,  4.5000,  4.5000,  4.5000,  4.5000,  4.5000,\n",
      "            4.5000,  4.5000,  4.5000,  4.5000,  4.5000],\n",
      "          [ 5.5000,  5.5000,  5.5000,  5.5000,  5.5000,  5.5000,  5.5000,\n",
      "            5.5000,  5.5000,  5.5000,  5.5000,  5.5000,  5.5000,  5.5000,\n",
      "            5.5000,  5.5000,  5.5000,  5.5000,  5.5000],\n",
      "          [ 6.5000,  6.5000,  6.5000,  6.5000,  6.5000,  6.5000,  6.5000,\n",
      "            6.5000,  6.5000,  6.5000,  6.5000,  6.5000,  6.5000,  6.5000,\n",
      "            6.5000,  6.5000,  6.5000,  6.5000,  6.5000],\n",
      "          [ 7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000,\n",
      "            7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000,\n",
      "            7.5000,  7.5000,  7.5000,  7.5000,  7.5000],\n",
      "          [ 8.5000,  8.5000,  8.5000,  8.5000,  8.5000,  8.5000,  8.5000,\n",
      "            8.5000,  8.5000,  8.5000,  8.5000,  8.5000,  8.5000,  8.5000,\n",
      "            8.5000,  8.5000,  8.5000,  8.5000,  8.5000],\n",
      "          [ 9.5000,  9.5000,  9.5000,  9.5000,  9.5000,  9.5000,  9.5000,\n",
      "            9.5000,  9.5000,  9.5000,  9.5000,  9.5000,  9.5000,  9.5000,\n",
      "            9.5000,  9.5000,  9.5000,  9.5000,  9.5000],\n",
      "          [10.5000, 10.5000, 10.5000, 10.5000, 10.5000, 10.5000, 10.5000,\n",
      "           10.5000, 10.5000, 10.5000, 10.5000, 10.5000, 10.5000, 10.5000,\n",
      "           10.5000, 10.5000, 10.5000, 10.5000, 10.5000],\n",
      "          [11.5000, 11.5000, 11.5000, 11.5000, 11.5000, 11.5000, 11.5000,\n",
      "           11.5000, 11.5000, 11.5000, 11.5000, 11.5000, 11.5000, 11.5000,\n",
      "           11.5000, 11.5000, 11.5000, 11.5000, 11.5000],\n",
      "          [12.5000, 12.5000, 12.5000, 12.5000, 12.5000, 12.5000, 12.5000,\n",
      "           12.5000, 12.5000, 12.5000, 12.5000, 12.5000, 12.5000, 12.5000,\n",
      "           12.5000, 12.5000, 12.5000, 12.5000, 12.5000],\n",
      "          [13.5000, 13.5000, 13.5000, 13.5000, 13.5000, 13.5000, 13.5000,\n",
      "           13.5000, 13.5000, 13.5000, 13.5000, 13.5000, 13.5000, 13.5000,\n",
      "           13.5000, 13.5000, 13.5000, 13.5000, 13.5000],\n",
      "          [14.5000, 14.5000, 14.5000, 14.5000, 14.5000, 14.5000, 14.5000,\n",
      "           14.5000, 14.5000, 14.5000, 14.5000, 14.5000, 14.5000, 14.5000,\n",
      "           14.5000, 14.5000, 14.5000, 14.5000, 14.5000],\n",
      "          [15.5000, 15.5000, 15.5000, 15.5000, 15.5000, 15.5000, 15.5000,\n",
      "           15.5000, 15.5000, 15.5000, 15.5000, 15.5000, 15.5000, 15.5000,\n",
      "           15.5000, 15.5000, 15.5000, 15.5000, 15.5000],\n",
      "          [16.5000, 16.5000, 16.5000, 16.5000, 16.5000, 16.5000, 16.5000,\n",
      "           16.5000, 16.5000, 16.5000, 16.5000, 16.5000, 16.5000, 16.5000,\n",
      "           16.5000, 16.5000, 16.5000, 16.5000, 16.5000],\n",
      "          [17.5000, 17.5000, 17.5000, 17.5000, 17.5000, 17.5000, 17.5000,\n",
      "           17.5000, 17.5000, 17.5000, 17.5000, 17.5000, 17.5000, 17.5000,\n",
      "           17.5000, 17.5000, 17.5000, 17.5000, 17.5000],\n",
      "          [18.5000, 18.5000, 18.5000, 18.5000, 18.5000, 18.5000, 18.5000,\n",
      "           18.5000, 18.5000, 18.5000, 18.5000, 18.5000, 18.5000, 18.5000,\n",
      "           18.5000, 18.5000, 18.5000, 18.5000, 18.5000]]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "[142, 192, 459]\n",
      "[110, 243, 401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000],\n",
      "          [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
      "           0.5000, 0.5000, 0.5000]]]], device='cuda:0',\n",
      "       grad_fn=<SigmoidBackward>)\n",
      "tensor([[[[ 0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,\n",
      "            0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,\n",
      "            0.5000,  0.5000,  0.5000,  0.5000,  0.5000],\n",
      "          [ 1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,\n",
      "            1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,  1.5000,\n",
      "            1.5000,  1.5000,  1.5000,  1.5000,  1.5000],\n",
      "          [ 2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,\n",
      "            2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,  2.5000,\n",
      "            2.5000,  2.5000,  2.5000,  2.5000,  2.5000],\n",
      "          [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,\n",
      "            3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000,\n",
      "            3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n",
      "          [ 4.5000,  4.5000,  4.5000,  4.5000,  4.5000,  4.5000,  4.5000,\n",
      "            4.5000,  4.5000,  4.5000,  4.5000,  4.5000,  4.5000,  4.5000,\n",
      "            4.5000,  4.5000,  4.5000,  4.5000,  4.5000],\n",
      "          [ 5.5000,  5.5000,  5.5000,  5.5000,  5.5000,  5.5000,  5.5000,\n",
      "            5.5000,  5.5000,  5.5000,  5.5000,  5.5000,  5.5000,  5.5000,\n",
      "            5.5000,  5.5000,  5.5000,  5.5000,  5.5000],\n",
      "          [ 6.5000,  6.5000,  6.5000,  6.5000,  6.5000,  6.5000,  6.5000,\n",
      "            6.5000,  6.5000,  6.5000,  6.5000,  6.5000,  6.5000,  6.5000,\n",
      "            6.5000,  6.5000,  6.5000,  6.5000,  6.5000],\n",
      "          [ 7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000,\n",
      "            7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000,  7.5000,\n",
      "            7.5000,  7.5000,  7.5000,  7.5000,  7.5000],\n",
      "          [ 8.5000,  8.5000,  8.5000,  8.5000,  8.5000,  8.5000,  8.5000,\n",
      "            8.5000,  8.5000,  8.5000,  8.5000,  8.5000,  8.5000,  8.5000,\n",
      "            8.5000,  8.5000,  8.5000,  8.5000,  8.5000],\n",
      "          [ 9.5000,  9.5000,  9.5000,  9.5000,  9.5000,  9.5000,  9.5000,\n",
      "            9.5000,  9.5000,  9.5000,  9.5000,  9.5000,  9.5000,  9.5000,\n",
      "            9.5000,  9.5000,  9.5000,  9.5000,  9.5000],\n",
      "          [10.5000, 10.5000, 10.5000, 10.5000, 10.5000, 10.5000, 10.5000,\n",
      "           10.5000, 10.5000, 10.5000, 10.5000, 10.5000, 10.5000, 10.5000,\n",
      "           10.5000, 10.5000, 10.5000, 10.5000, 10.5000],\n",
      "          [11.5000, 11.5000, 11.5000, 11.5000, 11.5000, 11.5000, 11.5000,\n",
      "           11.5000, 11.5000, 11.5000, 11.5000, 11.5000, 11.5000, 11.5000,\n",
      "           11.5000, 11.5000, 11.5000, 11.5000, 11.5000],\n",
      "          [12.5000, 12.5000, 12.5000, 12.5000, 12.5000, 12.5000, 12.5000,\n",
      "           12.5000, 12.5000, 12.5000, 12.5000, 12.5000, 12.5000, 12.5000,\n",
      "           12.5000, 12.5000, 12.5000, 12.5000, 12.5000],\n",
      "          [13.5000, 13.5000, 13.5000, 13.5000, 13.5000, 13.5000, 13.5000,\n",
      "           13.5000, 13.5000, 13.5000, 13.5000, 13.5000, 13.5000, 13.5000,\n",
      "           13.5000, 13.5000, 13.5000, 13.5000, 13.5000],\n",
      "          [14.5000, 14.5000, 14.5000, 14.5000, 14.5000, 14.5000, 14.5000,\n",
      "           14.5000, 14.5000, 14.5000, 14.5000, 14.5000, 14.5000, 14.5000,\n",
      "           14.5000, 14.5000, 14.5000, 14.5000, 14.5000],\n",
      "          [15.5000, 15.5000, 15.5000, 15.5000, 15.5000, 15.5000, 15.5000,\n",
      "           15.5000, 15.5000, 15.5000, 15.5000, 15.5000, 15.5000, 15.5000,\n",
      "           15.5000, 15.5000, 15.5000, 15.5000, 15.5000],\n",
      "          [16.5000, 16.5000, 16.5000, 16.5000, 16.5000, 16.5000, 16.5000,\n",
      "           16.5000, 16.5000, 16.5000, 16.5000, 16.5000, 16.5000, 16.5000,\n",
      "           16.5000, 16.5000, 16.5000, 16.5000, 16.5000],\n",
      "          [17.5000, 17.5000, 17.5000, 17.5000, 17.5000, 17.5000, 17.5000,\n",
      "           17.5000, 17.5000, 17.5000, 17.5000, 17.5000, 17.5000, 17.5000,\n",
      "           17.5000, 17.5000, 17.5000, 17.5000, 17.5000],\n",
      "          [18.5000, 18.5000, 18.5000, 18.5000, 18.5000, 18.5000, 18.5000,\n",
      "           18.5000, 18.5000, 18.5000, 18.5000, 18.5000, 18.5000, 18.5000,\n",
      "           18.5000, 18.5000, 18.5000, 18.5000, 18.5000]]]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([3, 1, 85, 19, 19])\n",
      "Loss (epoch: 2): 4857.1074\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input = np.array([1 for i in range(608 * 608 * 3 * 1)]).reshape(1, 3, 608, 608)\n",
    "#target = np.array([0 for i in range(7 * 7 * 30)])\n",
    "target = np.array([0 for i in range(3 * 19 * 19 * 85)])\n",
    "\n",
    "input_tensor = torch.Tensor(input)\n",
    "output_tensor = torch.Tensor(target)\n",
    "\n",
    "#x = np.array([1 for i in range(608 * 608 * 3)]).reshape(1, 3, 608, 608)\n",
    "#x = torch.tensor(x)\n",
    "\n",
    "learning_rate = 0.08\n",
    "epoch_size = 2\n",
    "steps_for_printing_out_loss = 1\n",
    "\n",
    "YOLO_v4_Module_WIP = YOLO_v4_model(layer_details, layer_type)\n",
    "YOLO_v4_Module_WIP.cuda()\n",
    "\n",
    "\n",
    "\n",
    "#YOLO_v4_Module_WIP.load_state_dict(torch.load(\"D:/Installation/yolov4.pt\"))\n",
    "\n",
    "#YOLO_v4_Module_WIP.load_weights(\"D:/Installation/yolov4.weights\")\n",
    "#Model_WIP.to(device)\n",
    "loss_functioin = nn.MSELoss()\n",
    "optimizer = optim.SGD(YOLO_v4_Module_WIP.parameters(), lr = learning_rate)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for name, param in YOLO_v4_Module_WIP.named_parameters():\n",
    "    print('name: ', name)\n",
    "    print(type(param))\n",
    "    print('param.shape: ', param.shape)\n",
    "    print('param.requires_grad: ', param.requires_grad)\n",
    "    print('=====')\n",
    "#transfer learning:\n",
    "\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if name in ['fc.weight', 'fc.bias']:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\"\"\"\n",
    "\n",
    "input = input_tensor.cuda()\n",
    "target = output_tensor.cuda()\n",
    "\n",
    "def training_model():\n",
    "    for i in range(1, epoch_size + 1):\n",
    "        optimizer.zero_grad()\n",
    "        output = YOLO_v4_Module_WIP(input.cuda())\n",
    "        print(output.size())\n",
    "        #b_x, b_y, b_w, b_h, objective_p, class_p = YOLO_v4_Module_WIP(input.cuda())\n",
    "        #output = b_x\n",
    "        #loss = loss_functioin(output, target.reshape(output.size(0), output.size(1), output.size(2), output.size(3)))\n",
    "        \n",
    "        loss = loss_functioin(output, target.reshape(output.size(0), output.size(1), output.size(2), output.size(3), output.size(4)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % (steps_for_printing_out_loss) == 0:\n",
    "            print('Loss (epoch: ' + str(i) + '): ' + str(loss.cpu().detach().numpy()))\n",
    "    #torch.save({'state_dict': YOLO_v4_Module_WIP.state_dict(),'optimizer': optimizer.state_dict()}, model_file_path)\n",
    "\n",
    "\n",
    "training_model()\n",
    "\n",
    "#YOLO_v4_Module_WIP.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#all_layerrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'size': '1',\n",
       " 'stride': '1',\n",
       " 'pad': '1',\n",
       " 'filters': '255',\n",
       " 'activation': 'linear'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_details[160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5,\n",
       " 2.0,\n",
       " 2.375,\n",
       " 4.5,\n",
       " 5.0,\n",
       " 3.5,\n",
       " 2.25,\n",
       " 4.6875,\n",
       " 4.75,\n",
       " 3.4375,\n",
       " 4.5,\n",
       " 9.125,\n",
       " 4.4375,\n",
       " 3.4375,\n",
       " 6.0,\n",
       " 7.59375,\n",
       " 14.34375,\n",
       " 12.53125]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all_layers\n",
    "n = [76, 38, 19]\n",
    "m = [608 / i for i in n for m in range(6)] \n",
    "print(m)\n",
    "\n",
    "\n",
    "anchor = [12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401]\n",
    "[anchor[i] / m[i] for i in range(len(m))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-16-13f47200f125>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-16-13f47200f125>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    self.YOLO_v4_layers.append(Conv_Layer_box(in_channel[i], out_channel[i], kernel_size= kernel_size[i], stride = stride[i], activation_func = activation_func[i], batch_normalization = batch_normalization[i]))\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "class YOLO_v4_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.YOLO_v4_layers = nn.ModuleList()\n",
    "        \n",
    "            self.YOLO_v4_layers.append(Conv_Layer_box(in_channel[i], out_channel[i], kernel_size= kernel_size[i], stride = stride[i], activation_func = activation_func[i], batch_normalization = batch_normalization[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class abc():\n",
    "    def __init__(self, qwe, out):\n",
    "        print(qwe)\n",
    "        \n",
    "abc(4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = \"1,2,3\"\n",
    "m = abc.split(\",\")\n",
    "m\n",
    "abc = \"1\"\n",
    "m = abc.split(\",\")\n",
    "m\n",
    "k = 4\n",
    "\n",
    "HX = []\n",
    "\n",
    "for r in k:\n",
    "    print(r)\n",
    "    print(k[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightfile = \"D:/Installation/yolov4.weights\"\n",
    "fp = open(weightfile, 'rb')\n",
    "header = np.fromfile(fp, count=5, dtype=np.int32)\n",
    "header = torch.from_numpy(header)\n",
    "seen = header[3]\n",
    "buf = np.fromfile(fp, dtype=np.float32)\n",
    "fp.close()\n",
    "\n",
    "start = 0\n",
    "ind = -2\n",
    "buf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_layers.0.conv_box.0.weight\n",
      "torch.Size([32, 3, 3, 3])\n",
      "all_layers.0.conv_box.1.weight\n",
      "torch.Size([32])\n",
      "all_layers.0.conv_box.1.bias\n",
      "torch.Size([32])\n",
      "all_layers.0.conv_box.1.running_mean\n",
      "torch.Size([32])\n",
      "all_layers.0.conv_box.1.running_var\n",
      "torch.Size([32])\n",
      "all_layers.0.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.1.conv_box.0.weight\n",
      "torch.Size([64, 32, 3, 3])\n",
      "all_layers.1.conv_box.1.weight\n",
      "torch.Size([64])\n",
      "all_layers.1.conv_box.1.bias\n",
      "torch.Size([64])\n",
      "all_layers.1.conv_box.1.running_mean\n",
      "torch.Size([64])\n",
      "all_layers.1.conv_box.1.running_var\n",
      "torch.Size([64])\n",
      "all_layers.1.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.2.conv_box.0.weight\n",
      "torch.Size([64, 64, 1, 1])\n",
      "all_layers.2.conv_box.1.weight\n",
      "torch.Size([64])\n",
      "all_layers.2.conv_box.1.bias\n",
      "torch.Size([64])\n",
      "all_layers.2.conv_box.1.running_mean\n",
      "torch.Size([64])\n",
      "all_layers.2.conv_box.1.running_var\n",
      "torch.Size([64])\n",
      "all_layers.2.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.4.conv_box.0.weight\n",
      "torch.Size([64, 64, 1, 1])\n",
      "all_layers.4.conv_box.1.weight\n",
      "torch.Size([64])\n",
      "all_layers.4.conv_box.1.bias\n",
      "torch.Size([64])\n",
      "all_layers.4.conv_box.1.running_mean\n",
      "torch.Size([64])\n",
      "all_layers.4.conv_box.1.running_var\n",
      "torch.Size([64])\n",
      "all_layers.4.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.5.conv_box.0.weight\n",
      "torch.Size([32, 64, 1, 1])\n",
      "all_layers.5.conv_box.1.weight\n",
      "torch.Size([32])\n",
      "all_layers.5.conv_box.1.bias\n",
      "torch.Size([32])\n",
      "all_layers.5.conv_box.1.running_mean\n",
      "torch.Size([32])\n",
      "all_layers.5.conv_box.1.running_var\n",
      "torch.Size([32])\n",
      "all_layers.5.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.6.conv_box.0.weight\n",
      "torch.Size([64, 32, 3, 3])\n",
      "all_layers.6.conv_box.1.weight\n",
      "torch.Size([64])\n",
      "all_layers.6.conv_box.1.bias\n",
      "torch.Size([64])\n",
      "all_layers.6.conv_box.1.running_mean\n",
      "torch.Size([64])\n",
      "all_layers.6.conv_box.1.running_var\n",
      "torch.Size([64])\n",
      "all_layers.6.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.8.conv_box.0.weight\n",
      "torch.Size([64, 64, 1, 1])\n",
      "all_layers.8.conv_box.1.weight\n",
      "torch.Size([64])\n",
      "all_layers.8.conv_box.1.bias\n",
      "torch.Size([64])\n",
      "all_layers.8.conv_box.1.running_mean\n",
      "torch.Size([64])\n",
      "all_layers.8.conv_box.1.running_var\n",
      "torch.Size([64])\n",
      "all_layers.8.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.10.conv_box.0.weight\n",
      "torch.Size([64, 64, 1, 1])\n",
      "all_layers.10.conv_box.1.weight\n",
      "torch.Size([64])\n",
      "all_layers.10.conv_box.1.bias\n",
      "torch.Size([64])\n",
      "all_layers.10.conv_box.1.running_mean\n",
      "torch.Size([64])\n",
      "all_layers.10.conv_box.1.running_var\n",
      "torch.Size([64])\n",
      "all_layers.10.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.11.conv_box.0.weight\n",
      "torch.Size([128, 64, 3, 3])\n",
      "all_layers.11.conv_box.1.weight\n",
      "torch.Size([128])\n",
      "all_layers.11.conv_box.1.bias\n",
      "torch.Size([128])\n",
      "all_layers.11.conv_box.1.running_mean\n",
      "torch.Size([128])\n",
      "all_layers.11.conv_box.1.running_var\n",
      "torch.Size([128])\n",
      "all_layers.11.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.12.conv_box.0.weight\n",
      "torch.Size([64, 128, 1, 1])\n",
      "all_layers.12.conv_box.1.weight\n",
      "torch.Size([64])\n",
      "all_layers.12.conv_box.1.bias\n",
      "torch.Size([64])\n",
      "all_layers.12.conv_box.1.running_mean\n",
      "torch.Size([64])\n",
      "all_layers.12.conv_box.1.running_var\n",
      "torch.Size([64])\n",
      "all_layers.12.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.14.conv_box.0.weight\n",
      "torch.Size([64, 128, 1, 1])\n",
      "all_layers.14.conv_box.1.weight\n",
      "torch.Size([64])\n",
      "all_layers.14.conv_box.1.bias\n",
      "torch.Size([64])\n",
      "all_layers.14.conv_box.1.running_mean\n",
      "torch.Size([64])\n",
      "all_layers.14.conv_box.1.running_var\n",
      "torch.Size([64])\n",
      "all_layers.14.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.15.conv_box.0.weight\n",
      "torch.Size([64, 64, 1, 1])\n",
      "all_layers.15.conv_box.1.weight\n",
      "torch.Size([64])\n",
      "all_layers.15.conv_box.1.bias\n",
      "torch.Size([64])\n",
      "all_layers.15.conv_box.1.running_mean\n",
      "torch.Size([64])\n",
      "all_layers.15.conv_box.1.running_var\n",
      "torch.Size([64])\n",
      "all_layers.15.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.16.conv_box.0.weight\n",
      "torch.Size([64, 64, 3, 3])\n",
      "all_layers.16.conv_box.1.weight\n",
      "torch.Size([64])\n",
      "all_layers.16.conv_box.1.bias\n",
      "torch.Size([64])\n",
      "all_layers.16.conv_box.1.running_mean\n",
      "torch.Size([64])\n",
      "all_layers.16.conv_box.1.running_var\n",
      "torch.Size([64])\n",
      "all_layers.16.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.18.conv_box.0.weight\n",
      "torch.Size([64, 64, 1, 1])\n",
      "all_layers.18.conv_box.1.weight\n",
      "torch.Size([64])\n",
      "all_layers.18.conv_box.1.bias\n",
      "torch.Size([64])\n",
      "all_layers.18.conv_box.1.running_mean\n",
      "torch.Size([64])\n",
      "all_layers.18.conv_box.1.running_var\n",
      "torch.Size([64])\n",
      "all_layers.18.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.19.conv_box.0.weight\n",
      "torch.Size([64, 64, 3, 3])\n",
      "all_layers.19.conv_box.1.weight\n",
      "torch.Size([64])\n",
      "all_layers.19.conv_box.1.bias\n",
      "torch.Size([64])\n",
      "all_layers.19.conv_box.1.running_mean\n",
      "torch.Size([64])\n",
      "all_layers.19.conv_box.1.running_var\n",
      "torch.Size([64])\n",
      "all_layers.19.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.21.conv_box.0.weight\n",
      "torch.Size([64, 64, 1, 1])\n",
      "all_layers.21.conv_box.1.weight\n",
      "torch.Size([64])\n",
      "all_layers.21.conv_box.1.bias\n",
      "torch.Size([64])\n",
      "all_layers.21.conv_box.1.running_mean\n",
      "torch.Size([64])\n",
      "all_layers.21.conv_box.1.running_var\n",
      "torch.Size([64])\n",
      "all_layers.21.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.23.conv_box.0.weight\n",
      "torch.Size([128, 64, 1, 1])\n",
      "all_layers.23.conv_box.1.weight\n",
      "torch.Size([128])\n",
      "all_layers.23.conv_box.1.bias\n",
      "torch.Size([128])\n",
      "all_layers.23.conv_box.1.running_mean\n",
      "torch.Size([128])\n",
      "all_layers.23.conv_box.1.running_var\n",
      "torch.Size([128])\n",
      "all_layers.23.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.24.conv_box.0.weight\n",
      "torch.Size([256, 128, 3, 3])\n",
      "all_layers.24.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.24.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.24.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.24.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.24.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.25.conv_box.0.weight\n",
      "torch.Size([128, 256, 1, 1])\n",
      "all_layers.25.conv_box.1.weight\n",
      "torch.Size([128])\n",
      "all_layers.25.conv_box.1.bias\n",
      "torch.Size([128])\n",
      "all_layers.25.conv_box.1.running_mean\n",
      "torch.Size([128])\n",
      "all_layers.25.conv_box.1.running_var\n",
      "torch.Size([128])\n",
      "all_layers.25.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.27.conv_box.0.weight\n",
      "torch.Size([128, 256, 1, 1])\n",
      "all_layers.27.conv_box.1.weight\n",
      "torch.Size([128])\n",
      "all_layers.27.conv_box.1.bias\n",
      "torch.Size([128])\n",
      "all_layers.27.conv_box.1.running_mean\n",
      "torch.Size([128])\n",
      "all_layers.27.conv_box.1.running_var\n",
      "torch.Size([128])\n",
      "all_layers.27.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.28.conv_box.0.weight\n",
      "torch.Size([128, 128, 1, 1])\n",
      "all_layers.28.conv_box.1.weight\n",
      "torch.Size([128])\n",
      "all_layers.28.conv_box.1.bias\n",
      "torch.Size([128])\n",
      "all_layers.28.conv_box.1.running_mean\n",
      "torch.Size([128])\n",
      "all_layers.28.conv_box.1.running_var\n",
      "torch.Size([128])\n",
      "all_layers.28.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.29.conv_box.0.weight\n",
      "torch.Size([128, 128, 3, 3])\n",
      "all_layers.29.conv_box.1.weight\n",
      "torch.Size([128])\n",
      "all_layers.29.conv_box.1.bias\n",
      "torch.Size([128])\n",
      "all_layers.29.conv_box.1.running_mean\n",
      "torch.Size([128])\n",
      "all_layers.29.conv_box.1.running_var\n",
      "torch.Size([128])\n",
      "all_layers.29.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.31.conv_box.0.weight\n",
      "torch.Size([128, 128, 1, 1])\n",
      "all_layers.31.conv_box.1.weight\n",
      "torch.Size([128])\n",
      "all_layers.31.conv_box.1.bias\n",
      "torch.Size([128])\n",
      "all_layers.31.conv_box.1.running_mean\n",
      "torch.Size([128])\n",
      "all_layers.31.conv_box.1.running_var\n",
      "torch.Size([128])\n",
      "all_layers.31.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.32.conv_box.0.weight\n",
      "torch.Size([128, 128, 3, 3])\n",
      "all_layers.32.conv_box.1.weight\n",
      "torch.Size([128])\n",
      "all_layers.32.conv_box.1.bias\n",
      "torch.Size([128])\n",
      "all_layers.32.conv_box.1.running_mean\n",
      "torch.Size([128])\n",
      "all_layers.32.conv_box.1.running_var\n",
      "torch.Size([128])\n",
      "all_layers.32.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.34.conv_box.0.weight\n",
      "torch.Size([128, 128, 1, 1])\n",
      "all_layers.34.conv_box.1.weight\n",
      "torch.Size([128])\n",
      "all_layers.34.conv_box.1.bias\n",
      "torch.Size([128])\n",
      "all_layers.34.conv_box.1.running_mean\n",
      "torch.Size([128])\n",
      "all_layers.34.conv_box.1.running_var\n",
      "torch.Size([128])\n",
      "all_layers.34.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.35.conv_box.0.weight\n",
      "torch.Size([128, 128, 3, 3])\n",
      "all_layers.35.conv_box.1.weight\n",
      "torch.Size([128])\n",
      "all_layers.35.conv_box.1.bias\n",
      "torch.Size([128])\n",
      "all_layers.35.conv_box.1.running_mean\n",
      "torch.Size([128])\n",
      "all_layers.35.conv_box.1.running_var\n",
      "torch.Size([128])\n",
      "all_layers.35.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.37.conv_box.0.weight\n",
      "torch.Size([128, 128, 1, 1])\n",
      "all_layers.37.conv_box.1.weight\n",
      "torch.Size([128])\n",
      "all_layers.37.conv_box.1.bias\n",
      "torch.Size([128])\n",
      "all_layers.37.conv_box.1.running_mean\n",
      "torch.Size([128])\n",
      "all_layers.37.conv_box.1.running_var\n",
      "torch.Size([128])\n",
      "all_layers.37.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.38.conv_box.0.weight\n",
      "torch.Size([128, 128, 3, 3])\n",
      "all_layers.38.conv_box.1.weight\n",
      "torch.Size([128])\n",
      "all_layers.38.conv_box.1.bias\n",
      "torch.Size([128])\n",
      "all_layers.38.conv_box.1.running_mean\n",
      "torch.Size([128])\n",
      "all_layers.38.conv_box.1.running_var\n",
      "torch.Size([128])\n",
      "all_layers.38.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.40.conv_box.0.weight\n",
      "torch.Size([128, 128, 1, 1])\n",
      "all_layers.40.conv_box.1.weight\n",
      "torch.Size([128])\n",
      "all_layers.40.conv_box.1.bias\n",
      "torch.Size([128])\n",
      "all_layers.40.conv_box.1.running_mean\n",
      "torch.Size([128])\n",
      "all_layers.40.conv_box.1.running_var\n",
      "torch.Size([128])\n",
      "all_layers.40.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.41.conv_box.0.weight\n",
      "torch.Size([128, 128, 3, 3])\n",
      "all_layers.41.conv_box.1.weight\n",
      "torch.Size([128])\n",
      "all_layers.41.conv_box.1.bias\n",
      "torch.Size([128])\n",
      "all_layers.41.conv_box.1.running_mean\n",
      "torch.Size([128])\n",
      "all_layers.41.conv_box.1.running_var\n",
      "torch.Size([128])\n",
      "all_layers.41.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.43.conv_box.0.weight\n",
      "torch.Size([128, 128, 1, 1])\n",
      "all_layers.43.conv_box.1.weight\n",
      "torch.Size([128])\n",
      "all_layers.43.conv_box.1.bias\n",
      "torch.Size([128])\n",
      "all_layers.43.conv_box.1.running_mean\n",
      "torch.Size([128])\n",
      "all_layers.43.conv_box.1.running_var\n",
      "torch.Size([128])\n",
      "all_layers.43.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.44.conv_box.0.weight\n",
      "torch.Size([128, 128, 3, 3])\n",
      "all_layers.44.conv_box.1.weight\n",
      "torch.Size([128])\n",
      "all_layers.44.conv_box.1.bias\n",
      "torch.Size([128])\n",
      "all_layers.44.conv_box.1.running_mean\n",
      "torch.Size([128])\n",
      "all_layers.44.conv_box.1.running_var\n",
      "torch.Size([128])\n",
      "all_layers.44.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.46.conv_box.0.weight\n",
      "torch.Size([128, 128, 1, 1])\n",
      "all_layers.46.conv_box.1.weight\n",
      "torch.Size([128])\n",
      "all_layers.46.conv_box.1.bias\n",
      "torch.Size([128])\n",
      "all_layers.46.conv_box.1.running_mean\n",
      "torch.Size([128])\n",
      "all_layers.46.conv_box.1.running_var\n",
      "torch.Size([128])\n",
      "all_layers.46.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.47.conv_box.0.weight\n",
      "torch.Size([128, 128, 3, 3])\n",
      "all_layers.47.conv_box.1.weight\n",
      "torch.Size([128])\n",
      "all_layers.47.conv_box.1.bias\n",
      "torch.Size([128])\n",
      "all_layers.47.conv_box.1.running_mean\n",
      "torch.Size([128])\n",
      "all_layers.47.conv_box.1.running_var\n",
      "torch.Size([128])\n",
      "all_layers.47.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.49.conv_box.0.weight\n",
      "torch.Size([128, 128, 1, 1])\n",
      "all_layers.49.conv_box.1.weight\n",
      "torch.Size([128])\n",
      "all_layers.49.conv_box.1.bias\n",
      "torch.Size([128])\n",
      "all_layers.49.conv_box.1.running_mean\n",
      "torch.Size([128])\n",
      "all_layers.49.conv_box.1.running_var\n",
      "torch.Size([128])\n",
      "all_layers.49.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.50.conv_box.0.weight\n",
      "torch.Size([128, 128, 3, 3])\n",
      "all_layers.50.conv_box.1.weight\n",
      "torch.Size([128])\n",
      "all_layers.50.conv_box.1.bias\n",
      "torch.Size([128])\n",
      "all_layers.50.conv_box.1.running_mean\n",
      "torch.Size([128])\n",
      "all_layers.50.conv_box.1.running_var\n",
      "torch.Size([128])\n",
      "all_layers.50.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.52.conv_box.0.weight\n",
      "torch.Size([128, 128, 1, 1])\n",
      "all_layers.52.conv_box.1.weight\n",
      "torch.Size([128])\n",
      "all_layers.52.conv_box.1.bias\n",
      "torch.Size([128])\n",
      "all_layers.52.conv_box.1.running_mean\n",
      "torch.Size([128])\n",
      "all_layers.52.conv_box.1.running_var\n",
      "torch.Size([128])\n",
      "all_layers.52.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.54.conv_box.0.weight\n",
      "torch.Size([256, 128, 1, 1])\n",
      "all_layers.54.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.54.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.54.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.54.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.54.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.55.conv_box.0.weight\n",
      "torch.Size([512, 256, 3, 3])\n",
      "all_layers.55.conv_box.1.weight\n",
      "torch.Size([512])\n",
      "all_layers.55.conv_box.1.bias\n",
      "torch.Size([512])\n",
      "all_layers.55.conv_box.1.running_mean\n",
      "torch.Size([512])\n",
      "all_layers.55.conv_box.1.running_var\n",
      "torch.Size([512])\n",
      "all_layers.55.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.56.conv_box.0.weight\n",
      "torch.Size([256, 512, 1, 1])\n",
      "all_layers.56.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.56.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.56.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.56.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.56.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.58.conv_box.0.weight\n",
      "torch.Size([256, 512, 1, 1])\n",
      "all_layers.58.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.58.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.58.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.58.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.58.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.59.conv_box.0.weight\n",
      "torch.Size([256, 256, 1, 1])\n",
      "all_layers.59.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.59.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.59.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.59.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.59.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.60.conv_box.0.weight\n",
      "torch.Size([256, 256, 3, 3])\n",
      "all_layers.60.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.60.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.60.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.60.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.60.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.62.conv_box.0.weight\n",
      "torch.Size([256, 256, 1, 1])\n",
      "all_layers.62.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.62.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.62.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.62.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.62.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.63.conv_box.0.weight\n",
      "torch.Size([256, 256, 3, 3])\n",
      "all_layers.63.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.63.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.63.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.63.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.63.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.65.conv_box.0.weight\n",
      "torch.Size([256, 256, 1, 1])\n",
      "all_layers.65.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.65.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.65.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.65.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.65.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.66.conv_box.0.weight\n",
      "torch.Size([256, 256, 3, 3])\n",
      "all_layers.66.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.66.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.66.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.66.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.66.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.68.conv_box.0.weight\n",
      "torch.Size([256, 256, 1, 1])\n",
      "all_layers.68.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.68.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.68.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.68.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.68.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.69.conv_box.0.weight\n",
      "torch.Size([256, 256, 3, 3])\n",
      "all_layers.69.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.69.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.69.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.69.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.69.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.71.conv_box.0.weight\n",
      "torch.Size([256, 256, 1, 1])\n",
      "all_layers.71.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.71.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.71.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.71.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.71.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.72.conv_box.0.weight\n",
      "torch.Size([256, 256, 3, 3])\n",
      "all_layers.72.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.72.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.72.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.72.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.72.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.74.conv_box.0.weight\n",
      "torch.Size([256, 256, 1, 1])\n",
      "all_layers.74.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.74.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.74.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.74.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.74.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.75.conv_box.0.weight\n",
      "torch.Size([256, 256, 3, 3])\n",
      "all_layers.75.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.75.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.75.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.75.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.75.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.77.conv_box.0.weight\n",
      "torch.Size([256, 256, 1, 1])\n",
      "all_layers.77.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.77.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.77.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.77.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.77.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.78.conv_box.0.weight\n",
      "torch.Size([256, 256, 3, 3])\n",
      "all_layers.78.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.78.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.78.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.78.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.78.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.80.conv_box.0.weight\n",
      "torch.Size([256, 256, 1, 1])\n",
      "all_layers.80.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.80.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.80.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.80.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.80.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.81.conv_box.0.weight\n",
      "torch.Size([256, 256, 3, 3])\n",
      "all_layers.81.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.81.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.81.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.81.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.81.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.83.conv_box.0.weight\n",
      "torch.Size([256, 256, 1, 1])\n",
      "all_layers.83.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.83.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.83.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.83.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.83.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.85.conv_box.0.weight\n",
      "torch.Size([512, 256, 1, 1])\n",
      "all_layers.85.conv_box.1.weight\n",
      "torch.Size([512])\n",
      "all_layers.85.conv_box.1.bias\n",
      "torch.Size([512])\n",
      "all_layers.85.conv_box.1.running_mean\n",
      "torch.Size([512])\n",
      "all_layers.85.conv_box.1.running_var\n",
      "torch.Size([512])\n",
      "all_layers.85.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.86.conv_box.0.weight\n",
      "torch.Size([1024, 512, 3, 3])\n",
      "all_layers.86.conv_box.1.weight\n",
      "torch.Size([1024])\n",
      "all_layers.86.conv_box.1.bias\n",
      "torch.Size([1024])\n",
      "all_layers.86.conv_box.1.running_mean\n",
      "torch.Size([1024])\n",
      "all_layers.86.conv_box.1.running_var\n",
      "torch.Size([1024])\n",
      "all_layers.86.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.87.conv_box.0.weight\n",
      "torch.Size([512, 1024, 1, 1])\n",
      "all_layers.87.conv_box.1.weight\n",
      "torch.Size([512])\n",
      "all_layers.87.conv_box.1.bias\n",
      "torch.Size([512])\n",
      "all_layers.87.conv_box.1.running_mean\n",
      "torch.Size([512])\n",
      "all_layers.87.conv_box.1.running_var\n",
      "torch.Size([512])\n",
      "all_layers.87.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.89.conv_box.0.weight\n",
      "torch.Size([512, 1024, 1, 1])\n",
      "all_layers.89.conv_box.1.weight\n",
      "torch.Size([512])\n",
      "all_layers.89.conv_box.1.bias\n",
      "torch.Size([512])\n",
      "all_layers.89.conv_box.1.running_mean\n",
      "torch.Size([512])\n",
      "all_layers.89.conv_box.1.running_var\n",
      "torch.Size([512])\n",
      "all_layers.89.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.90.conv_box.0.weight\n",
      "torch.Size([512, 512, 1, 1])\n",
      "all_layers.90.conv_box.1.weight\n",
      "torch.Size([512])\n",
      "all_layers.90.conv_box.1.bias\n",
      "torch.Size([512])\n",
      "all_layers.90.conv_box.1.running_mean\n",
      "torch.Size([512])\n",
      "all_layers.90.conv_box.1.running_var\n",
      "torch.Size([512])\n",
      "all_layers.90.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.91.conv_box.0.weight\n",
      "torch.Size([512, 512, 3, 3])\n",
      "all_layers.91.conv_box.1.weight\n",
      "torch.Size([512])\n",
      "all_layers.91.conv_box.1.bias\n",
      "torch.Size([512])\n",
      "all_layers.91.conv_box.1.running_mean\n",
      "torch.Size([512])\n",
      "all_layers.91.conv_box.1.running_var\n",
      "torch.Size([512])\n",
      "all_layers.91.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.93.conv_box.0.weight\n",
      "torch.Size([512, 512, 1, 1])\n",
      "all_layers.93.conv_box.1.weight\n",
      "torch.Size([512])\n",
      "all_layers.93.conv_box.1.bias\n",
      "torch.Size([512])\n",
      "all_layers.93.conv_box.1.running_mean\n",
      "torch.Size([512])\n",
      "all_layers.93.conv_box.1.running_var\n",
      "torch.Size([512])\n",
      "all_layers.93.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.94.conv_box.0.weight\n",
      "torch.Size([512, 512, 3, 3])\n",
      "all_layers.94.conv_box.1.weight\n",
      "torch.Size([512])\n",
      "all_layers.94.conv_box.1.bias\n",
      "torch.Size([512])\n",
      "all_layers.94.conv_box.1.running_mean\n",
      "torch.Size([512])\n",
      "all_layers.94.conv_box.1.running_var\n",
      "torch.Size([512])\n",
      "all_layers.94.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.96.conv_box.0.weight\n",
      "torch.Size([512, 512, 1, 1])\n",
      "all_layers.96.conv_box.1.weight\n",
      "torch.Size([512])\n",
      "all_layers.96.conv_box.1.bias\n",
      "torch.Size([512])\n",
      "all_layers.96.conv_box.1.running_mean\n",
      "torch.Size([512])\n",
      "all_layers.96.conv_box.1.running_var\n",
      "torch.Size([512])\n",
      "all_layers.96.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.97.conv_box.0.weight\n",
      "torch.Size([512, 512, 3, 3])\n",
      "all_layers.97.conv_box.1.weight\n",
      "torch.Size([512])\n",
      "all_layers.97.conv_box.1.bias\n",
      "torch.Size([512])\n",
      "all_layers.97.conv_box.1.running_mean\n",
      "torch.Size([512])\n",
      "all_layers.97.conv_box.1.running_var\n",
      "torch.Size([512])\n",
      "all_layers.97.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.99.conv_box.0.weight\n",
      "torch.Size([512, 512, 1, 1])\n",
      "all_layers.99.conv_box.1.weight\n",
      "torch.Size([512])\n",
      "all_layers.99.conv_box.1.bias\n",
      "torch.Size([512])\n",
      "all_layers.99.conv_box.1.running_mean\n",
      "torch.Size([512])\n",
      "all_layers.99.conv_box.1.running_var\n",
      "torch.Size([512])\n",
      "all_layers.99.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.100.conv_box.0.weight\n",
      "torch.Size([512, 512, 3, 3])\n",
      "all_layers.100.conv_box.1.weight\n",
      "torch.Size([512])\n",
      "all_layers.100.conv_box.1.bias\n",
      "torch.Size([512])\n",
      "all_layers.100.conv_box.1.running_mean\n",
      "torch.Size([512])\n",
      "all_layers.100.conv_box.1.running_var\n",
      "torch.Size([512])\n",
      "all_layers.100.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.102.conv_box.0.weight\n",
      "torch.Size([512, 512, 1, 1])\n",
      "all_layers.102.conv_box.1.weight\n",
      "torch.Size([512])\n",
      "all_layers.102.conv_box.1.bias\n",
      "torch.Size([512])\n",
      "all_layers.102.conv_box.1.running_mean\n",
      "torch.Size([512])\n",
      "all_layers.102.conv_box.1.running_var\n",
      "torch.Size([512])\n",
      "all_layers.102.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.104.conv_box.0.weight\n",
      "torch.Size([1024, 512, 1, 1])\n",
      "all_layers.104.conv_box.1.weight\n",
      "torch.Size([1024])\n",
      "all_layers.104.conv_box.1.bias\n",
      "torch.Size([1024])\n",
      "all_layers.104.conv_box.1.running_mean\n",
      "torch.Size([1024])\n",
      "all_layers.104.conv_box.1.running_var\n",
      "torch.Size([1024])\n",
      "all_layers.104.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.105.conv_box.0.weight\n",
      "torch.Size([512, 1024, 1, 1])\n",
      "all_layers.105.conv_box.1.weight\n",
      "torch.Size([512])\n",
      "all_layers.105.conv_box.1.bias\n",
      "torch.Size([512])\n",
      "all_layers.105.conv_box.1.running_mean\n",
      "torch.Size([512])\n",
      "all_layers.105.conv_box.1.running_var\n",
      "torch.Size([512])\n",
      "all_layers.105.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.106.conv_box.0.weight\n",
      "torch.Size([1024, 512, 3, 3])\n",
      "all_layers.106.conv_box.1.weight\n",
      "torch.Size([1024])\n",
      "all_layers.106.conv_box.1.bias\n",
      "torch.Size([1024])\n",
      "all_layers.106.conv_box.1.running_mean\n",
      "torch.Size([1024])\n",
      "all_layers.106.conv_box.1.running_var\n",
      "torch.Size([1024])\n",
      "all_layers.106.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.107.conv_box.0.weight\n",
      "torch.Size([512, 1024, 1, 1])\n",
      "all_layers.107.conv_box.1.weight\n",
      "torch.Size([512])\n",
      "all_layers.107.conv_box.1.bias\n",
      "torch.Size([512])\n",
      "all_layers.107.conv_box.1.running_mean\n",
      "torch.Size([512])\n",
      "all_layers.107.conv_box.1.running_var\n",
      "torch.Size([512])\n",
      "all_layers.107.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.114.conv_box.0.weight\n",
      "torch.Size([512, 512, 1, 1])\n",
      "all_layers.114.conv_box.1.weight\n",
      "torch.Size([512])\n",
      "all_layers.114.conv_box.1.bias\n",
      "torch.Size([512])\n",
      "all_layers.114.conv_box.1.running_mean\n",
      "torch.Size([512])\n",
      "all_layers.114.conv_box.1.running_var\n",
      "torch.Size([512])\n",
      "all_layers.114.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.115.conv_box.0.weight\n",
      "torch.Size([1024, 512, 3, 3])\n",
      "all_layers.115.conv_box.1.weight\n",
      "torch.Size([1024])\n",
      "all_layers.115.conv_box.1.bias\n",
      "torch.Size([1024])\n",
      "all_layers.115.conv_box.1.running_mean\n",
      "torch.Size([1024])\n",
      "all_layers.115.conv_box.1.running_var\n",
      "torch.Size([1024])\n",
      "all_layers.115.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.116.conv_box.0.weight\n",
      "torch.Size([512, 1024, 1, 1])\n",
      "all_layers.116.conv_box.1.weight\n",
      "torch.Size([512])\n",
      "all_layers.116.conv_box.1.bias\n",
      "torch.Size([512])\n",
      "all_layers.116.conv_box.1.running_mean\n",
      "torch.Size([512])\n",
      "all_layers.116.conv_box.1.running_var\n",
      "torch.Size([512])\n",
      "all_layers.116.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.117.conv_box.0.weight\n",
      "torch.Size([256, 512, 1, 1])\n",
      "all_layers.117.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.117.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.117.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.117.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.117.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.120.conv_box.0.weight\n",
      "torch.Size([256, 512, 1, 1])\n",
      "all_layers.120.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.120.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.120.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.120.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.120.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.122.conv_box.0.weight\n",
      "torch.Size([256, 256, 1, 1])\n",
      "all_layers.122.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.122.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.122.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.122.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.122.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.123.conv_box.0.weight\n",
      "torch.Size([512, 256, 3, 3])\n",
      "all_layers.123.conv_box.1.weight\n",
      "torch.Size([512])\n",
      "all_layers.123.conv_box.1.bias\n",
      "torch.Size([512])\n",
      "all_layers.123.conv_box.1.running_mean\n",
      "torch.Size([512])\n",
      "all_layers.123.conv_box.1.running_var\n",
      "torch.Size([512])\n",
      "all_layers.123.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.124.conv_box.0.weight\n",
      "torch.Size([256, 512, 1, 1])\n",
      "all_layers.124.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.124.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.124.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.124.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.124.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.125.conv_box.0.weight\n",
      "torch.Size([512, 256, 3, 3])\n",
      "all_layers.125.conv_box.1.weight\n",
      "torch.Size([512])\n",
      "all_layers.125.conv_box.1.bias\n",
      "torch.Size([512])\n",
      "all_layers.125.conv_box.1.running_mean\n",
      "torch.Size([512])\n",
      "all_layers.125.conv_box.1.running_var\n",
      "torch.Size([512])\n",
      "all_layers.125.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.126.conv_box.0.weight\n",
      "torch.Size([256, 512, 1, 1])\n",
      "all_layers.126.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.126.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.126.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.126.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.126.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.127.conv_box.0.weight\n",
      "torch.Size([128, 256, 1, 1])\n",
      "all_layers.127.conv_box.1.weight\n",
      "torch.Size([128])\n",
      "all_layers.127.conv_box.1.bias\n",
      "torch.Size([128])\n",
      "all_layers.127.conv_box.1.running_mean\n",
      "torch.Size([128])\n",
      "all_layers.127.conv_box.1.running_var\n",
      "torch.Size([128])\n",
      "all_layers.127.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.130.conv_box.0.weight\n",
      "torch.Size([128, 256, 1, 1])\n",
      "all_layers.130.conv_box.1.weight\n",
      "torch.Size([128])\n",
      "all_layers.130.conv_box.1.bias\n",
      "torch.Size([128])\n",
      "all_layers.130.conv_box.1.running_mean\n",
      "torch.Size([128])\n",
      "all_layers.130.conv_box.1.running_var\n",
      "torch.Size([128])\n",
      "all_layers.130.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.132.conv_box.0.weight\n",
      "torch.Size([128, 128, 1, 1])\n",
      "all_layers.132.conv_box.1.weight\n",
      "torch.Size([128])\n",
      "all_layers.132.conv_box.1.bias\n",
      "torch.Size([128])\n",
      "all_layers.132.conv_box.1.running_mean\n",
      "torch.Size([128])\n",
      "all_layers.132.conv_box.1.running_var\n",
      "torch.Size([128])\n",
      "all_layers.132.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.133.conv_box.0.weight\n",
      "torch.Size([256, 128, 3, 3])\n",
      "all_layers.133.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.133.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.133.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.133.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.133.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.134.conv_box.0.weight\n",
      "torch.Size([128, 256, 1, 1])\n",
      "all_layers.134.conv_box.1.weight\n",
      "torch.Size([128])\n",
      "all_layers.134.conv_box.1.bias\n",
      "torch.Size([128])\n",
      "all_layers.134.conv_box.1.running_mean\n",
      "torch.Size([128])\n",
      "all_layers.134.conv_box.1.running_var\n",
      "torch.Size([128])\n",
      "all_layers.134.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.135.conv_box.0.weight\n",
      "torch.Size([256, 128, 3, 3])\n",
      "all_layers.135.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.135.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.135.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.135.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.135.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.136.conv_box.0.weight\n",
      "torch.Size([128, 256, 1, 1])\n",
      "all_layers.136.conv_box.1.weight\n",
      "torch.Size([128])\n",
      "all_layers.136.conv_box.1.bias\n",
      "torch.Size([128])\n",
      "all_layers.136.conv_box.1.running_mean\n",
      "torch.Size([128])\n",
      "all_layers.136.conv_box.1.running_var\n",
      "torch.Size([128])\n",
      "all_layers.136.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.137.conv_box.0.weight\n",
      "torch.Size([256, 128, 3, 3])\n",
      "all_layers.137.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.137.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.137.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.137.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.137.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.138.conv_box.0.weight\n",
      "torch.Size([255, 256, 1, 1])\n",
      "all_layers.138.conv_box.0.bias\n",
      "torch.Size([255])\n",
      "all_layers.141.conv_box.0.weight\n",
      "torch.Size([256, 128, 3, 3])\n",
      "all_layers.141.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.141.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.141.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.141.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.141.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.143.conv_box.0.weight\n",
      "torch.Size([256, 256, 1, 1])\n",
      "all_layers.143.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.143.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.143.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.143.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.143.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.144.conv_box.0.weight\n",
      "torch.Size([512, 256, 3, 3])\n",
      "all_layers.144.conv_box.1.weight\n",
      "torch.Size([512])\n",
      "all_layers.144.conv_box.1.bias\n",
      "torch.Size([512])\n",
      "all_layers.144.conv_box.1.running_mean\n",
      "torch.Size([512])\n",
      "all_layers.144.conv_box.1.running_var\n",
      "torch.Size([512])\n",
      "all_layers.144.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.145.conv_box.0.weight\n",
      "torch.Size([256, 512, 1, 1])\n",
      "all_layers.145.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.145.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.145.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.145.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.145.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.146.conv_box.0.weight\n",
      "torch.Size([512, 256, 3, 3])\n",
      "all_layers.146.conv_box.1.weight\n",
      "torch.Size([512])\n",
      "all_layers.146.conv_box.1.bias\n",
      "torch.Size([512])\n",
      "all_layers.146.conv_box.1.running_mean\n",
      "torch.Size([512])\n",
      "all_layers.146.conv_box.1.running_var\n",
      "torch.Size([512])\n",
      "all_layers.146.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.147.conv_box.0.weight\n",
      "torch.Size([256, 512, 1, 1])\n",
      "all_layers.147.conv_box.1.weight\n",
      "torch.Size([256])\n",
      "all_layers.147.conv_box.1.bias\n",
      "torch.Size([256])\n",
      "all_layers.147.conv_box.1.running_mean\n",
      "torch.Size([256])\n",
      "all_layers.147.conv_box.1.running_var\n",
      "torch.Size([256])\n",
      "all_layers.147.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.148.conv_box.0.weight\n",
      "torch.Size([512, 256, 3, 3])\n",
      "all_layers.148.conv_box.1.weight\n",
      "torch.Size([512])\n",
      "all_layers.148.conv_box.1.bias\n",
      "torch.Size([512])\n",
      "all_layers.148.conv_box.1.running_mean\n",
      "torch.Size([512])\n",
      "all_layers.148.conv_box.1.running_var\n",
      "torch.Size([512])\n",
      "all_layers.148.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.149.conv_box.0.weight\n",
      "torch.Size([255, 512, 1, 1])\n",
      "all_layers.149.conv_box.0.bias\n",
      "torch.Size([255])\n",
      "all_layers.152.conv_box.0.weight\n",
      "torch.Size([512, 256, 3, 3])\n",
      "all_layers.152.conv_box.1.weight\n",
      "torch.Size([512])\n",
      "all_layers.152.conv_box.1.bias\n",
      "torch.Size([512])\n",
      "all_layers.152.conv_box.1.running_mean\n",
      "torch.Size([512])\n",
      "all_layers.152.conv_box.1.running_var\n",
      "torch.Size([512])\n",
      "all_layers.152.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.154.conv_box.0.weight\n",
      "torch.Size([512, 512, 1, 1])\n",
      "all_layers.154.conv_box.1.weight\n",
      "torch.Size([512])\n",
      "all_layers.154.conv_box.1.bias\n",
      "torch.Size([512])\n",
      "all_layers.154.conv_box.1.running_mean\n",
      "torch.Size([512])\n",
      "all_layers.154.conv_box.1.running_var\n",
      "torch.Size([512])\n",
      "all_layers.154.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.155.conv_box.0.weight\n",
      "torch.Size([1024, 512, 3, 3])\n",
      "all_layers.155.conv_box.1.weight\n",
      "torch.Size([1024])\n",
      "all_layers.155.conv_box.1.bias\n",
      "torch.Size([1024])\n",
      "all_layers.155.conv_box.1.running_mean\n",
      "torch.Size([1024])\n",
      "all_layers.155.conv_box.1.running_var\n",
      "torch.Size([1024])\n",
      "all_layers.155.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.156.conv_box.0.weight\n",
      "torch.Size([512, 1024, 1, 1])\n",
      "all_layers.156.conv_box.1.weight\n",
      "torch.Size([512])\n",
      "all_layers.156.conv_box.1.bias\n",
      "torch.Size([512])\n",
      "all_layers.156.conv_box.1.running_mean\n",
      "torch.Size([512])\n",
      "all_layers.156.conv_box.1.running_var\n",
      "torch.Size([512])\n",
      "all_layers.156.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.157.conv_box.0.weight\n",
      "torch.Size([1024, 512, 3, 3])\n",
      "all_layers.157.conv_box.1.weight\n",
      "torch.Size([1024])\n",
      "all_layers.157.conv_box.1.bias\n",
      "torch.Size([1024])\n",
      "all_layers.157.conv_box.1.running_mean\n",
      "torch.Size([1024])\n",
      "all_layers.157.conv_box.1.running_var\n",
      "torch.Size([1024])\n",
      "all_layers.157.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.158.conv_box.0.weight\n",
      "torch.Size([512, 1024, 1, 1])\n",
      "all_layers.158.conv_box.1.weight\n",
      "torch.Size([512])\n",
      "all_layers.158.conv_box.1.bias\n",
      "torch.Size([512])\n",
      "all_layers.158.conv_box.1.running_mean\n",
      "torch.Size([512])\n",
      "all_layers.158.conv_box.1.running_var\n",
      "torch.Size([512])\n",
      "all_layers.158.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.159.conv_box.0.weight\n",
      "torch.Size([1024, 512, 3, 3])\n",
      "all_layers.159.conv_box.1.weight\n",
      "torch.Size([1024])\n",
      "all_layers.159.conv_box.1.bias\n",
      "torch.Size([1024])\n",
      "all_layers.159.conv_box.1.running_mean\n",
      "torch.Size([1024])\n",
      "all_layers.159.conv_box.1.running_var\n",
      "torch.Size([1024])\n",
      "all_layers.159.conv_box.1.num_batches_tracked\n",
      "torch.Size([])\n",
      "all_layers.160.conv_box.0.weight\n",
      "torch.Size([255, 1024, 1, 1])\n",
      "all_layers.160.conv_box.0.bias\n",
      "torch.Size([255])\n",
      "648\n"
     ]
    }
   ],
   "source": [
    "HX_weight = YOLO_v4_Module_WIP.state_dict()\n",
    "i = 0\n",
    "HX = []\n",
    "\n",
    "for kk in HX_weight:\n",
    "    i += 1\n",
    "    print(kk)\n",
    "    print(HX_weight[kk].size())\n",
    "    HX.append(HX_weight[kk].size())\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.load(\"D:/Installation/yolov4.pt\")\n",
    "#d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_list.0.Conv2d.weight\n",
      "torch.Size([32, 3, 3, 3])\n",
      "module_list.0.BatchNorm2d.weight\n",
      "torch.Size([32])\n",
      "module_list.0.BatchNorm2d.bias\n",
      "torch.Size([32])\n",
      "module_list.0.BatchNorm2d.running_mean\n",
      "torch.Size([32])\n",
      "module_list.0.BatchNorm2d.running_var\n",
      "torch.Size([32])\n",
      "module_list.0.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.1.Conv2d.weight\n",
      "torch.Size([64, 32, 3, 3])\n",
      "module_list.1.BatchNorm2d.weight\n",
      "torch.Size([64])\n",
      "module_list.1.BatchNorm2d.bias\n",
      "torch.Size([64])\n",
      "module_list.1.BatchNorm2d.running_mean\n",
      "torch.Size([64])\n",
      "module_list.1.BatchNorm2d.running_var\n",
      "torch.Size([64])\n",
      "module_list.1.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.2.Conv2d.weight\n",
      "torch.Size([64, 64, 1, 1])\n",
      "module_list.2.BatchNorm2d.weight\n",
      "torch.Size([64])\n",
      "module_list.2.BatchNorm2d.bias\n",
      "torch.Size([64])\n",
      "module_list.2.BatchNorm2d.running_mean\n",
      "torch.Size([64])\n",
      "module_list.2.BatchNorm2d.running_var\n",
      "torch.Size([64])\n",
      "module_list.2.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.4.Conv2d.weight\n",
      "torch.Size([64, 64, 1, 1])\n",
      "module_list.4.BatchNorm2d.weight\n",
      "torch.Size([64])\n",
      "module_list.4.BatchNorm2d.bias\n",
      "torch.Size([64])\n",
      "module_list.4.BatchNorm2d.running_mean\n",
      "torch.Size([64])\n",
      "module_list.4.BatchNorm2d.running_var\n",
      "torch.Size([64])\n",
      "module_list.4.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.5.Conv2d.weight\n",
      "torch.Size([32, 64, 1, 1])\n",
      "module_list.5.BatchNorm2d.weight\n",
      "torch.Size([32])\n",
      "module_list.5.BatchNorm2d.bias\n",
      "torch.Size([32])\n",
      "module_list.5.BatchNorm2d.running_mean\n",
      "torch.Size([32])\n",
      "module_list.5.BatchNorm2d.running_var\n",
      "torch.Size([32])\n",
      "module_list.5.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.6.Conv2d.weight\n",
      "torch.Size([64, 32, 3, 3])\n",
      "module_list.6.BatchNorm2d.weight\n",
      "torch.Size([64])\n",
      "module_list.6.BatchNorm2d.bias\n",
      "torch.Size([64])\n",
      "module_list.6.BatchNorm2d.running_mean\n",
      "torch.Size([64])\n",
      "module_list.6.BatchNorm2d.running_var\n",
      "torch.Size([64])\n",
      "module_list.6.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.8.Conv2d.weight\n",
      "torch.Size([64, 64, 1, 1])\n",
      "module_list.8.BatchNorm2d.weight\n",
      "torch.Size([64])\n",
      "module_list.8.BatchNorm2d.bias\n",
      "torch.Size([64])\n",
      "module_list.8.BatchNorm2d.running_mean\n",
      "torch.Size([64])\n",
      "module_list.8.BatchNorm2d.running_var\n",
      "torch.Size([64])\n",
      "module_list.8.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.10.Conv2d.weight\n",
      "torch.Size([64, 128, 1, 1])\n",
      "module_list.10.BatchNorm2d.weight\n",
      "torch.Size([64])\n",
      "module_list.10.BatchNorm2d.bias\n",
      "torch.Size([64])\n",
      "module_list.10.BatchNorm2d.running_mean\n",
      "torch.Size([64])\n",
      "module_list.10.BatchNorm2d.running_var\n",
      "torch.Size([64])\n",
      "module_list.10.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.11.Conv2d.weight\n",
      "torch.Size([128, 64, 3, 3])\n",
      "module_list.11.BatchNorm2d.weight\n",
      "torch.Size([128])\n",
      "module_list.11.BatchNorm2d.bias\n",
      "torch.Size([128])\n",
      "module_list.11.BatchNorm2d.running_mean\n",
      "torch.Size([128])\n",
      "module_list.11.BatchNorm2d.running_var\n",
      "torch.Size([128])\n",
      "module_list.11.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.12.Conv2d.weight\n",
      "torch.Size([64, 128, 1, 1])\n",
      "module_list.12.BatchNorm2d.weight\n",
      "torch.Size([64])\n",
      "module_list.12.BatchNorm2d.bias\n",
      "torch.Size([64])\n",
      "module_list.12.BatchNorm2d.running_mean\n",
      "torch.Size([64])\n",
      "module_list.12.BatchNorm2d.running_var\n",
      "torch.Size([64])\n",
      "module_list.12.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.14.Conv2d.weight\n",
      "torch.Size([64, 128, 1, 1])\n",
      "module_list.14.BatchNorm2d.weight\n",
      "torch.Size([64])\n",
      "module_list.14.BatchNorm2d.bias\n",
      "torch.Size([64])\n",
      "module_list.14.BatchNorm2d.running_mean\n",
      "torch.Size([64])\n",
      "module_list.14.BatchNorm2d.running_var\n",
      "torch.Size([64])\n",
      "module_list.14.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.15.Conv2d.weight\n",
      "torch.Size([64, 64, 1, 1])\n",
      "module_list.15.BatchNorm2d.weight\n",
      "torch.Size([64])\n",
      "module_list.15.BatchNorm2d.bias\n",
      "torch.Size([64])\n",
      "module_list.15.BatchNorm2d.running_mean\n",
      "torch.Size([64])\n",
      "module_list.15.BatchNorm2d.running_var\n",
      "torch.Size([64])\n",
      "module_list.15.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.16.Conv2d.weight\n",
      "torch.Size([64, 64, 3, 3])\n",
      "module_list.16.BatchNorm2d.weight\n",
      "torch.Size([64])\n",
      "module_list.16.BatchNorm2d.bias\n",
      "torch.Size([64])\n",
      "module_list.16.BatchNorm2d.running_mean\n",
      "torch.Size([64])\n",
      "module_list.16.BatchNorm2d.running_var\n",
      "torch.Size([64])\n",
      "module_list.16.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.18.Conv2d.weight\n",
      "torch.Size([64, 64, 1, 1])\n",
      "module_list.18.BatchNorm2d.weight\n",
      "torch.Size([64])\n",
      "module_list.18.BatchNorm2d.bias\n",
      "torch.Size([64])\n",
      "module_list.18.BatchNorm2d.running_mean\n",
      "torch.Size([64])\n",
      "module_list.18.BatchNorm2d.running_var\n",
      "torch.Size([64])\n",
      "module_list.18.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.19.Conv2d.weight\n",
      "torch.Size([64, 64, 3, 3])\n",
      "module_list.19.BatchNorm2d.weight\n",
      "torch.Size([64])\n",
      "module_list.19.BatchNorm2d.bias\n",
      "torch.Size([64])\n",
      "module_list.19.BatchNorm2d.running_mean\n",
      "torch.Size([64])\n",
      "module_list.19.BatchNorm2d.running_var\n",
      "torch.Size([64])\n",
      "module_list.19.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.21.Conv2d.weight\n",
      "torch.Size([64, 64, 1, 1])\n",
      "module_list.21.BatchNorm2d.weight\n",
      "torch.Size([64])\n",
      "module_list.21.BatchNorm2d.bias\n",
      "torch.Size([64])\n",
      "module_list.21.BatchNorm2d.running_mean\n",
      "torch.Size([64])\n",
      "module_list.21.BatchNorm2d.running_var\n",
      "torch.Size([64])\n",
      "module_list.21.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.23.Conv2d.weight\n",
      "torch.Size([128, 128, 1, 1])\n",
      "module_list.23.BatchNorm2d.weight\n",
      "torch.Size([128])\n",
      "module_list.23.BatchNorm2d.bias\n",
      "torch.Size([128])\n",
      "module_list.23.BatchNorm2d.running_mean\n",
      "torch.Size([128])\n",
      "module_list.23.BatchNorm2d.running_var\n",
      "torch.Size([128])\n",
      "module_list.23.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.24.Conv2d.weight\n",
      "torch.Size([256, 128, 3, 3])\n",
      "module_list.24.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.24.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.24.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.24.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.24.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.25.Conv2d.weight\n",
      "torch.Size([128, 256, 1, 1])\n",
      "module_list.25.BatchNorm2d.weight\n",
      "torch.Size([128])\n",
      "module_list.25.BatchNorm2d.bias\n",
      "torch.Size([128])\n",
      "module_list.25.BatchNorm2d.running_mean\n",
      "torch.Size([128])\n",
      "module_list.25.BatchNorm2d.running_var\n",
      "torch.Size([128])\n",
      "module_list.25.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.27.Conv2d.weight\n",
      "torch.Size([128, 256, 1, 1])\n",
      "module_list.27.BatchNorm2d.weight\n",
      "torch.Size([128])\n",
      "module_list.27.BatchNorm2d.bias\n",
      "torch.Size([128])\n",
      "module_list.27.BatchNorm2d.running_mean\n",
      "torch.Size([128])\n",
      "module_list.27.BatchNorm2d.running_var\n",
      "torch.Size([128])\n",
      "module_list.27.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.28.Conv2d.weight\n",
      "torch.Size([128, 128, 1, 1])\n",
      "module_list.28.BatchNorm2d.weight\n",
      "torch.Size([128])\n",
      "module_list.28.BatchNorm2d.bias\n",
      "torch.Size([128])\n",
      "module_list.28.BatchNorm2d.running_mean\n",
      "torch.Size([128])\n",
      "module_list.28.BatchNorm2d.running_var\n",
      "torch.Size([128])\n",
      "module_list.28.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.29.Conv2d.weight\n",
      "torch.Size([128, 128, 3, 3])\n",
      "module_list.29.BatchNorm2d.weight\n",
      "torch.Size([128])\n",
      "module_list.29.BatchNorm2d.bias\n",
      "torch.Size([128])\n",
      "module_list.29.BatchNorm2d.running_mean\n",
      "torch.Size([128])\n",
      "module_list.29.BatchNorm2d.running_var\n",
      "torch.Size([128])\n",
      "module_list.29.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.31.Conv2d.weight\n",
      "torch.Size([128, 128, 1, 1])\n",
      "module_list.31.BatchNorm2d.weight\n",
      "torch.Size([128])\n",
      "module_list.31.BatchNorm2d.bias\n",
      "torch.Size([128])\n",
      "module_list.31.BatchNorm2d.running_mean\n",
      "torch.Size([128])\n",
      "module_list.31.BatchNorm2d.running_var\n",
      "torch.Size([128])\n",
      "module_list.31.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.32.Conv2d.weight\n",
      "torch.Size([128, 128, 3, 3])\n",
      "module_list.32.BatchNorm2d.weight\n",
      "torch.Size([128])\n",
      "module_list.32.BatchNorm2d.bias\n",
      "torch.Size([128])\n",
      "module_list.32.BatchNorm2d.running_mean\n",
      "torch.Size([128])\n",
      "module_list.32.BatchNorm2d.running_var\n",
      "torch.Size([128])\n",
      "module_list.32.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.34.Conv2d.weight\n",
      "torch.Size([128, 128, 1, 1])\n",
      "module_list.34.BatchNorm2d.weight\n",
      "torch.Size([128])\n",
      "module_list.34.BatchNorm2d.bias\n",
      "torch.Size([128])\n",
      "module_list.34.BatchNorm2d.running_mean\n",
      "torch.Size([128])\n",
      "module_list.34.BatchNorm2d.running_var\n",
      "torch.Size([128])\n",
      "module_list.34.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.35.Conv2d.weight\n",
      "torch.Size([128, 128, 3, 3])\n",
      "module_list.35.BatchNorm2d.weight\n",
      "torch.Size([128])\n",
      "module_list.35.BatchNorm2d.bias\n",
      "torch.Size([128])\n",
      "module_list.35.BatchNorm2d.running_mean\n",
      "torch.Size([128])\n",
      "module_list.35.BatchNorm2d.running_var\n",
      "torch.Size([128])\n",
      "module_list.35.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.37.Conv2d.weight\n",
      "torch.Size([128, 128, 1, 1])\n",
      "module_list.37.BatchNorm2d.weight\n",
      "torch.Size([128])\n",
      "module_list.37.BatchNorm2d.bias\n",
      "torch.Size([128])\n",
      "module_list.37.BatchNorm2d.running_mean\n",
      "torch.Size([128])\n",
      "module_list.37.BatchNorm2d.running_var\n",
      "torch.Size([128])\n",
      "module_list.37.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.38.Conv2d.weight\n",
      "torch.Size([128, 128, 3, 3])\n",
      "module_list.38.BatchNorm2d.weight\n",
      "torch.Size([128])\n",
      "module_list.38.BatchNorm2d.bias\n",
      "torch.Size([128])\n",
      "module_list.38.BatchNorm2d.running_mean\n",
      "torch.Size([128])\n",
      "module_list.38.BatchNorm2d.running_var\n",
      "torch.Size([128])\n",
      "module_list.38.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.40.Conv2d.weight\n",
      "torch.Size([128, 128, 1, 1])\n",
      "module_list.40.BatchNorm2d.weight\n",
      "torch.Size([128])\n",
      "module_list.40.BatchNorm2d.bias\n",
      "torch.Size([128])\n",
      "module_list.40.BatchNorm2d.running_mean\n",
      "torch.Size([128])\n",
      "module_list.40.BatchNorm2d.running_var\n",
      "torch.Size([128])\n",
      "module_list.40.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.41.Conv2d.weight\n",
      "torch.Size([128, 128, 3, 3])\n",
      "module_list.41.BatchNorm2d.weight\n",
      "torch.Size([128])\n",
      "module_list.41.BatchNorm2d.bias\n",
      "torch.Size([128])\n",
      "module_list.41.BatchNorm2d.running_mean\n",
      "torch.Size([128])\n",
      "module_list.41.BatchNorm2d.running_var\n",
      "torch.Size([128])\n",
      "module_list.41.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.43.Conv2d.weight\n",
      "torch.Size([128, 128, 1, 1])\n",
      "module_list.43.BatchNorm2d.weight\n",
      "torch.Size([128])\n",
      "module_list.43.BatchNorm2d.bias\n",
      "torch.Size([128])\n",
      "module_list.43.BatchNorm2d.running_mean\n",
      "torch.Size([128])\n",
      "module_list.43.BatchNorm2d.running_var\n",
      "torch.Size([128])\n",
      "module_list.43.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.44.Conv2d.weight\n",
      "torch.Size([128, 128, 3, 3])\n",
      "module_list.44.BatchNorm2d.weight\n",
      "torch.Size([128])\n",
      "module_list.44.BatchNorm2d.bias\n",
      "torch.Size([128])\n",
      "module_list.44.BatchNorm2d.running_mean\n",
      "torch.Size([128])\n",
      "module_list.44.BatchNorm2d.running_var\n",
      "torch.Size([128])\n",
      "module_list.44.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.46.Conv2d.weight\n",
      "torch.Size([128, 128, 1, 1])\n",
      "module_list.46.BatchNorm2d.weight\n",
      "torch.Size([128])\n",
      "module_list.46.BatchNorm2d.bias\n",
      "torch.Size([128])\n",
      "module_list.46.BatchNorm2d.running_mean\n",
      "torch.Size([128])\n",
      "module_list.46.BatchNorm2d.running_var\n",
      "torch.Size([128])\n",
      "module_list.46.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.47.Conv2d.weight\n",
      "torch.Size([128, 128, 3, 3])\n",
      "module_list.47.BatchNorm2d.weight\n",
      "torch.Size([128])\n",
      "module_list.47.BatchNorm2d.bias\n",
      "torch.Size([128])\n",
      "module_list.47.BatchNorm2d.running_mean\n",
      "torch.Size([128])\n",
      "module_list.47.BatchNorm2d.running_var\n",
      "torch.Size([128])\n",
      "module_list.47.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.49.Conv2d.weight\n",
      "torch.Size([128, 128, 1, 1])\n",
      "module_list.49.BatchNorm2d.weight\n",
      "torch.Size([128])\n",
      "module_list.49.BatchNorm2d.bias\n",
      "torch.Size([128])\n",
      "module_list.49.BatchNorm2d.running_mean\n",
      "torch.Size([128])\n",
      "module_list.49.BatchNorm2d.running_var\n",
      "torch.Size([128])\n",
      "module_list.49.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.50.Conv2d.weight\n",
      "torch.Size([128, 128, 3, 3])\n",
      "module_list.50.BatchNorm2d.weight\n",
      "torch.Size([128])\n",
      "module_list.50.BatchNorm2d.bias\n",
      "torch.Size([128])\n",
      "module_list.50.BatchNorm2d.running_mean\n",
      "torch.Size([128])\n",
      "module_list.50.BatchNorm2d.running_var\n",
      "torch.Size([128])\n",
      "module_list.50.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.52.Conv2d.weight\n",
      "torch.Size([128, 128, 1, 1])\n",
      "module_list.52.BatchNorm2d.weight\n",
      "torch.Size([128])\n",
      "module_list.52.BatchNorm2d.bias\n",
      "torch.Size([128])\n",
      "module_list.52.BatchNorm2d.running_mean\n",
      "torch.Size([128])\n",
      "module_list.52.BatchNorm2d.running_var\n",
      "torch.Size([128])\n",
      "module_list.52.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.54.Conv2d.weight\n",
      "torch.Size([256, 256, 1, 1])\n",
      "module_list.54.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.54.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.54.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.54.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.54.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.55.Conv2d.weight\n",
      "torch.Size([512, 256, 3, 3])\n",
      "module_list.55.BatchNorm2d.weight\n",
      "torch.Size([512])\n",
      "module_list.55.BatchNorm2d.bias\n",
      "torch.Size([512])\n",
      "module_list.55.BatchNorm2d.running_mean\n",
      "torch.Size([512])\n",
      "module_list.55.BatchNorm2d.running_var\n",
      "torch.Size([512])\n",
      "module_list.55.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.56.Conv2d.weight\n",
      "torch.Size([256, 512, 1, 1])\n",
      "module_list.56.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.56.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.56.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.56.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.56.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.58.Conv2d.weight\n",
      "torch.Size([256, 512, 1, 1])\n",
      "module_list.58.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.58.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.58.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.58.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.58.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.59.Conv2d.weight\n",
      "torch.Size([256, 256, 1, 1])\n",
      "module_list.59.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.59.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.59.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.59.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.59.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.60.Conv2d.weight\n",
      "torch.Size([256, 256, 3, 3])\n",
      "module_list.60.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.60.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.60.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.60.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.60.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.62.Conv2d.weight\n",
      "torch.Size([256, 256, 1, 1])\n",
      "module_list.62.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.62.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.62.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.62.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.62.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.63.Conv2d.weight\n",
      "torch.Size([256, 256, 3, 3])\n",
      "module_list.63.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.63.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.63.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.63.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.63.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.65.Conv2d.weight\n",
      "torch.Size([256, 256, 1, 1])\n",
      "module_list.65.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.65.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.65.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.65.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.65.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.66.Conv2d.weight\n",
      "torch.Size([256, 256, 3, 3])\n",
      "module_list.66.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.66.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.66.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.66.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.66.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.68.Conv2d.weight\n",
      "torch.Size([256, 256, 1, 1])\n",
      "module_list.68.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.68.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.68.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.68.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.68.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.69.Conv2d.weight\n",
      "torch.Size([256, 256, 3, 3])\n",
      "module_list.69.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.69.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.69.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.69.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.69.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.71.Conv2d.weight\n",
      "torch.Size([256, 256, 1, 1])\n",
      "module_list.71.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.71.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.71.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.71.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.71.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.72.Conv2d.weight\n",
      "torch.Size([256, 256, 3, 3])\n",
      "module_list.72.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.72.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.72.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.72.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.72.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.74.Conv2d.weight\n",
      "torch.Size([256, 256, 1, 1])\n",
      "module_list.74.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.74.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.74.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.74.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.74.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.75.Conv2d.weight\n",
      "torch.Size([256, 256, 3, 3])\n",
      "module_list.75.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.75.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.75.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.75.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.75.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.77.Conv2d.weight\n",
      "torch.Size([256, 256, 1, 1])\n",
      "module_list.77.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.77.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.77.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.77.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.77.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.78.Conv2d.weight\n",
      "torch.Size([256, 256, 3, 3])\n",
      "module_list.78.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.78.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.78.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.78.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.78.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.80.Conv2d.weight\n",
      "torch.Size([256, 256, 1, 1])\n",
      "module_list.80.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.80.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.80.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.80.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.80.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.81.Conv2d.weight\n",
      "torch.Size([256, 256, 3, 3])\n",
      "module_list.81.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.81.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.81.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.81.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.81.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.83.Conv2d.weight\n",
      "torch.Size([256, 256, 1, 1])\n",
      "module_list.83.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.83.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.83.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.83.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.83.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.85.Conv2d.weight\n",
      "torch.Size([512, 512, 1, 1])\n",
      "module_list.85.BatchNorm2d.weight\n",
      "torch.Size([512])\n",
      "module_list.85.BatchNorm2d.bias\n",
      "torch.Size([512])\n",
      "module_list.85.BatchNorm2d.running_mean\n",
      "torch.Size([512])\n",
      "module_list.85.BatchNorm2d.running_var\n",
      "torch.Size([512])\n",
      "module_list.85.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.86.Conv2d.weight\n",
      "torch.Size([1024, 512, 3, 3])\n",
      "module_list.86.BatchNorm2d.weight\n",
      "torch.Size([1024])\n",
      "module_list.86.BatchNorm2d.bias\n",
      "torch.Size([1024])\n",
      "module_list.86.BatchNorm2d.running_mean\n",
      "torch.Size([1024])\n",
      "module_list.86.BatchNorm2d.running_var\n",
      "torch.Size([1024])\n",
      "module_list.86.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.87.Conv2d.weight\n",
      "torch.Size([512, 1024, 1, 1])\n",
      "module_list.87.BatchNorm2d.weight\n",
      "torch.Size([512])\n",
      "module_list.87.BatchNorm2d.bias\n",
      "torch.Size([512])\n",
      "module_list.87.BatchNorm2d.running_mean\n",
      "torch.Size([512])\n",
      "module_list.87.BatchNorm2d.running_var\n",
      "torch.Size([512])\n",
      "module_list.87.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.89.Conv2d.weight\n",
      "torch.Size([512, 1024, 1, 1])\n",
      "module_list.89.BatchNorm2d.weight\n",
      "torch.Size([512])\n",
      "module_list.89.BatchNorm2d.bias\n",
      "torch.Size([512])\n",
      "module_list.89.BatchNorm2d.running_mean\n",
      "torch.Size([512])\n",
      "module_list.89.BatchNorm2d.running_var\n",
      "torch.Size([512])\n",
      "module_list.89.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.90.Conv2d.weight\n",
      "torch.Size([512, 512, 1, 1])\n",
      "module_list.90.BatchNorm2d.weight\n",
      "torch.Size([512])\n",
      "module_list.90.BatchNorm2d.bias\n",
      "torch.Size([512])\n",
      "module_list.90.BatchNorm2d.running_mean\n",
      "torch.Size([512])\n",
      "module_list.90.BatchNorm2d.running_var\n",
      "torch.Size([512])\n",
      "module_list.90.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.91.Conv2d.weight\n",
      "torch.Size([512, 512, 3, 3])\n",
      "module_list.91.BatchNorm2d.weight\n",
      "torch.Size([512])\n",
      "module_list.91.BatchNorm2d.bias\n",
      "torch.Size([512])\n",
      "module_list.91.BatchNorm2d.running_mean\n",
      "torch.Size([512])\n",
      "module_list.91.BatchNorm2d.running_var\n",
      "torch.Size([512])\n",
      "module_list.91.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.93.Conv2d.weight\n",
      "torch.Size([512, 512, 1, 1])\n",
      "module_list.93.BatchNorm2d.weight\n",
      "torch.Size([512])\n",
      "module_list.93.BatchNorm2d.bias\n",
      "torch.Size([512])\n",
      "module_list.93.BatchNorm2d.running_mean\n",
      "torch.Size([512])\n",
      "module_list.93.BatchNorm2d.running_var\n",
      "torch.Size([512])\n",
      "module_list.93.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.94.Conv2d.weight\n",
      "torch.Size([512, 512, 3, 3])\n",
      "module_list.94.BatchNorm2d.weight\n",
      "torch.Size([512])\n",
      "module_list.94.BatchNorm2d.bias\n",
      "torch.Size([512])\n",
      "module_list.94.BatchNorm2d.running_mean\n",
      "torch.Size([512])\n",
      "module_list.94.BatchNorm2d.running_var\n",
      "torch.Size([512])\n",
      "module_list.94.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.96.Conv2d.weight\n",
      "torch.Size([512, 512, 1, 1])\n",
      "module_list.96.BatchNorm2d.weight\n",
      "torch.Size([512])\n",
      "module_list.96.BatchNorm2d.bias\n",
      "torch.Size([512])\n",
      "module_list.96.BatchNorm2d.running_mean\n",
      "torch.Size([512])\n",
      "module_list.96.BatchNorm2d.running_var\n",
      "torch.Size([512])\n",
      "module_list.96.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.97.Conv2d.weight\n",
      "torch.Size([512, 512, 3, 3])\n",
      "module_list.97.BatchNorm2d.weight\n",
      "torch.Size([512])\n",
      "module_list.97.BatchNorm2d.bias\n",
      "torch.Size([512])\n",
      "module_list.97.BatchNorm2d.running_mean\n",
      "torch.Size([512])\n",
      "module_list.97.BatchNorm2d.running_var\n",
      "torch.Size([512])\n",
      "module_list.97.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.99.Conv2d.weight\n",
      "torch.Size([512, 512, 1, 1])\n",
      "module_list.99.BatchNorm2d.weight\n",
      "torch.Size([512])\n",
      "module_list.99.BatchNorm2d.bias\n",
      "torch.Size([512])\n",
      "module_list.99.BatchNorm2d.running_mean\n",
      "torch.Size([512])\n",
      "module_list.99.BatchNorm2d.running_var\n",
      "torch.Size([512])\n",
      "module_list.99.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.100.Conv2d.weight\n",
      "torch.Size([512, 512, 3, 3])\n",
      "module_list.100.BatchNorm2d.weight\n",
      "torch.Size([512])\n",
      "module_list.100.BatchNorm2d.bias\n",
      "torch.Size([512])\n",
      "module_list.100.BatchNorm2d.running_mean\n",
      "torch.Size([512])\n",
      "module_list.100.BatchNorm2d.running_var\n",
      "torch.Size([512])\n",
      "module_list.100.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.102.Conv2d.weight\n",
      "torch.Size([512, 512, 1, 1])\n",
      "module_list.102.BatchNorm2d.weight\n",
      "torch.Size([512])\n",
      "module_list.102.BatchNorm2d.bias\n",
      "torch.Size([512])\n",
      "module_list.102.BatchNorm2d.running_mean\n",
      "torch.Size([512])\n",
      "module_list.102.BatchNorm2d.running_var\n",
      "torch.Size([512])\n",
      "module_list.102.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.104.Conv2d.weight\n",
      "torch.Size([1024, 1024, 1, 1])\n",
      "module_list.104.BatchNorm2d.weight\n",
      "torch.Size([1024])\n",
      "module_list.104.BatchNorm2d.bias\n",
      "torch.Size([1024])\n",
      "module_list.104.BatchNorm2d.running_mean\n",
      "torch.Size([1024])\n",
      "module_list.104.BatchNorm2d.running_var\n",
      "torch.Size([1024])\n",
      "module_list.104.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.105.Conv2d.weight\n",
      "torch.Size([512, 1024, 1, 1])\n",
      "module_list.105.BatchNorm2d.weight\n",
      "torch.Size([512])\n",
      "module_list.105.BatchNorm2d.bias\n",
      "torch.Size([512])\n",
      "module_list.105.BatchNorm2d.running_mean\n",
      "torch.Size([512])\n",
      "module_list.105.BatchNorm2d.running_var\n",
      "torch.Size([512])\n",
      "module_list.105.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.106.Conv2d.weight\n",
      "torch.Size([1024, 512, 3, 3])\n",
      "module_list.106.BatchNorm2d.weight\n",
      "torch.Size([1024])\n",
      "module_list.106.BatchNorm2d.bias\n",
      "torch.Size([1024])\n",
      "module_list.106.BatchNorm2d.running_mean\n",
      "torch.Size([1024])\n",
      "module_list.106.BatchNorm2d.running_var\n",
      "torch.Size([1024])\n",
      "module_list.106.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.107.Conv2d.weight\n",
      "torch.Size([512, 1024, 1, 1])\n",
      "module_list.107.BatchNorm2d.weight\n",
      "torch.Size([512])\n",
      "module_list.107.BatchNorm2d.bias\n",
      "torch.Size([512])\n",
      "module_list.107.BatchNorm2d.running_mean\n",
      "torch.Size([512])\n",
      "module_list.107.BatchNorm2d.running_var\n",
      "torch.Size([512])\n",
      "module_list.107.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.114.Conv2d.weight\n",
      "torch.Size([512, 2048, 1, 1])\n",
      "module_list.114.BatchNorm2d.weight\n",
      "torch.Size([512])\n",
      "module_list.114.BatchNorm2d.bias\n",
      "torch.Size([512])\n",
      "module_list.114.BatchNorm2d.running_mean\n",
      "torch.Size([512])\n",
      "module_list.114.BatchNorm2d.running_var\n",
      "torch.Size([512])\n",
      "module_list.114.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.115.Conv2d.weight\n",
      "torch.Size([1024, 512, 3, 3])\n",
      "module_list.115.BatchNorm2d.weight\n",
      "torch.Size([1024])\n",
      "module_list.115.BatchNorm2d.bias\n",
      "torch.Size([1024])\n",
      "module_list.115.BatchNorm2d.running_mean\n",
      "torch.Size([1024])\n",
      "module_list.115.BatchNorm2d.running_var\n",
      "torch.Size([1024])\n",
      "module_list.115.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.116.Conv2d.weight\n",
      "torch.Size([512, 1024, 1, 1])\n",
      "module_list.116.BatchNorm2d.weight\n",
      "torch.Size([512])\n",
      "module_list.116.BatchNorm2d.bias\n",
      "torch.Size([512])\n",
      "module_list.116.BatchNorm2d.running_mean\n",
      "torch.Size([512])\n",
      "module_list.116.BatchNorm2d.running_var\n",
      "torch.Size([512])\n",
      "module_list.116.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.117.Conv2d.weight\n",
      "torch.Size([256, 512, 1, 1])\n",
      "module_list.117.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.117.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.117.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.117.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.117.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.120.Conv2d.weight\n",
      "torch.Size([256, 512, 1, 1])\n",
      "module_list.120.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.120.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.120.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.120.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.120.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.122.Conv2d.weight\n",
      "torch.Size([256, 512, 1, 1])\n",
      "module_list.122.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.122.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.122.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.122.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.122.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.123.Conv2d.weight\n",
      "torch.Size([512, 256, 3, 3])\n",
      "module_list.123.BatchNorm2d.weight\n",
      "torch.Size([512])\n",
      "module_list.123.BatchNorm2d.bias\n",
      "torch.Size([512])\n",
      "module_list.123.BatchNorm2d.running_mean\n",
      "torch.Size([512])\n",
      "module_list.123.BatchNorm2d.running_var\n",
      "torch.Size([512])\n",
      "module_list.123.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.124.Conv2d.weight\n",
      "torch.Size([256, 512, 1, 1])\n",
      "module_list.124.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.124.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.124.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.124.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.124.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.125.Conv2d.weight\n",
      "torch.Size([512, 256, 3, 3])\n",
      "module_list.125.BatchNorm2d.weight\n",
      "torch.Size([512])\n",
      "module_list.125.BatchNorm2d.bias\n",
      "torch.Size([512])\n",
      "module_list.125.BatchNorm2d.running_mean\n",
      "torch.Size([512])\n",
      "module_list.125.BatchNorm2d.running_var\n",
      "torch.Size([512])\n",
      "module_list.125.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.126.Conv2d.weight\n",
      "torch.Size([256, 512, 1, 1])\n",
      "module_list.126.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.126.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.126.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.126.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.126.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.127.Conv2d.weight\n",
      "torch.Size([128, 256, 1, 1])\n",
      "module_list.127.BatchNorm2d.weight\n",
      "torch.Size([128])\n",
      "module_list.127.BatchNorm2d.bias\n",
      "torch.Size([128])\n",
      "module_list.127.BatchNorm2d.running_mean\n",
      "torch.Size([128])\n",
      "module_list.127.BatchNorm2d.running_var\n",
      "torch.Size([128])\n",
      "module_list.127.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.130.Conv2d.weight\n",
      "torch.Size([128, 256, 1, 1])\n",
      "module_list.130.BatchNorm2d.weight\n",
      "torch.Size([128])\n",
      "module_list.130.BatchNorm2d.bias\n",
      "torch.Size([128])\n",
      "module_list.130.BatchNorm2d.running_mean\n",
      "torch.Size([128])\n",
      "module_list.130.BatchNorm2d.running_var\n",
      "torch.Size([128])\n",
      "module_list.130.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.132.Conv2d.weight\n",
      "torch.Size([128, 256, 1, 1])\n",
      "module_list.132.BatchNorm2d.weight\n",
      "torch.Size([128])\n",
      "module_list.132.BatchNorm2d.bias\n",
      "torch.Size([128])\n",
      "module_list.132.BatchNorm2d.running_mean\n",
      "torch.Size([128])\n",
      "module_list.132.BatchNorm2d.running_var\n",
      "torch.Size([128])\n",
      "module_list.132.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.133.Conv2d.weight\n",
      "torch.Size([256, 128, 3, 3])\n",
      "module_list.133.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.133.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.133.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.133.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.133.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.134.Conv2d.weight\n",
      "torch.Size([128, 256, 1, 1])\n",
      "module_list.134.BatchNorm2d.weight\n",
      "torch.Size([128])\n",
      "module_list.134.BatchNorm2d.bias\n",
      "torch.Size([128])\n",
      "module_list.134.BatchNorm2d.running_mean\n",
      "torch.Size([128])\n",
      "module_list.134.BatchNorm2d.running_var\n",
      "torch.Size([128])\n",
      "module_list.134.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.135.Conv2d.weight\n",
      "torch.Size([256, 128, 3, 3])\n",
      "module_list.135.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.135.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.135.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.135.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.135.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.136.Conv2d.weight\n",
      "torch.Size([128, 256, 1, 1])\n",
      "module_list.136.BatchNorm2d.weight\n",
      "torch.Size([128])\n",
      "module_list.136.BatchNorm2d.bias\n",
      "torch.Size([128])\n",
      "module_list.136.BatchNorm2d.running_mean\n",
      "torch.Size([128])\n",
      "module_list.136.BatchNorm2d.running_var\n",
      "torch.Size([128])\n",
      "module_list.136.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.137.Conv2d.weight\n",
      "torch.Size([256, 128, 3, 3])\n",
      "module_list.137.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.137.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.137.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.137.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.137.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.138.Conv2d.weight\n",
      "torch.Size([255, 256, 1, 1])\n",
      "module_list.138.Conv2d.bias\n",
      "torch.Size([255])\n",
      "module_list.141.Conv2d.weight\n",
      "torch.Size([256, 128, 3, 3])\n",
      "module_list.141.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.141.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.141.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.141.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.141.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.143.Conv2d.weight\n",
      "torch.Size([256, 512, 1, 1])\n",
      "module_list.143.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.143.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.143.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.143.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.143.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.144.Conv2d.weight\n",
      "torch.Size([512, 256, 3, 3])\n",
      "module_list.144.BatchNorm2d.weight\n",
      "torch.Size([512])\n",
      "module_list.144.BatchNorm2d.bias\n",
      "torch.Size([512])\n",
      "module_list.144.BatchNorm2d.running_mean\n",
      "torch.Size([512])\n",
      "module_list.144.BatchNorm2d.running_var\n",
      "torch.Size([512])\n",
      "module_list.144.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.145.Conv2d.weight\n",
      "torch.Size([256, 512, 1, 1])\n",
      "module_list.145.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.145.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.145.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.145.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.145.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.146.Conv2d.weight\n",
      "torch.Size([512, 256, 3, 3])\n",
      "module_list.146.BatchNorm2d.weight\n",
      "torch.Size([512])\n",
      "module_list.146.BatchNorm2d.bias\n",
      "torch.Size([512])\n",
      "module_list.146.BatchNorm2d.running_mean\n",
      "torch.Size([512])\n",
      "module_list.146.BatchNorm2d.running_var\n",
      "torch.Size([512])\n",
      "module_list.146.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.147.Conv2d.weight\n",
      "torch.Size([256, 512, 1, 1])\n",
      "module_list.147.BatchNorm2d.weight\n",
      "torch.Size([256])\n",
      "module_list.147.BatchNorm2d.bias\n",
      "torch.Size([256])\n",
      "module_list.147.BatchNorm2d.running_mean\n",
      "torch.Size([256])\n",
      "module_list.147.BatchNorm2d.running_var\n",
      "torch.Size([256])\n",
      "module_list.147.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.148.Conv2d.weight\n",
      "torch.Size([512, 256, 3, 3])\n",
      "module_list.148.BatchNorm2d.weight\n",
      "torch.Size([512])\n",
      "module_list.148.BatchNorm2d.bias\n",
      "torch.Size([512])\n",
      "module_list.148.BatchNorm2d.running_mean\n",
      "torch.Size([512])\n",
      "module_list.148.BatchNorm2d.running_var\n",
      "torch.Size([512])\n",
      "module_list.148.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.149.Conv2d.weight\n",
      "torch.Size([255, 512, 1, 1])\n",
      "module_list.149.Conv2d.bias\n",
      "torch.Size([255])\n",
      "module_list.152.Conv2d.weight\n",
      "torch.Size([512, 256, 3, 3])\n",
      "module_list.152.BatchNorm2d.weight\n",
      "torch.Size([512])\n",
      "module_list.152.BatchNorm2d.bias\n",
      "torch.Size([512])\n",
      "module_list.152.BatchNorm2d.running_mean\n",
      "torch.Size([512])\n",
      "module_list.152.BatchNorm2d.running_var\n",
      "torch.Size([512])\n",
      "module_list.152.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.154.Conv2d.weight\n",
      "torch.Size([512, 1024, 1, 1])\n",
      "module_list.154.BatchNorm2d.weight\n",
      "torch.Size([512])\n",
      "module_list.154.BatchNorm2d.bias\n",
      "torch.Size([512])\n",
      "module_list.154.BatchNorm2d.running_mean\n",
      "torch.Size([512])\n",
      "module_list.154.BatchNorm2d.running_var\n",
      "torch.Size([512])\n",
      "module_list.154.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.155.Conv2d.weight\n",
      "torch.Size([1024, 512, 3, 3])\n",
      "module_list.155.BatchNorm2d.weight\n",
      "torch.Size([1024])\n",
      "module_list.155.BatchNorm2d.bias\n",
      "torch.Size([1024])\n",
      "module_list.155.BatchNorm2d.running_mean\n",
      "torch.Size([1024])\n",
      "module_list.155.BatchNorm2d.running_var\n",
      "torch.Size([1024])\n",
      "module_list.155.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.156.Conv2d.weight\n",
      "torch.Size([512, 1024, 1, 1])\n",
      "module_list.156.BatchNorm2d.weight\n",
      "torch.Size([512])\n",
      "module_list.156.BatchNorm2d.bias\n",
      "torch.Size([512])\n",
      "module_list.156.BatchNorm2d.running_mean\n",
      "torch.Size([512])\n",
      "module_list.156.BatchNorm2d.running_var\n",
      "torch.Size([512])\n",
      "module_list.156.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.157.Conv2d.weight\n",
      "torch.Size([1024, 512, 3, 3])\n",
      "module_list.157.BatchNorm2d.weight\n",
      "torch.Size([1024])\n",
      "module_list.157.BatchNorm2d.bias\n",
      "torch.Size([1024])\n",
      "module_list.157.BatchNorm2d.running_mean\n",
      "torch.Size([1024])\n",
      "module_list.157.BatchNorm2d.running_var\n",
      "torch.Size([1024])\n",
      "module_list.157.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.158.Conv2d.weight\n",
      "torch.Size([512, 1024, 1, 1])\n",
      "module_list.158.BatchNorm2d.weight\n",
      "torch.Size([512])\n",
      "module_list.158.BatchNorm2d.bias\n",
      "torch.Size([512])\n",
      "module_list.158.BatchNorm2d.running_mean\n",
      "torch.Size([512])\n",
      "module_list.158.BatchNorm2d.running_var\n",
      "torch.Size([512])\n",
      "module_list.158.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.159.Conv2d.weight\n",
      "torch.Size([1024, 512, 3, 3])\n",
      "module_list.159.BatchNorm2d.weight\n",
      "torch.Size([1024])\n",
      "module_list.159.BatchNorm2d.bias\n",
      "torch.Size([1024])\n",
      "module_list.159.BatchNorm2d.running_mean\n",
      "torch.Size([1024])\n",
      "module_list.159.BatchNorm2d.running_var\n",
      "torch.Size([1024])\n",
      "module_list.159.BatchNorm2d.num_batches_tracked\n",
      "torch.Size([])\n",
      "module_list.160.Conv2d.weight\n",
      "torch.Size([255, 1024, 1, 1])\n",
      "module_list.160.Conv2d.bias\n",
      "torch.Size([255])\n",
      "648\n"
     ]
    }
   ],
   "source": [
    "mm = d['model']\n",
    "i = 0\n",
    "yolo_v4_size = []\n",
    "for kk in mm:\n",
    "    i += 1\n",
    "    print(kk)\n",
    "    print(mm[kk].size())\n",
    "    yolo_v4_size.append(mm[kk].size())\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 128, 1, 1])\n",
      "torch.Size([64, 64, 1, 1])\n",
      "42\n",
      "torch.Size([128, 128, 1, 1])\n",
      "torch.Size([128, 64, 1, 1])\n",
      "96\n",
      "torch.Size([256, 256, 1, 1])\n",
      "torch.Size([256, 128, 1, 1])\n",
      "222\n",
      "torch.Size([512, 512, 1, 1])\n",
      "torch.Size([512, 256, 1, 1])\n",
      "348\n",
      "torch.Size([1024, 1024, 1, 1])\n",
      "torch.Size([1024, 512, 1, 1])\n",
      "426\n",
      "torch.Size([512, 2048, 1, 1])\n",
      "torch.Size([512, 512, 1, 1])\n",
      "450\n",
      "torch.Size([256, 512, 1, 1])\n",
      "torch.Size([256, 256, 1, 1])\n",
      "480\n",
      "torch.Size([128, 256, 1, 1])\n",
      "torch.Size([128, 128, 1, 1])\n",
      "522\n",
      "torch.Size([256, 512, 1, 1])\n",
      "torch.Size([256, 256, 1, 1])\n",
      "566\n",
      "torch.Size([512, 1024, 1, 1])\n",
      "torch.Size([512, 512, 1, 1])\n",
      "610\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(yolo_v4_size)):\n",
    "    #print(yolo_v4_size[i] == HX[i])\n",
    "    if (yolo_v4_size[i] == HX[i]) == False:\n",
    "        print(yolo_v4_size[i])\n",
    "        print(HX[i])\n",
    "        print(i)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
